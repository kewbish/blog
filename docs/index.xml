<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Yours, Kewbish - a collection of articles on tech and thought.</title>
    <link>https://kewbi.sh/blog/</link>
    <description>Latest Yours, Kewbish posts</description>
    <managingEditor>(Emilie Ma (Kewbish))</managingEditor>
    
	<atom:link href="https://kewbi.sh/blog/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>End-Of-Life</title>
      <link>https://kewbi.sh/blog/posts/231224/</link>
      <pubDate>24 Dec 2023</pubDate>
      <author>Emilie Ma (Kewbish)</author>
      <description>On bitrot, deprecation, and deletion.</description>
      <content:encoded><![CDATA[<p>I&rsquo;ve been thinking about mortality and longevity recently. Today, I&rsquo;d like to think about durability of our data, the lifespan of the websites we rely on, and the process of software reaching End-Of-Life.</p>
<p>A natural final destination for software is deprecation and deletion. Sure, some mission-critical software will stay in use for decades to come, but even the essential tools of the 70s have been replaced just fifty years later. This is understandable, since computer science is such a young field, but during this evolution, I&rsquo;m sure many lines of BASIC, Pascal, and all those languages now regarded as ancient have been wiped and lost to time.</p>
<p>There are initiatives that aim to prevent or delay that from happening. The <a href="https://archive.org/">Internet Archive</a> is perhaps the most well-known example of this I can think of. The <a href="https://archiveprogram.github.com/arctic-vault/">GitHub Arctic Vault</a> program similarly strives to preserve current-day code as far into the future as possible. In 2020, all active public repos were archived into an Arctic vault, deep in the permafrost. I&rsquo;ll note, though, that GitHub&rsquo;s own site points to the possibility of climate change affecting the storage site (albeit only a thin layer of the permafrost under which the vault lies) among other factors.</p>
<p>Even if our data far outlasts us, is there value in holding onto it beyond historical curiosity? I think sentimentality can apply to software too. We all have childhood memories that have deeply affected us, so it makes sense that our data and the software we hold dear would as well. In this post, I&rsquo;ll primarily discuss personal data and websites, since I&rsquo;ve seen more examples of them experiencing this slow EOL, but there are parallels to be drawn with binary software and hardware as well.</p>
<p>It&rsquo;s painful to watch things we care about fade into obscurity, then stop working or lack support, then enter the digital ether. Same goes for people. Just as we reflect on the impact of those who have passed away on our own lives, I think it&rsquo;s interesting to think about how the makeup of our toolkits will shift as software hits its EOL.</p>
<h2 id="elementary-my-dear-watson">Elementary, my dear Watson</h2>
<p>I named my first laptop Watson. I got it in the summer before sixth grade, and used it almost til my senior year of high school. The summer before I started high school, I dual-booted Watson (originally running Windows) with Linux, and I quickly turned to using Linux daily over Windows.</p>
<p>However, at some point after I dual-booted with Linux, my Windows became completely sluggish. Logging in would take about three minutes. When it&rsquo;d finally load, I&rsquo;d get a black screen with just a cursor, and even the shortcuts to open Task Manager weren&rsquo;t working.</p>
<p>About a year ago, my dad&rsquo;s laptop, a very old Toshiba bought not long after I was born, also started slowing down. I volunteered my old laptop, Watson, for him to use, and set to getting it ready for him.</p>
<p>I was trying to fix the Windows partition of my system while only reformatting what was necessary. I&rsquo;d transferred over the contents of my Linux partition to my new device, so there was no sentimentality there, but there were still some old screenshots and game files on the Windows side that I&rsquo;d have preferred to keep. Over the course of a month, I tried to fix Windows without wiping everything. I&rsquo;d never looked at those files after switching to Linux, and I don&rsquo;t know why I was so attached to them. In the end, in a Herculean display of rationality, I wiped it all and factory reset my laptop. It works now, and my dad&rsquo;s happy with his &rsquo;new&rsquo; machine. I still think about what I&rsquo;ve lost in that process though — the files that were on that drive that I&rsquo;ll never get back.</p>
<p>Computers have afforded both digital abundance and digital hoarding. There are few reasons not to just hang onto everything, and it feels <em>wrong</em> to let files go. Just buy a hard disk and copy files over, regardless of if you&rsquo;ll look at them again. There&rsquo;s a niggling voice in my head wanting to keep things around for the sake of some nebulous &lsquo;future me&rsquo;. There are folks that seem to have similar hangups: I have a systems professor who said that every time his disk gets close to filling up, he buys a new laptop with twice as much storage and copies everything over. Per Murphy&rsquo;s law, it&rsquo;s worked out okay so far.</p>
<p>The files I deleted on my Windows drive were from my formative years of learning to use a computer. I weirdly sometimes want them back as keepsakes of how much I&rsquo;ve grown, but I think I&rsquo;m coming to terms with the fact that knowing I&rsquo;ve had those experiences is enough. Yet it still feels wrong to hard-delete files, like I&rsquo;m actively cutting the lifespan of my data short.</p>
<p>Even if you&rsquo;re not the one pulling the trigger and hitting <code>Del</code>, I wonder how much data online is continuously being lost passively. It doesn&rsquo;t make financial sense to retain records forever — data is cheap, but at scale, I realize that storing it can add up. Google is deleting inactive Gmail accounts, and reportedly, so does Instagram. This, more passive, type of deletion is also interesting to consider — it&rsquo;s like the data is slowly marching towards its deletion.</p>
<p>Today, there are on the order of 120 zettabytes (2^70 bytes) of data in the world, but I don&rsquo;t know if this statistic accounts for deleted or inactive data. That&rsquo;s more difficult to estimate, but if there&rsquo;s 330 million terabytes of data created per day, there&rsquo;s got to be at least a few million deleted. I wonder what we lose each day. The 330 million terabytes is primarily video data, so there&rsquo;s probably plenty of old video content in what&rsquo;s deleted, but perhaps some meaningful software too.</p>
<h2 id="davids-ipad">David&rsquo;s iPad</h2>
<p>My family still has one of the original iPad Airs from a company gift almost a decade ago. The limited set of apps I use runs perfectly fine, and for the most part websites work alright too. But as the web, or really, JavaScript standards, move onwards, website after website has stopped quite working as it used to. Reddit doesn&rsquo;t load comments, ad-heavy recipe sites keep throwing &lsquo;A problem repeatedly occurred&rsquo;, and a class of sites that appear to be using React show the same slap in the face: &lsquo;Application error&rsquo; and &lsquo;Browser not supported&rsquo;.</p>
<p>I&rsquo;m for change and innovation in the web space. As someone who&rsquo;s blindly thrown ES2023 features into code without worrying too much about browser support or interoperability, I get it. It&rsquo;s not the fault of web maintainers in any way. Yet it feels odd to have that reminder that things on the Internet aren&rsquo;t here, in the same state that they are now, accessible in the same ways they are now, forever.</p>
<p>The rest of my devices are fairly up-to-date, so I haven&rsquo;t suffered too much from device support issues beyond using this old iPad. It&rsquo;s a good reminder to me, though, that even if software sticks around for a long time, the hardware we have may not always fully support it. GitHub has decided to archive so many repositories in its Arctic Code Vault: in the future, will we even have the hardware to run it? We&rsquo;ll have to rely on emulators and apocryphal knowledge of processor, browser, and operating system quirks.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<h2 id="this-domain-is-for-sale">This domain is for sale</h2>
<p>To keep software innovating, sometimes we can&rsquo;t support all devices. To some extent, that&rsquo;s acceptable. The core functionality still works, we get new devices, the world goes on. But what happens when the app itself goes down?</p>
<p>There&rsquo;s websites out there that were in a &lsquo;it works now but don&rsquo;t touch it&rsquo; state, abandoned or forgotten. I came across <a href="https://stablequarters.org/">this equine center site</a> that appeared to use to have live horse cams, which are understandably no longer activated, as the center shut down in 2018. I wonder how the site&rsquo;s still running, five years later. Granted, it&rsquo;s very Web 1.0, so I doubt there were many critical dependencies, but the domain and hosting&rsquo;s still up.</p>
<p>Sites like these suffer from something similar to, but not entirely like, bit rot. With bit rot, functionality itself glitches or corrupts, but here it&rsquo;s like piece by piece of the site fades away. First the outlinks die, then some iframe source no longer connects, and so on. Because of backwards compatibility, most things keep working, so it&rsquo;s like a slowly decaying time capsule with a ticking timer for when it&rsquo;ll disappear.</p>
<p>Archive.org does a good job at capturing most sites on the internet to preserve snapshots before they go down, but they don&rsquo;t capture the full interactivity of pages. Outlinks aren&rsquo;t captured by default, so even if a page loads, we might not be able to access the websites in its latent space. As well, apps that require authentication won&rsquo;t get captured. Fifty years down the line, we&rsquo;ll only have screenshots and video recordings left.</p>
<p>As well, hosting costs someone somewhere money, even if it&rsquo;s ostensibly offered for free, so it makes sense that apps aren&rsquo;t hosted forever either. However, it&rsquo;s a lot easier to justify keeping an app up if it&rsquo;s running for free, as was possible with Heroku and its competitors. I&rsquo;d be interested in an estimate of what small percentage of the Internet <a href="https://www.heroku.com/">Heroku</a> took down just by removing its free tier. It&rsquo;s likely small, but biased towards personal passion projects. I&rsquo;d think work like this is highly sentimental to their creators, but perhaps not so much so that it&rsquo;s worth spending extra on hosting<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<p>Finally, I&rsquo;d like to consider domain names. It&rsquo;s impossible to buy a domain name forever — we&rsquo;re just renting them by the year<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. Gilles Castel, notable for their Vim + LaTeX notetaking workflow, unfortunately passed away in 2022, but <a href="https://castel.dev">their site</a> remains up. Perhaps they prepaid for a domain for many years, or perhaps someone close is taking care of the registration for them. I can&rsquo;t help but wonder what happens as long-held domains and hosting plans expire, particularly for proprietary websites where the original source may be lost entirely.</p>
<h2 id="conclusion">Conclusion</h2>
<p>One open-source project that inspired many of the deprecation-related thoughts in this post is <a href="https://github.com/jdecked/twemoji/">twemoji</a>, Twitter&rsquo;s signature emoji library, now forked from the prior Twitter open-source project. A few months ago, I saw <a href="https://github.com/twitter/twemoji/issues/570">this issue</a>, noting that there were problems with licensing prior emoji assets for reuse, so the project would stagnate. I use Twemoji as my default emoji font, and I realized that while previously released emojis would still work, more and more emojis would appear as <a href="https://en.wikipedia.org/wiki/Noto_fonts#Etymology">tofus</a> as time went on. I would literally be able to see the font decay.</p>
<p>Happily though, <a href="https://github.com/jdecked/twemoji/pull/51">this PR</a> recently got merged with the Unicode 15.0 updates and what I assume was approval from Twitter to continue the project. It&rsquo;s still a volunteer-run effort, but Discord as a company seems invested in contributing, so I&rsquo;m optimistic that Twemoji will continue to be updated until the inevitable process of software EOL kicks in yet again.</p>
<p>My dad once very thoughtfully reflected that everything needs maintenance: health, physical objects, relationships. So does software. When there&rsquo;s no one there, or no more resources available, to maintain it, the metaphorical health of the sites we frequent today will begin to decline. Eventually, we won&rsquo;t be able to locate them anymore, when their domains expire; to run them anymore, when their backends stop being hosted or our new processors ditch support. I&rsquo;d reckon they&rsquo;ll eventually all be deleted as well, despite our best efforts to memorialize and archive them.</p>
<p>As they say, though, such is life. While our software still runs (or hobbles along) for us today, we can learn to appreciate its quirks and the marks it&rsquo;s left on us. They say life is fleeting, and maybe it&rsquo;s okay that software is too.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>The game emulator communities give me hope in this respect, as well as those working on emulating classic systems like <a href="https://lisa.sunder.net/">the Lisa</a>.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>After Heroku announced the end of its free tier, many switched to <a href="https://fly.io">Fly.io</a>, which also offers a reasonable free tier. I made the switch for my CORS proxy for <a href="https://github.com/kewbish/matter">Matter</a>, but that was because I was actively using Matter. I suspect there were many little tools like Matter that went down just because someone didn&rsquo;t bother with manually porting things over.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>See also this post by Chuck Grimmett on <a href="https://cagrimmett.com/tech/2023/11/04/domain-longevity/">how we can keep our domains around after death</a>.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>Serialization</title>
      <link>https://kewbi.sh/blog/posts/231126/</link>
      <pubDate>26 Nov 2023</pubDate>
      <author>Emilie Ma (Kewbish)</author>
      <description>On reducing dimensions and preserving semantics.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>I remember when I first started learning Python and realized what an <a href="https://peps.python.org/pep-0498/">f-string</a> was. It was mind-blowing to me back then that not only could you print variables in a certain format, but also modify those variables and do other computation within the brackets, and get it to all display nicely. I was amazed that I could get variables of all different types to pretty-print themselves with an f-string.</p>
<p>I now know that under the hood, the f-string formatting is just calling <code>__str__()</code>, and that all the types I tried just had good <code>__str__()</code>s defined. This is an example of serialization: formally, transforming an object to a different representation so it can be saved to disk or transmitted over the wire. I like to think of it as dimension reduction in a representation — kind of like squashing a cube into a square, or in this case taking a photo of the cube (converting it into 2D data) to be faxed<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. To bring this dimension reduction back to Python, if I ran the following:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>dict <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#34;a&#34;</span>: <span style="color:#ae81ff">1</span>, <span style="color:#e6db74">&#34;b&#34;</span>: <span style="color:#ae81ff">2</span>}
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>dict<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><p>I&rsquo;d get <code>{&quot;a&quot;: 1, &quot;b&quot;: 2}</code> as the output. This converts the dictionary into a string so it can be transmitted via printing, but in doing so, the string loses some of the properties of the original dictionary. You can&rsquo;t call <code>.items()</code> on the string, nor can you add new key-value pairs. During serialization, the object&rsquo;s lost some of its intrinsic properties while retaining the same core information.</p>
<p>This is an interesting problem (feature?) of serialization, and it&rsquo;s something I&rsquo;d like to explore further, especially in the context of exporting metadata from apps and tool interoperability. In this post, I&rsquo;ll dive into JavaScript&rsquo;s infamous <code>[object Object]</code>, JavaScript&rsquo;s JSON serialization, and alternative serialization methods, like dehydration of data, before pivoting a little into what it means to really own our data (spoiler alert: it has to do with serialization too!)</p>
<h2 id="object-object">[object Object]</h2>
<p>I was once helping <a href="https://maplebacon.org/">Maple Bacon</a> run their UBC-specific CTF, SaplingCTF. I was working primarily on the CTFd theme, and for this iteration of SaplingCTF in particular, I was trying to get some custom styling working for progression challenges. I thought I had it all working, until things inevitably broke during the competition and I had to do some debugging and hotfixing. While trying to deploy a fix as soon as possible, I left an <code>alert(obj)</code> in there somewhere, which meant that each time someone reloaded the challenge page they&rsquo;d be greeted with <code>[object Object]</code>. Not my best work.</p>
<p><code>[object Object]</code> has been ubiquitous in my JavaScript experiences. Forgot to wrap something in <code>JSON.stringify()</code> before <code>console.log()</code>-ing it? <code>[object Object]</code><sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<p><code>[object Object]</code> comes from the <code>Object</code>&rsquo;s prototype&rsquo;s <code>toString()</code> method returning <code>[object Object]</code>. Functions get serialized as <code>[object Function]</code> and date objects as <code>[object Date]</code>. While all of these are objects, the second word in their serialization depends on their constructor type.</p>
<p>But this all changes if you <code>JSON.stringify()</code> the object. If you try it on <code>{}</code>, you&rsquo;ll get <code>&quot;{}&quot;</code>. If you try it on a date, you&rsquo;ll get a date. What gives?</p>
<p>For one, <code>Date</code>s implement the <code>toJSON()</code> method, so <code>JSON.stringify()</code> knows to show the ISO representation of the date as its serialized value. In general, properties that have <code>.toJSON()</code> will have that method called to determine how to serialize them, and others, like <code>undefined</code>, <code>Map</code>s, and <code>Set</code>s, will be ignored. <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON/stringify#description">MDN</a> has more about the specifics of the algorithm here.</p>
<p>I recently learned that you can also provide your own <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON/stringify#the_replacer_parameter">replacer function</a> to <code>JSON.stringify()</code> to tell it how to serialize certain types, like <code>ArrayBuffer</code>s or the aforementioned <code>Map</code>s and <code>Set</code>s. It&rsquo;s called on the object being stringified, then called recursively on each of the object&rsquo;s properties. You can check the type of the object (e.g. <code>object instanceof SpecialClass</code>) and return a value satisfactorily serializing the object&rsquo;s properties to be included into the JSON. <code>JSON.parse()</code> also has a reciprocal function called the <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON/parse#using_the_reviver_parameter">reviver function</a>. This is useful for including things like <code>Set</code>s in a JSON string, which I typically serialize as an array and rebuild into a <code>Set</code> on the other side.</p>
<h2 id="serialization-in-the-wild">Serialization in the Wild</h2>
<p>There are other ways to approach serialization in JavaScript that avoid having to create your own custom reducers for common datatypes and that handle cyclical references well. For example, Cloudflare&rsquo;s local Workers simulator, <code>miniflare</code>, uses <a href="https://github.com/Rich-Harris/devalue"><code>devalue</code></a> to flatten objects representing Workers abstractions into JSON so they can be piped through to different parts of the simulator<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. <code>devalue</code> brings support for <code>JSON.stringify</code>ing common objects like <code>Map</code>s and handles cyclical references, which you&rsquo;d otherwise have to write replacer functions for. A nice bonus is that it can even unflatten values that are part of a larger string. This is called rehydration: if we think of serializing values as vacuum-packing and drying food for easier storage and later consumption, rehydration is the &lsquo;just add water&rsquo; part of eating the MREs. While using it, I noted that <code>devalue</code> has some pretty neat raw output as well. If I recall correctly, it outputs seemingly nonsensical nested arrays of numbers and string keys — think something like <code>[[0, &quot;a&quot;, []], &quot;b&quot;]</code>. The project goals state that it&rsquo;s intentionally not human readable, but it&rsquo;s interesting that if you squinted hard enough, you can kinda tell where the structure comes from.</p>
<p>On the more theoretical PL side, I also recently learned about <a href="https://en.wikipedia.org/wiki/Thunk">thunks</a>. Thunks are a way to delay evaluation of an expensive function (and historically, to delay evaluation of its arguments) until later. In JavaScript, they can be implemented quite easily with arrow functions wrapping some function with other arguments and passing the variable name of the thunk around.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-js" data-lang="js"><span style="display:flex;"><span><span style="color:#66d9ef">const</span> <span style="color:#a6e22e">expensiveFunction</span> <span style="color:#f92672">=</span> (<span style="color:#a6e22e">n</span><span style="color:#f92672">:</span> <span style="color:#66d9ef">int</span>) =&gt; {
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">/* calculate factorization of n */</span>
</span></span><span style="display:flex;"><span>};
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">const</span> <span style="color:#a6e22e">thunk</span> <span style="color:#f92672">=</span> () =&gt; <span style="color:#a6e22e">expensiveFunction</span>(<span style="color:#ae81ff">1337</span>);
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">doSomethingElse</span>(<span style="color:#a6e22e">thunk</span>); <span style="color:#75715e">// can pass in the thunk function to later be evaluated
</span></span></span></code></pre></div><p>Thunks are mostly used to avoid executing code, but I&rsquo;ve seen them used in serializing expensive or unreliable function results as well. I was <a href="https://github.com/jepsen-io/maelstrom/blob/main/doc/05-datomic/03-persistent-trees.md">optimizing a toy distributed KV store</a> recently and I was using &rsquo;thunks&rsquo; to store mappings of unique IDs to values. This let me only occasionally retrieve the underlying values when needed to hydrate parts of the KV map while keeping other parts unevaluated and ready for computation later. To me, thunks feel like a type of serialization too, since they package up a function in a representation for use (evaluation) in another context. However, since they&rsquo;re usually implemented with basic built-in language features and are usually never read in their bytecode form outside of the program&rsquo;s execution, maybe this one&rsquo;s a bit of a stretch.</p>
<p>While Python&rsquo;s <a href="https://docs.python.org/3/library/pickle.html">pickling</a> is in an entirely different language, it also bears mentioning in a post about serialization. This past semester, I was working on a research project that had been started by another student the summer before. <a href="https://github.com/ubcdlab/pr-issue-topology-project">The project</a> involved a lot of web scraping and I wasn&rsquo;t looking forward to having to do it again, but the previous student had the foresight to save all the scraped Python objects as pickles! This made it incredibly easy to load in a big class object and start manipulating data. The scraping had also been performed with the help of some other libraries, and Pickle preserves the functions defined on each object, so even without the libraries installed I was able to call basic functions and get started quickly.</p>
<p>Pickling in Python is the richest form of serialization I&rsquo;ve worked with yet - the fact that the object and all its properties and functions can be recreated from a simple file means that none of the original data or behaviour is lost. At the start of this article, I mentioned how serialization often feels like dimension reduction: after data is serialized, it typically loses some behaviour or some data attributes, even if it&rsquo;s parsed and reconstructed later. Pickling makes serialization without this reduction practical.</p>
<p>A section on the forms of serialization I&rsquo;ve encountered so far wouldn&rsquo;t be complete without a final little hat-tip to <a href="https://en.wikipedia.org/wiki/Racket_(programming_language)">Racket</a> and its Lispy concepts of data as code and code as data with S-expressions. I can&rsquo;t put it into words, but there seems to be something intrinsically connecting the concepts of serializability and having your code live as data and vice versa.</p>
<h2 id="iframe-pickling-the-web"><code>&lt;iframe&gt;</code>: Pickling the Web</h2>
<p>If serializing is converting something into a form that can be transmitted, there&rsquo;s also something interesting in primitives that don&rsquo;t have to be converted to be transmitted. Pickling is one such modality, but so are <code>&lt;iframe&gt;</code>s. Pardon my continual gross misuse of &ldquo;serialization&rdquo;, but I also think <code>&lt;iframe&gt;</code> embed tags are, if not a form of serialization, at least something very closely adjacent. They&rsquo;re arguably one of the most ubiquitous and well-embedded (pun not intended) on the web. They were introduced in 1997<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> (after Python&rsquo;s <code>pickle</code> was implemented!)</p>
<p>In some ways, they&rsquo;re similar to pickle files — they preserve the entire functionality of the object and allow the site to be integrated into the rest of another object. They enable websites to be part of each other in a rich way like pickle files, with some extra strict container boundaries for security.</p>
<p>On the other hand, they&rsquo;re not persistent like pickle files, even if the hosting website is a local file. They depend on the current iteration of the website, and if the embedded site goes down, the hosting site won&rsquo;t display properly as well. You could embed an <code>&lt;iframe&gt;</code> to the <code>archive.org</code> as well, but that might not necessarily reflect the most recent updates to the site. Maybe embedding an <code>archive.org</code> link is somewhat closer to serialization than live <code>&lt;iframe&gt;</code>s.</p>
<h2 id="tool-interoperability">Tool Interoperability</h2>
<p>At the core of serialization is the idea that data <em>moves</em>: between scripts (Pickling), between obscured code and developer-facing output (JSON), and between domains on the web (<code>&lt;iframe&gt;</code>s). We&rsquo;ve seen in this post all the ways serialization can enable interesting theoretical behaviour, but what happens in the real world?</p>
<p>I think serialization-forward software is almost necessarily at odds with the walled-garden, mystery-cloud approach of most SaaS tools. Nowadays, most tools support some way to export your data, but moving between software typically causes you to lose something. I once did a big migration from OneNote to raw Markdown, which, to state the obvious, meant losing all my image positioning, coloured highlights, and so on. That&rsquo;s a big fidelity jump, so I&rsquo;ll forgive OneNote, but there&rsquo;s so many more examples online. You can export webpages to PDF, losing easy access to their hyperlinks or to interactive media. Depending on the service, exports can range from CSVs to JSON — you might get all the required data for your use case, but you also might not. You&rsquo;ll certainly have to recreate the underlying functionality of the data, or find a tool to do so. If you find another app and switch, it&rsquo;ll often take significant work to convert to the serialization format expected by the new program.</p>
<p>Serialization, and ways to extract rich metadata, is key for true interoperability. In an ideal world, we&rsquo;d also be able to get some base idea of the object&rsquo;s original interactivity models. Today, we&rsquo;re stuck with copy-and-pasting raw unformatted text and taking static screenshots that don&rsquo;t encode any of a tool&rsquo;s behaviour.</p>
<p>The performance of ways to do this is another issue to tackle. To enable a practical workflow where you&rsquo;re able to switch seamlessly between apps, your context-switching processing time should be as low as possible. This is a challenge when you&rsquo;ll likely have to convert between file formats and reconcile differences in structure on the fly, all of which appear very expensive time-wise.</p>
<p>I see aspects of this interoperability in tools already — MS Word allows you to embed and interact with MS Excel charts, for example. There are also a plethora of sketchy scripts on GitHub that allow you to convert and import between tools. But they lack the authoritative support, polish, and ease of use that I think are necessary to bring this potential ecosystem together.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Serialization and representation are so closely intertwined that if I ran <code>s/serialize/represent/g</code> on this post it&rsquo;d still make sense (and I&rsquo;d still have to apologize for my overly stretched usage of either word). I don&rsquo;t know how to differentiate between them, and I think in this post I&rsquo;ve mixed both up and added a few more tablespoons of general interest in data to boot.</p>
<p>I&rsquo;m interested in thinking about more open ways to move data around, relying perhaps on more easily serialized forms of data, like JSON, and figuring out how to codify original behaviour or intent. Ink And Switch&rsquo;s <a href="https://www.inkandswitch.com/potluck/">Potluck</a> and <a href="https://www.inkandswitch.com/cambria/">Cambria</a> projects both fit closely to this space, and I&rsquo;ve also been inspired by Alexander Obenauer&rsquo;s <a href="https://alexanderobenauer.com/labnotes/002/">idea of universal data portability</a> and thoughts on <a href="https://alexanderobenauer.com/labnotes/000/#:~:text=Having%20fun%20with%20item%20views">data views</a>. <a href="https://streamlit.io/">Streamlit</a>, a Python framework to build data-based apps, seems like an adjacent step towards what I&rsquo;m envisioning as well. I&rsquo;d like to dive specifically into how to represent original intent at a rich level sometime — we&rsquo;ll see what comes out of that thought.</p>
<p>Also, on a final note, I&rsquo;ll be giving a lightning talk at <a href="https://neovimconf.live/">Neovimconf 2023</a> about my personal knowledge management system in Vim! Tickets are free, so tune in on December 8th, 2023 if you&rsquo;re interested in learning more about taking notes with Vim, a little splash of Ripgrep, and some FZF.vim too.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>By &lsquo;dimension&rsquo; here, I mean one &lsquo;aspect&rsquo; or &lsquo;axis&rsquo; of data. I loosely also include &rsquo;the ability to do something&rsquo; - something like an instance function - as a dimension. I think the term &lsquo;dimension&rsquo; is probably overloaded here, and there&rsquo;s probably a better way to express this sort of &lsquo;squishing&rsquo;.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>This mistake once derailed an entire hackathon project for about 6 of the 36 hours. It was extremely frustrating to diagnose as a JavaScript beginner - what, was I supposed to know that <code>fetch()</code> was expecting a string for a request&rsquo;s body?&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>When I was working on updating <a href="https://github.com/cloudflare/miniflare/pull/641">the Queues implementation</a>, I looked into it for sending the Queue&rsquo;s messages around. To be honest, when making the PR, all the serialization and buffer manipulation felt a bit like black magic, primarily because I was unfamiliar with how Buffers and ArrayBuffers worked. Learning more of the JS internals, like this, is something I want to improve at.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>It was a surprise to me that Python&rsquo;s pickle library <a href="https://github.com/python/cpython/commit/a48061a5804418a63aac24bfce444fd555e3ffe7">predates <code>&lt;iframe&gt;</code>s</a>. The first commit in the Cpython Pickle library file was in 1995, but there are references to an even older &lsquo;flatten&rsquo; version of the library.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>Highlighting Text in Vim</title>
      <link>https://kewbi.sh/blog/posts/230807/</link>
      <pubDate>07 Aug 2023</pubDate>
      <author>Emilie Ma (Kewbish)</author>
      <description>On highlighting (the Sharpie and Hi-Liter kind).</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>I&rsquo;ve always liked text-based interfaces like TUIs and interactive CLIs. They&rsquo;re consistent, familiar, and themeable (since they&rsquo;re just text). I associate TUIs and text-based interfaces in general with a bit of an initial learning curve but also with a conceptual simplicity and most importantly, a lot of power. Interfaces like these are easy to make your own and to slip into.</p>
<p>Take Vim, for example. It&rsquo;s infamous for its comparatively esoteric keybinds - how do you quit again? But once you take a moment to learn its modes and its keybind &lsquo;vocabulary&rsquo;, you&rsquo;ll find that it&rsquo;s predictable and dependable. I won&rsquo;t go far as to say the keybinds are intuitive, but they <em>can</em> be reasonably added to your intuition model.</p>
<p>I started using Vim around 2020, when I was transfering all my notes from OneNote to Markdown. I was still in high school, so I had the liberty of not being as serious about my notes. I had grand ideals of being efficient and learning a new skill, and I&rsquo;d heard about Vim from all the jokes online. It seemed like a fun challenge to learn. For a while, I used Vim bindings within VSCode, but when I switched to Linux in 2021, I made Vim my daily-driver editor.</p>
<p>I recently ported my configuration files over to NeoVim, since I&rsquo;ve been seeing more and more plugins that rely on some NeoVim-specific features. Take the <a href="https://github.com/giusgad/pets.nvim">pets.nvim</a> extension for example. Most plugins also limit support to NeoVim for some nice QOL features. <a href="https://github.com/neoclide/coc.nvim">COC.nvim</a> type annotations display better and don&rsquo;t break on NeoVim, and the <a href="https://github.com/iamcco/coc-spell-checker">spelling plugin</a> I use underlines better. Most of my config has stayed the same - it&rsquo;s been a super easy port, and that says a lot about the level of care to maintain Vim interop.</p>
<p>I&rsquo;ve also started adding more personal touches to my notetaking Vim config. I thought it&rsquo;d be cool to add some color to my setup, which is currently extremely grey, and added some snippets for colouring text in my notes. I&rsquo;ve always wondered if it&rsquo;d be possible to replicate the OneNote / typical word processor features of highlighting. I&rsquo;ve thought about replicating the ease of use of text colouring in OneNote via a visual selection → highlight mechanism in Vim, and I&rsquo;ve just lately figured out how to do it.</p>
<figure><img src="/img/230807/my-highlighting-setup.png"
         alt="Figure 1. My current highlighting setup."/><figcaption>
            <p><em>Figure 1. My current highlighting setup.</em></p>
        </figcaption>
</figure>

<p>This is a post about implementing a reasonably usable text highlighting feature in Vim, and the other fun Vim features I discovered along the way.</p>
<h2 id="snippet-expansions">Snippet Expansions</h2>
<p>There are loads of Vim plugins for inserting snippets, like <a href="https://github.com/honza/vim-snippets">vim-snippets</a> or <a href="https://github.com/SirVer/ultisnips">Ultisnips</a>. However, for the simple text highlighting that I use in my notes, I&rsquo;ve found it easier to just use insert mode level remappings natively in Vim.</p>
<p>For my insert mode remaps, I tend to map combinations starting with my leader key, <!-- raw HTML omitted -->&lt;/kbd&gt;. Starting combos with your leader avoids the situation where you really do want to type a sequence of keys instead of execute your keymap. My leader key&rsquo;s not something that I&rsquo;d typically type in writing, so the short delay Vim imposes when I want to type the single character is bearable.</p>
<p>Here&rsquo;s the basic form of one of my native snippet expansions:</p>
<pre tabindex="0"><code>inoremap &lt;leader&gt;[shortcut] [text to insert]
</code></pre><ul>
<li><code>i-</code> denotes that it&rsquo;s a remapping that only applies in insert mode.</li>
<li><code>-no-</code> denotes that we shouldn&rsquo;t substitute any of the following characters for remapped versions, effectively making it non-recursive. For example, if we&rsquo;d defined &rsquo;s&rsquo; elsewhere to insert &lsquo;ß&rsquo; in insert mode, adding <code>-no-</code> makes the remap insert &rsquo;s&rsquo; if the remap itself contains &rsquo;s&rsquo;.</li>
<li><code>-remap</code> denotes the remap.</li>
<li>The second bit here is the keymap to press. I use my leader key (<code>\</code>) followed by a quick shortcut.</li>
<li>The third part is the text to insert.</li>
</ul>
<p>For example, to insert <code>(CN)</code> in a note, I use the following keybind.</p>
<pre tabindex="0"><code>inoremap &lt;leader&gt;cn (CN)
</code></pre><p>I only make a snippet mapping when I find myself having to repeatedly type long or physically awkward phrases. These types of snippets are useful for movements that would otherwise require shifting and wrapping and generally more finger movement than desired. Instead of holding <code>shift</code> and pressing <code>9-c-n</code>, I can just tap <code>leader-c-n</code><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<h2 id="text-styling">Text Styling</h2>
<p>Onto the fun part - text styling. In Vim, you can recreate text highlights (changing the background colour of text), font colour changes (changing the foreground colour of text), bolding, italicizing, underlining, and striking through text. There&rsquo;s probably more to this list - there are a <em>lot</em> of terminal escape sequences.</p>
<p>To do this, you can add <code>autogroup</code>s in your <code>.vimrc</code> or <code>init.vim</code>. If you want, you can also <a href="https://web.archive.org/web/20230130221652/http://learnvimscriptthehardway.stevelosh.com/">create your own Vim plugin</a> to keep things organized, and so you can more easily distribute your updates.</p>
<p>Here&rsquo;s a basic skeleton for how to change the colour of text:</p>
<pre tabindex="0"><code>augroup notes
    autocmd!
    autocmd FileType markdown syntax match CorrodeClassmateNote /\v\(CN\)/
    autocmd FileType markdown hi CorrodeClassmateNote ctermfg=152 guifg=#afd7d7
augroup END
</code></pre><p>This renders to:</p>
<figure><img src="/img/230807/highlighting-example.png"
         alt="Figure 2. An example of highlighting (CN) in light blue."/><figcaption>
            <p><em>Figure 2. An example of highlighting (CN) in light blue.</em></p>
        </figcaption>
</figure>

<p>The first <a href="https://blog.sidebits.tech/vim-autocommands/"><code>autocmd</code></a> line creates a syntax match &lsquo;class&rsquo;, and the second <code>autocmd</code> <code>hi</code>ghlights the text. The syntax match class is a regex - you can use the <code>\v</code> escape option to avoid escaping all the regex special characters manually. Here, I&rsquo;m matching the literal <code>(CN)</code><sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<p>Each match can be added to a group (<code>CorrodeClassmateNote</code> is a match group, for example), and the name of the match group is the identifier you&rsquo;ll use to style it. Avoid using <a href="https://jordanelver.co.uk/blog/2015/05/27/working-with-vim-colorschemes/">common group names</a> so you don&rsquo;t override your existing theme. The <code>FileType markdown</code> also makes this match group only apply in Markdown files, though you can use filename globs or choose another Vim filetype.</p>
<p>The second line highlights the <code>CorrodeClassmateNote</code> group with the given <code>ctermfg</code> (foreground colour used in the terminal) or <code>guifg</code> (colour used if you use GUI Vim instead). <code>guifg</code> can be in hex, but <code>ctermfg</code> needs to be one of the Xterm prompt colours <a href="https://www.ditig.com/256-colors-cheat-sheet">supported by your terminal</a>.</p>
<p>You can change <code>ctermfg</code> and <code>guifg</code> to <code>ctermbg</code> and <code>guibg</code> respectively to change the <em>background</em> colour used in the terminal or GUI. This gets you a typical &lsquo;highlighter&rsquo; functionality.</p>
<figure><img src="/img/230807/background-highlighting-example.png"
         alt="Figure 3. Highlighting (CN)&#39;s background in light blue."/><figcaption>
            <p><em>Figure 3. Highlighting (CN)&rsquo;s background in light blue.</em></p>
        </figcaption>
</figure>

<p>If you want to bold or italicize text, just add <code>cterm=bold gui=bold</code> or <code>cterm=italic gui=italic</code> to the end of the <code>hi</code> <code>autocmd</code>. And if you&rsquo;d like to apply both, <code>cterm=bold,italic gui=bold,italic</code> will do the trick. See <a href="https://vimdoc.sourceforge.net/htmldoc/syntax.html#attr-list"><code>:help attr-list</code></a> for more information on the attributes you can use to style text here.</p>
<h2 id="ctrl-shift-c-or-changing-font-colour">Ctrl-Shift-C (or, Changing Font Colour)</h2>
<p>If you want to colour not just specific text, but replicate a general &lsquo;change colour&rsquo; functionality in Vim, define a match group like so:</p>
<pre tabindex="0"><code>autocmd FileType markdown syntax match GreenHighlight /\v\(\#G(.*)\)/
autocmd FileType markdown hi GreenHighlight ctermfg=121 guifg=#88ff88
</code></pre><p>This highlights all matches of <code>(#G text)</code> a special shade of green. In Vim, it renders to this:</p>
<figure><img src="/img/230807/green-highlighting-example.png"
         alt="Figure 4. Highlighting some text in WebWork green."/><figcaption>
            <p><em>Figure 4. Highlighting some text in WebWork green.</em></p>
        </figcaption>
</figure>

<p>You can define multiple match groups with different prefixes - say, <code>(#Y text)</code> for yellow instead of green - for whatever you want to use. You can also define a remap in visual mode to wrap the current selection with the appropriate syntax to highlight it:</p>
<pre tabindex="0"><code>vnoremap &lt;leader&gt;hg c(#G &lt;C-r&gt;&#34;)&lt;C-c&gt;
</code></pre><p>Select some text in visual mode, then hit <code>&lt;leader&gt;hg</code> to add the highlighting markup. This makes use of vim&rsquo;s default register with the &lsquo;c&rsquo; command to replace text selected in visual mode with the wrapper <code>(#G ... )</code>, and uses <code>&lt;C-r&gt;&quot;</code> (Ctrl-r) to paste the contents that you just selected back into the wrapper. The final <code>&lt;C-c&gt;</code> exits insert mode for you for convenience.</p>
<p>The issue with this approach is that you&rsquo;ll have to add markup to your text to get it to highlight. While it&rsquo;ll still be there in the raw file contents, let&rsquo;s address the visual aspect and hide the <code>(#G ... )</code> wrapper in-editor.</p>
<h2 id="a-cleaner-look-with-conceal">A Cleaner Look with <code>conceal</code></h2>
<p>This&rsquo;ll make use of Vim&rsquo;s <a href="https://vimdoc.sourceforge.net/htmldoc/syntax.html#conceal">conceal</a> feature. It leverages match groups, similarly to text highlighting, to hide text that matches some pattern until you move your cursor to that line. People typically use this to replace syntax with more aesthetically pleasing characters, like replacing <code>lambda x: </code> with <code>λ x:</code>. It can also be used to hide comments that aren&rsquo;t distracting, make inline flashcards that only preview when you hover on a line, and prettifies Markdown markup and <a href="https://github.com/jalvesaq/zotcite">Zotcite</a> citations.</p>
<p>To hide the highlighting wrapper markup:</p>
<pre tabindex="0"><code>autocmd FileType markdown syntax match GreenHLConceal /\v(\(\#G\s)/ conceal
autocmd FileType markdown syntax match GreenHLConceal /\v(\(\#G\s(.*))@&lt;=\)/ conceal
autocmd FileType markdown syntax match GreenHighlight /\v(\(\#G\s)@&lt;=([^\)]*)\)@=/
autocmd FileType markdown setlocal conceallevel=3
</code></pre><p>The first <code>syntax match</code> line matches the first part of the wrapper, <code>(#G</code>, and the second uses Vim&rsquo;s lookback syntax to match any <code>)</code> preceded by a <code>(#G</code>. The <code>GreenHighlight</code> line has been extended a bit to deal with multiple markups per line too.</p>
<p>The fourth line sets the file&rsquo;s <code>concealllevel</code> to 3. Concealled groups have &rsquo;levels&rsquo; of display, which can be controlled on a buffer-level basis. The default <code>conceallevel=0</code> means that all concealled text is still shown in-editor. <code>conceallevel=3</code>, what we&rsquo;re using, means that all concealled text is completely hidden until you move your cursor to that line. In between, <code>conceallevel</code> 1 and 2 can also display a single character replacement for the hidden text. For example, you can add <code>cchar=🟩</code> to the first syntax match to replace the concealled text (the <code>(#G</code>) with a green square emoji.</p>
<p>If you&rsquo;re adding multiple highlighting syntax matches, you can hide all their markup prefixes in one command by replacing the <code>G</code> in each of the first three <code>syntax match</code> lines to something like <code>[RGB]</code>. This will hide the markup for all <code>(#G ...)</code>, <code>(#B ...)</code>, and <code>(#R ...)</code> highlight groups.</p>
<p>This renders to this:</p>
<figure><img src="/img/230807/conceallevel.gif"
         alt="Figure 5. Hiding the highlighting markup on cursor move."/><figcaption>
            <p><em>Figure 5. Hiding the highlighting markup on cursor move.</em></p>
        </figcaption>
</figure>

<h2 id="conclusion">Conclusion</h2>
<p>These highlighting features have helped me keep track of class discussions more easily and make my terminal-based notetaking system a little more fun. I only have two of these highlighting shortcuts set up (for &ldquo;my notes&rdquo; and &ldquo;classmate&rsquo;s note&rdquo;), but I use them fairly frequently during classes.</p>
<p>There are probably plugins to automate the creation of snippets and of highlighting commands, but I don&rsquo;t create new highlighting groups often enough to make it worth understanding them. Vim is known for its steep learning curve and batteries-not-included approach: I think the fact that the &lsquo;proper&rsquo; approach to this custom highlighting is a couple of plugins or quite a few lines of Vimscript is a testament to that. But I think learning to make custom highlighting groups and learning a bit more about Vim&rsquo;s syntax matching is a great way to get started with Vimscript. For those with a bit of regex knowledge, the syntax is relatively clear and the <code>autocmd</code>s are a starting point for <a href="https://learnvimscriptthehardway.stevelosh.com/chapters/12.html">adding plenty of custom behaviour</a>.</p>
<p>Another feature I&rsquo;d like to explore now since I&rsquo;m using NeoVim is <a href="https://neovim.io/doc/user/api.html#api-extmark">virtual text, officially known as extmarks</a>. I first learned about this when the GitHub Copilot Vim plugin came out and I realized it wouldn&rsquo;t work with vanilla Vim<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. While Vim 9 <a href="https://vimhelp.org/textprop.txt.html#virtual-text">supports this feature</a>, there were a good few other reasons to switch to NeoVim, so I never tried it out in vanilla Vim.</p>
<p>I&rsquo;d like to continue to explore more Vim quirks on this blog - it&rsquo;s amusing to see how far I can push things. For a while, I tried to replicate a basic flashcard system in Vim, and I also have a couple updates to my <a href="https://kewbi.sh/blog/posts/210815/">FZF + RG</a> setup, specifically for for development. Recently, I even saw an interesting <a href="https://github.com/sigpwny/UIUCTF-2023-Public/tree/main/challenges/misc/vimjail1">CTF challenge</a> written in Vim. I&rsquo;ve been thinking of covering these, perhaps in a shorter format, but we&rsquo;ll see when I&rsquo;ll get around to it.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Not having to hold down shift makes adding these annotations in my notes a lot faster.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>I use this to denote &lsquo;classmate&rsquo;s note&rsquo; in discussion-heavy classes.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>I have yet to seriously try the plugin, even with virtual text support.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>Everything I Know About Stacked PRs</title>
      <link>https://kewbi.sh/blog/posts/230611/</link>
      <pubDate>11 Jun 2023</pubDate>
      <author>Emilie Ma (Kewbish)</author>
      <description>On a summer of shipping in stacks.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>I spent last summer interning at <a href="https://replit.com">Repl.it</a>, the browser-based coding platform that makes it easy to build and collaborate. I interned on their Workspace team, and primarily worked on <a href="https://blog.replit.com/nix-github-imports">porting their GitHub import flow to Nix</a> (and another super-secret project that hasn&rsquo;t been shipped yet). It was an incredible learning experience – I was mentored by insanely smart people and granted the autonomy to own development of an improved product flow. Check their platform out if you haven&rsquo;t already: it&rsquo;s just as amazing as the team behind it!</p>
<p>It&rsquo;s ironic that while working on GitHub imports to support Replit users&rsquo; dev workflows, a major technical challenge would be wrangling GitHub to support <em>my</em> dev workflow. While Replit&rsquo;s engineering team has no prescribed pull request submission process, I quickly learned that the most efficient way to get PRs reviewed quickly was to make them small. But when you&rsquo;re working on adding a new feature or flow, the amount of code changes needed is generally big. How do you split up a huge PR into smaller ones while making sure that they&rsquo;ll get merged in order and that they make sense as smaller units?</p>
<p>The solution was stacked pull requests. Instead of making a larger PR, split the PRs up into logical working chunks and stack them on top of each other. Let&rsquo;s say you&rsquo;re working on adding a new modal dialog for some flow. Inside, there&rsquo;s some interaction that calls out to a new API that you also implement, and some interaction that uses an existing API in a new way. Instead of submitting this all as one big PR, you can create a base PR with just the UI changes (<code>git checkout -b add_modal_ui</code> from <code>main</code>). Then, you can create another PR on a new branch off of the base PR with an implementation of the new API and hooking it up to the UI (<code>git checkout -b add_new_api</code> from <code>add_modal_ui</code>). After that, you can branch off this second PR again to integrate the existing API (<code>git checkout -b add_existing_api</code> from <code>add_new_api</code>).</p>
<p>The PR stack ends up looking like this:</p>
<pre tabindex="0"><code>main
└── add_modal_ui
    └── add_new_api
        └── add_existing_api
</code></pre><p>GitHub has a good UI for submitting stacked PRs that you might not have ever noticed. When submitting a new PR on a branch, you can choose the base branch that you submit your PR to. For example, when submitting the PR on the <code>add_new_api</code> branch, choose <code>add_modal_ui</code>. If you daisy-chain these, GitHub will automatically update the base branch of each PR in the stack as they get merged.</p>
<figure><img src="/img/230611/base-changes.png"
         alt="Figure 1. GitHub automatically changing a PR&#39;s base branch."/><figcaption>
            <p><em>Figure 1. GitHub automatically changing a PR&rsquo;s base branch.</em></p>
        </figcaption>
</figure>

<p>Because each individual PR builds on the previous one, but is a logically coherent and small addition, each PR can be reviewed individually. This speeds up PR review since reviewers don&rsquo;t have to comb through a hundred changed files trying to make sense of what affects what. It&rsquo;s easier to articulate intention with smaller PRs too, leading to better communication.</p>
<p>A PR stack typically gets merged from the leaf back down to the root PR (in the example, from <code>add_existing_api</code> to <code>add_modal_ui</code>) and back to the main development branch<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. This means that all your changes can still end up on the main branch as a single unit, as in the &ldquo;good ol&rsquo; Big PR&rdquo; approach, but they&rsquo;ve likely been easier to review and work on.</p>
<p>Sometimes the PRs can even be as small as a single commit, and the PR stack is essentially a PR of smaller PRs. Think of stacked PRs like an extension of the commit history feature within the GitHub code review pane. When reviewing big PRs, you can look through commit history to see how things were put together. Stacked PRs operate on the same principle: they isolate changes to their smallest logical units, but just do so more explicitly than with multiple commits.</p>
<p>Stacked PRs also kept me unblocked during the summer, even when I had lots of changes on the go that depended on each other. I didn&rsquo;t have to wait for a feature submitted as a large PR to be reviewed before working on something that built on top of it. This is especially nice when I submitted the base PRs for review before starting to work on PRs that build on it, since this way you can keep the stack small but keep the momentum of feature development going.</p>
<p>Stacked PR workflows aren&rsquo;t perfect. I spent a lot of time painstakingly resolving branch conflicts this summer<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup><sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>, especially after PRs lower in the stack had significant changes requested. When you&rsquo;re working on many different features within a feature, your PR stack can also turn into a gnarly PR tree. With more complicated changes to base PRs and inter-PR stack dependencies, we ran into what was lovingly known as &lsquo;rebase hell&rsquo;. As well, because PRs get merged into their parent PRs, the root PR still lands a big diff on the main branch, so take care to review changes in staging carefully before merging down. With stacked PRs, you also can&rsquo;t test individual PRs in production (unlike in short-lived branch workflows) which might be a deal-breaker if your local environment is limited.</p>
<p>In spite of all this, I still think stacked PRs are a useful tool in managing large PRs efficiently. This post covers everything I&rsquo;ve learned about working with stacked PRs, so you can skip the rebase hell and take advantage of some hard-learned workflows.</p>
<h2 id="merging-upstream-changes">Merging Upstream Changes</h2>
<p>Let&rsquo;s say you&rsquo;ve branched off main like so:</p>
<pre tabindex="0"><code>main
└── branch1
    └── branch2
</code></pre><p>You&rsquo;ve made some changes in <code>branch1</code> that you also want to incorporate into your changes in <code>branch2</code>: for example, <code>branch1</code> was reviewed and someone requested changes on underlying code that was developed in <code>branch1</code> but also depended on in <code>branch2</code>. Or perhaps <code>main</code> has had some significant updates that change your development environment, and you want to get those into your <code>branch2</code> as well.</p>
<p>The typical solution for this is while in <code>branch2</code>, run <code>git merge branch1</code> to get the new commits of <code>branch1</code> into <code>branch2</code>. Alternatively<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>, you can <code>git rebase branch1</code> on <code>branch2</code>, resolve merge conflicts, then <code>git push -f</code> onto <code>branch2</code>.</p>
<p>Another strategy is to run <code>git rebase --onto=main branch1 branch2</code> on any branch, then <code>git push -f</code>. If <code>branch1</code> has already been merged, you might need to find the <code>merge-base</code>, or the &lsquo;common ancestor&rsquo; between <code>main</code> and <code>branch2</code> with <code>git merge-base branch1 branch2</code> and run <code>git rebase --onto=main [merge-base-result] branch2</code>.</p>
<h2 id="avoiding-squashed-upstream-changes-in-bottom-of-stack">Avoiding Squashed Upstream Changes in Bottom of Stack</h2>
<p>In this footnote<sup id="fnref1:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, I said to avoid merging from the bottom of the stack up since this usually leaves a bunch of duplicate commits in the branches on the top of the stack as the commit hashes change on merge. But let&rsquo;s say you&rsquo;ve gone ahead and done this anyway, resulting in:</p>
<pre tabindex="0"><code>main (with branch1 changes)
└── branch2 (now with duplicate commits)
</code></pre><p>This gets tricky. My strategy was to try to keep a list of commits from <code>branch1</code> somewhere pre-merge, so that these commit messages can be compared then against <code>branch2</code>. Run <code>git rebase -i HEAD~x</code>, where <code>x</code> is the point at which you branched off <code>branch1</code> to <code>branch2</code>. Then in the interactive menu, drop all <code>branch1</code> commits. After running <code>git rebase main</code> and resolving merge conflicts, <code>branch2</code> should now only contain the changes unique to <code>branch2</code>. Run <code>git push -f</code> and you should be good to go! Phew.</p>
<h2 id="cherry-picking">Cherry-picking</h2>
<p>Another alternative fix for the above situation relies on careful cherry-picking. Let&rsquo;s say you&rsquo;ve branched off of <code>main</code> like the previous example but have merged down <code>branch1</code>:</p>
<pre tabindex="0"><code>main (with branch1 changes)
└── branch2
</code></pre><p>And you&rsquo;re in the same scenario: you need to get rid of duplicate commits in the new base PRs of the stack. Sometimes, when there are a few commits, it&rsquo;s easier to build a new branch with cherry-picked changes from the old <code>branch2</code>.</p>
<p>Run <code>git checkout main</code> and <code>git checkout -b branch2-new</code> to create a new branch incorporating the <code>branch1</code> changes with their new commit hashes. Then as with the previous section, note commit hashes of changes on <code>branch2</code> and cherry-pick them over with <code>git cherry-pick [commit_hash]</code>. You&rsquo;ll have to resolve conflicts along the way, but with smaller branches this can result in less resolution work than dropping commits and rebasing.</p>
<p>If all your <code>branch2</code> commits are consecutive, you can cherry-pick a range of commits all in one go. Run <code>git cherry-pick commit_hash_start^..commit_hash_end</code>. The <code>^</code> denotes to include the first hash, and the <code>..</code> denotes a continuous range between those two commits, inclusive.</p>
<h2 id="merging-changes-from-downstream">Merging Changes From Downstream</h2>
<p>Finally, let&rsquo;s cover how to merge changes down properly. Let&rsquo;s go back to the first example, and let&rsquo;s say you&rsquo;ve branched off main like so:</p>
<pre tabindex="0"><code>main
└── branch1
    └── branch2 ✓
</code></pre><p><code>branch2</code> has been approved. (Maybe <code>branch1</code> has as well, exciting!) The easiest way to merge things down to main from here is to go from the top of the stack (i.e. to go from <code>branch2</code>). This maintains commit hashes and avoids the complex rebasing strategies in the section above.</p>
<p>However, this results in larger diffs being applied <em>all at once</em> to main, so this may not be ideal for continually deploying environments where you&rsquo;d like to test incrementally. This also might not be a great idea if <code>main</code> is significantly ahead of <code>branch1</code> or <code>branch2</code>, which means there&rsquo;s more opportunities for untested breakage due to new <code>main</code> updates. My strategy was to merge down into the second-to-last PR on the stack (<code>branch1</code>), and rebase <code>main</code> into that so I&rsquo;d only have to resolve conflicts once. Then I&rsquo;d test, and once I was sure things were working, I&rsquo;d merge everything into main.</p>
<h2 id="conclusion">Conclusion</h2>
<p>These strategies should be enough to cover most stacked PR workflows - the commands are relatively straightforward, and carried me through a successful internship. However, I&rsquo;ve neglected to mention that there are tools out there that automate most of this workflow for you. While I haven&rsquo;t used it, many of my coworkers at Replit were playing around with <a href="https://graphite.dev/">Graphite.dev</a> at the time. It&rsquo;s a CLI that builds on top of git to handle this lower-level rebasing for you, and automates creating stacked PRs complete with a stack summary message in each PR. It looked useful, especially for the automatic rebasing, but I didn&rsquo;t want to bother migrating my workflow and learning new Git commands just then. I might check it out in the future though - if it can help me avoid rebase hell, I&rsquo;m all for it.</p>
<p>There are also some Git plugins that don&rsquo;t go quite to the extent of automating PR creation as Graphite does, but helps facilitate the details. I&rsquo;ve seen <a href="https://github.com/gitext-rs/git-stack">git-stack</a> and <a href="https://github.com/VirtusLab/git-machete">git-machete</a> mentioned before, as well as <a href="https://github.com/ezyang/ghstack/">ghstack</a> as a lightweight Graphite alternative.</p>
<p>Despite all the intricacies of a stacked PR workflow, I&rsquo;m still strongly convinced that these smaller PRs lead to more efficient PR review and faster development-feedback-ship loops. With it, I was able to ship features consistently and quickly last summer, and am still applying the same workflows at <a href="https://cloudflare.com">Cloudflare</a> this summer. I&rsquo;m sure this&rsquo;ll also be the case in future projects and work. The tooling supporting stacked PR workflows is becoming more robust and mature, and I&rsquo;m betting stacked PRs will become more and more prevalent in the coming years.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>This leads to the caveat that the base few PRs must be approved before the leaf PRs can be merged in. It&rsquo;s good to communicate that to your team when working with stacked PRs! Several times during my summer, my fellow intern and I made slow progress because we were constantly waiting for the base PRs to be approved and shipped before we could build on them again. Also try to avoid merging from the base up - this leads to a bunch of duplicated commits especially if your team uses the &lsquo;squash and merge&rsquo; strategy.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>It got so bad that at one point my fellow intern and I Slack huddled for a couple hours just picking through rebase conflicts. We called it a rebase party (complete with :partyparrot:s). Near the end of my internship, &lsquo;rebase hell&rsquo; became a running joke with my manager and some of my team.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>I recently learned about <code>git rerere</code>, which can help with the repeated rebase conflicts that can arise with stacked PRs. <code>git rerere</code> persists conflict resolutions and automatically tries to re-apply them if the same conflict comes up again later on in the rebase process.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>My preferred strategy, because it didn&rsquo;t leave merge commits in the history.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>The Secret Garden</title>
      <link>https://kewbi.sh/blog/posts/220130/</link>
      <pubDate>30 Jan 2022</pubDate>
      <author>Emilie Ma (Kewbish)</author>
      <description>On public and private thought cultivation.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>In the field of personal knowledge management, a popular term&rsquo;s popped up to describe knowledge bases: digital gardens. It&rsquo;s cute and endearly quaint - perhaps it reflects a sort of <a href="https://en.wikipedia.org/wiki/Walden">Walden-esque</a> desire to step away from the hustle of grind culture and bustle of digital life and returning to a humbler life of tending one&rsquo;s knowledge garden. There&rsquo;s a growing trend of publishing your personal wikis online, often complete with little Sprout / Sapling / Tree / Evergreen labels to denote the state of thoughts. The quintessential example I see linked the most is <a href="https://notes.andymatuschak.org/About_these_notes">Andy Matuschak&rsquo;s notes</a>. My good friend <a href="https://knowledge.uzpg.me/">Uzay Girit&rsquo;s</a> started one with his tool <a href="https://github.com/archivy/archivy">Archivy</a>. And many, many more - <a href="https://maggieappleton.com/garden">Maggie Appleton&rsquo;s</a>, <a href="https://www.mentalnodes.com/">Anne-Laure Le Cunff&rsquo;s</a>, and <a href="https://jzhao.xyz/thoughts/">Jacky Zhao&rsquo;s</a>, just to name a few.</p>
<p>These public gardens of thought fill a liminal space between a messy notebook, and more polished blog posts. It&rsquo;s part of the larger trend to work in public, ship in public, and now, think in public - gardens like this spark thoughts and conversations, and can open up room for more experimentation and play. Some blogs can feel corporate and stale, and lack the freshness of digital gardens that are updated weekly, or even daily. Digital gardens take away the pressure to perfect every last word - the phrase &lsquo;garden&rsquo; itself comes with connotations of a certain type of dirtiness, but good dirtiness. They come with the expectation that not everything will be perfect, but that eventually, every little thought and insight will grow, and provide joy and beauty and inspiration.</p>
<p>I could go on about the benefits of digital gardening, but I think plenty of people can do that better than I can<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. Instead, I&rsquo;ll discuss some of to preserve your knowledge only for yourself, and garden in private, not public. It sounds selfish and counterintuitive at first - after all, I&rsquo;m sure everyone&rsquo;s had at least one solid insight that&rsquo;d be of use to someone else. And even notwithstanding the value of thoughts and of publishing, there&rsquo;s a certain merit in encouraging people to expand their thinking habits. I know some of the course notes I took likely would be very useful to incoming students; and maybe some of my longer musings on tech culture could be interesting. But with all the practicality and advantages of digital gardens, I think there&rsquo;s a case for walled, secret gardens as well.</p>
<p>There&rsquo;s a novel I used to love as a kid - <a href="https://en.wikipedia.org/wiki/The_Secret_Garden">The Secret Garden</a> by Frances Hodgson Burnett. The main character begins as a spoilt little girl, forced to live with her uncle in a dreary corner of England. She hates her new home, and the people she&rsquo;s made to interact with. The rest of the story isn&rsquo;t really relevant, but the major finding and focal point of the story later becomes a secret garden she finds, tucked away into a corner of the manor. It&rsquo;s locked, but she somehow finds the key. From then on, the main character gradually becomes more carefree and joyous, spending her hours playing in the beautiful garden and having little escapades with the animals there. It&rsquo;s a cute story, but I think it&rsquo;s an apt metaphor for the maintenance and wonders of private knowledge.</p>
<p>This article is a collection of some of my thoughts on private and public memory, thought, and ideation. For the rest of this post, &lsquo;walled garden&rsquo;, or &lsquo;secret garden&rsquo; will refer to private wikis and knowledge bases whose primary intent isn&rsquo;t to be shared, or plainly isn&rsquo;t shared. I&rsquo;ll discuss the tending of these gardens, their benefits, and how they can work together with public digital gardens as well.</p>
<h2 id="pre-emptive-curation">Pre-emptive Curation</h2>
<p>I don&rsquo;t know if it&rsquo;s just me, but I feel that when I write for an audience, and for any person that isn&rsquo;t myself, my tone and my content and my voice changes. When I&rsquo;m writing these articles, I do feel like I&rsquo;m talking to a friend, but it&rsquo;s different than writing my own notes or scribbling away in my personal journal. I worry that if I&rsquo;d start a public garden, I&rsquo;d end up curating for others - imposing structures and optimizing for someone else&rsquo;s experience, not mine. And it&rsquo;s easy to say, &ldquo;Oh, just don&rsquo;t do that! Write as if you&rsquo;re writing for yourself, and no one else!&rdquo;, but to me, putting something out there in the world, even if it&rsquo;s something as small as a post here, is associated with a certain level of polish. That level of polish doesn&rsquo;t have to be very high - read some of my first couple blog posts for a great example of &rsquo;literally only spellchecked&rsquo;. But I&rsquo;d like to think that I do things now with a bit more intention, and that implicit drive to fulfill that resolution shapes how I write.</p>
<p>Having an audience shapes how you write, and how you take notes. At least for me, I felt that when I was working through my <a href="https://kewbi.sh/blog/posts/200629/">CS50</a> or <a href="https://kewbi.sh/blog/posts/201213/">CPSC 110</a> posts, I was writing actively with helping someone in mind. I was including basic things that were glossed over in lectures, and things that I myself already understood. This is good - if I&rsquo;d returned to my notes at any point in time, I&rsquo;d be able to get a bit more of an overview and brush up on the basics. But that&rsquo;s a small example, at least, of my perception of how my own writing changes with more technical explanations that are aimed at people, instead of just for myself. I found myself embedding lots more context - I did this a lot too when I started writing about my notetaking system. Almost every post would include a &lsquo;if you haven&rsquo;t heard of the Zettelkasten system&rsquo;, with the same links and the same references to Ahren&rsquo;s book and Luhmann himself.</p>
<p>In one of his tweets, Linus Lee said <a href="https://twitter.com/thesephist/status/1480274175545724928">that it seems like digital gardens and note dumps are moving to replace longer-form blog writing</a>. I agree - I feel like a good portion of the value of ideas comes from the context they occupy, and the potential they hold to spark future thoughts and ideas. I feel that public gardens tend to shift this focus from the context to the content itself, where often the context surrounding certain ideas is left out of notes entirely, and we&rsquo;re focusing just on the raw ideas. I was introduced to the concept of <a href="https://en.wikipedia.org/wiki/The_Death_of_the_Author">the death of the author</a> in English class last term, and I can see how that concept applies here. Notes in digital gardens can sometimes feel detached from the contexts they came out of, and because they&rsquo;re just published as-is by the author, who&rsquo;s retained all the implicit context but perhaps hasn&rsquo;t written it explicitly, it feels like they&rsquo;re being viewed solely as detached points.</p>
<p>One of the ways digital and secret gardens can work in concert is a sort of mixed publication method. People usually set it up so that they have some public folder in their knowledge base that gets published, leaving their own personal notes with potentially private information unpublicized. This unfortunately means that everything needs to be processed manually before you set up your notes to publish - this leads to more friction, and more time that someone could&rsquo;ve spent tending their own secret garden. However, it&rsquo;s also a good opportunity to revisit notes and think from others&rsquo; perspectives, allowing for opportunities to revise based on new contexts, or based on what others. While it&rsquo;s important to be careful not to overthink the review too much, I think it can also be a useful step of knowledge management regardless.</p>
<h2 id="into-your-mind">Into Your Mind</h2>
<p>Besides the privacy of the content, with publishing anything on the internet, I feel a slight moral obligation to at least double check what I&rsquo;ve written for major factual issues. I feel that, in that case, where I&rsquo;m reviewing even just for editorial errors and the like, I may as well polish things a bit more and turn it into more refined posts that can stand on their own. There&rsquo;s always going to be tacit judgement (? evaluation? there&rsquo;s probably a better word for it) of what you put out there on the internet. I personally feel that it&rsquo;s nicer to put more intention into things like that - I&rsquo;m sharing my voice, after all, and I&rsquo;d like to communicate it as clearly and truthfully as possible.</p>
<p>I tend to feel uncomfortable with taking responsibility for my unfinished work - I remember when I was working on projects last summer, I was loathe to share with friends. When I did, it was always after a caveat or two that &rsquo;this wasn&rsquo;t the final product!&rsquo;, just in case. This is something I&rsquo;m working on - as I mentioned in my <a href="https://kewbi.sh/blog/posts/220102/">2022 goals post</a>, I&rsquo;d like to share what I&rsquo;m doing more often, and gather more perspectives while my work is in progress. Right now, I&rsquo;m still working with this sense of uneasiness - I used to hate it when my friends would read my blog posts. It took me a long time, and I still cringe when they decide to link my posts in chat, but I&rsquo;m starting to feel more comfortable with that type of sharing.</p>
<p>Most of my articles here are works that are at least somewhat finished, and done with having a reader in mind. However, with digital gardens, notes are often a step or two under finished articles or coherent thoughts. If I felt this disconcerted by sharing work that I was already decently proud of, I don&rsquo;t think I&rsquo;d be alright with dumping all my thoughts out into the big, bad Web<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Maybe this is an issue with my personal mindset, but I tend to not like presenting ideas, or versions of things that I don&rsquo;t see as complete. I like having things I share be working at some minimial-viable-thought level at all times - maybe that&rsquo;s just me. For some people, digital gardens allowing them to share incomplete thoughts might inspire them to write more and post more often, but I don&rsquo;t feel that way. Digital gardens offer a window into the running state of consciousness of someone - into how they take notes, and into their mind. Some folks might find this liberating and empowering, but I find that this forces my ideas into certain streams of thinking over and over again. Especially with more personal views or unfinished, brewing thoughts, I think it&rsquo;s alright to leave those in private, secret gardens.</p>
<h2 id="conclusion">Conclusion</h2>
<p>All this lends a sense of responsibility, and of weight, to the task of tending one&rsquo;s digital garden. As an extension of your online identity, you&rsquo;re now in charge of maintaining your wiki. Sure, you could slap a giant notice that this is all to be taken at face value, and that you&rsquo;re not responsible for any errors or liability or whatever, but I still feel that it&rsquo;s too much pressure. I know plenty of people have their own views on this, and their own workflows where they just publish whatever and don&rsquo;t feel like they have to shape their thoughts to fit a specific voice. But for now, setting up a digital garden just isn&rsquo;t for me - perhaps that&rsquo;ll change in the future, I don&rsquo;t know. I think I prefer the quiet tending of a secret garden - one that I can, like in the novel, lock away, yet return to, and find joy in, whenever I like. I like writing for an audience of none (or, well, one) because it gives me freedom to leave thoughts tangled up, in the contexts they came from, and in the phrasing that they first occurred in. It takes away the pressure of having to polish each thought as I write it: I can leave that for another stage of tending my garden.</p>
<p>On one hand of the spectrum, digital gardens that are straight stream-of-consciousness, and true extensions of their landscaper&rsquo;s mind, suffer from high noise : signal ratios. By diluting core thoughts with a network of unfinished, work-in-progress thoughts that haven&rsquo;t fully bloomed yet, it&rsquo;s harder to make information useful, both to the author and to readers. More formal writing can feel distant, and repetitive at times, especially as you&rsquo;ll have to integrate context and evidence and properly support arguments. In short, public digital gardens can feel a bit brain-dumpy, and long-form content can be a bit stifling and formal. Secret gardens, and gardens that are a mix of private and public all across this spectrum, embrace thought&rsquo;s inherent lifecycle well. Secret gardens welcome the inherent stages of thought - because not all thoughts are immediately ready to share. They&rsquo;re a bit more informal and casual, as the primary audience is still just the author. But they allow room for thoughts to grow, take root, become hardy, and eventually be able to be transplanted gently into public gardens.</p>
<p>I see the larger movement towards digital gardens as a sort of reverse &rsquo;tragedy of the commons&rsquo;. Whereas it&rsquo;s usually that people neglect a shared resource for the sake of their own personal gains, assuming that others will take care of it, with digital gardens, I find that there&rsquo;s a lot more room for thought if you assume that you need to help no others, and that you should put writing for yourself at the forefront. There&rsquo;s a fine balance here, between oversharing and undersharing - between not tending a digital garden with the same care as longer, more formal writing, and putting too much thought into the publishing and overthinking it. I&rsquo;m still working on this, but with this blog, I think I&rsquo;ve found that good balance. Most of my articles come from well-connected thoughts that I&rsquo;ve chained together, and almost all of my more complex notes are derivatives of my posts. This way, I can still share the thoughts that I&rsquo;ve worked on the most, and tended the most, but leave the messier, less coherent work in my own secret garden.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Just Google &lsquo;digital gardens&rsquo;, click around a few links, and you&rsquo;ll be greeted with examples of people who are truly experts of thought horticulture.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>There&rsquo;s also something to say about this sense of resistance. Maybe that fact that I&rsquo;m scared, in a way, of sharing like this is a reason to finally start doing so more often.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>Time in Writing</title>
      <link>https://kewbi.sh/blog/posts/220116/</link>
      <pubDate>16 Jan 2022</pubDate>
      <author>Emilie Ma (Kewbish)</author>
      <description>On the role of time in iterative writing.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Over my break between my fall and spring terms of university, I&rsquo;d been writing a lot - mostly thinking of new article ideas for this blog. I&rsquo;d finally had more time to read through the huge backlog of interesting articles I&rsquo;d found for myself, so I spent some time riffing off of concepts that I&rsquo;d encountered and wanted to discuss. It was very peaceful to be able to sit down, unencumbered by the prospect of having to grind through endless maths problems, and just write. I honestly felt like I did a lot of good thinking over that break - it was like all the noise finally quieted down, and my brain decided to overcompensate for not doing much concrete work by coming up with a lot of thoughts. I decided to transform some of those thoughtchains into articles, and some into personal notes for myself. Bottom line - I spent a lot of time reading, writing, and musing this break: it was very enjoyable, and a good mental rest before I was thrown back into term two.</p>
<p>One of the things that was on my mind near the start of break was, as usual, still schoolwork. After classes let out for exam period, I still had one assignment left: my English paper<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. This was something I&rsquo;d been working on for almost half the term, and was worth a rather significant 25% of our grade. Needless to say, I was stressed about it, even after I&rsquo;d finally sent it off and started studying for other things. But after the chaos of exams faded away, I was still thinking about some of the pain points I&rsquo;d experienced while writing that essay, and specifically about the role of time on my writing. I don&rsquo;t mean time as in the time crunch, though that was certainly fun - I mean how time impacted my the contents of my writing, how my essay went from an outline to an actual paper, and how it changed over that period of time. There were other English assignments for the same class where I followed a similar process - with each piece, I realized just how drastically they&rsquo;d changed with editing; how they&rsquo;d changed over time.</p>
<p>There&rsquo;s a lot of buzzwords surrounding tools for thought, especially those referencing software that claim to bring new mediums for thought, elevating the human consciousness to unlock some magical thinking powers that we didn&rsquo;t see possible. I&rsquo;m only joking, but I think there&rsquo;s opportunities to explore how time is a significant effect on writing and on thought. Building software to be more &lsquo;aware&rsquo;, in a sense, of time has the potential to drive new innovations in tools for thought.</p>
<p>This post is a collection of a few ideas that I came up with while reflecting on writing my essay, particularly focusing on the role of time, history, and perspective on content and expression.</p>
<h2 id="semantic-version-control">Semantic Version Control</h2>
<p>With writing, I usually like having some way to access the history of my files. With Word, I used to be able to enable some option to be able to access the version history - I think a friend recently mentioned this is somehow built directly into the Office 365 product now. However, I currently use Vim for most of my personal notetaking, and LibreOffice for more formal schoolwork. Both of these softwares have ways that I&rsquo;m aware of that could enable me to &lsquo;step through&rsquo; these files in time, sort of like a literary debugger. Within Vim, I could enable the infinite undo tree and be able to track changes that way. LibreOffice, on the other hand, there&rsquo;s a single backup that&rsquo;s on by default, but also a Versions feature that&rsquo;s somewhat reminiscent of source control like Git. Both of these methods are a bit overkill for me at this point - for my Vim writing, it&rsquo;s backed up to a Git repo that syncs automatically each day, and I usually don&rsquo;t update more frequently than that. For my formal schoolwork that I have to write up, I generally just copy paste my main points below any new drafts, or litter my editing with comments that refer back to older versions of the document.</p>
<p>An interesting use case that I found myself thinking about recently is what I&rsquo;d term semantic version control. Maybe &lsquo;semantic&rsquo; isn&rsquo;t a great description for it, but you&rsquo;ll see. My guiding question for this whole thing was &lsquo;why isn&rsquo;t it possible to keep file chunks separated, so I can Ctrl-Z changes in one part of my writing, but not others?&rsquo;.</p>
<p>My reference model of the typical version control software is the ubiquitous Git. As far as I can tell, it operates on line-based diffs, essentially comparing individual lines within files to see where new chunks have been added or removed<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. This works well for most (almost all?) software development, where it&rsquo;s not logical to have files represent any intermediate states in between code changes. If something is committed, we want it to reflect that update. But for writing, and projects with more semantic nuance, it&rsquo;s hard to preserve these &lsquo;in-between&rsquo; states. For example, let&rsquo;s say I have three paragraphs, and I&rsquo;m editing each one. Before I start working, I commit my changes, and when I&rsquo;m done, I also commit my changes. At the end of my revisions, maybe I&rsquo;ve decided that I don&rsquo;t like my changes in paragraph one. Fine - I can revert incremental changes, even in typical VCS like Git (with <code>git checkout -p</code>), but this process is a bit complicated. I&rsquo;ve only tried this once (with a non-binary file), and managing merge conflicts was a nightmare. Perhaps I did the entire thing completely wrong: we&rsquo;ll give it the benefit of the doubt. However, where Git starts breaking apart, or at least being very complicated for this workflow, is when you&rsquo;d be pulling parts of commits all over the commit tree. Maybe you even want to fast-forward a paragraph or two down in the tree and start working off those changes, but how can you apply commits from the future? That&rsquo;s a rhetorical question - I think with enough <code>git checkout</code>ing and <code>git rebase -i</code>ing, a system like this&rsquo;d be possible.</p>
<p>But think of a semantic version control system, where each little atomic piece of writing could be individually manipulated, with its own version history. One way to implement this would be a system where each paragraph (or each sentence, each word, each character, even!) was a unique document, in Git terms. While you could edit the entire thing in one piece, each sub-document, with some arbitrary level of smallest unit, would be able to have its own history and own undo tree. I&rsquo;m reminded of software like Roam and other block-based note-taking systems, which could ostensibly integrate individual history systems for each block. If I understand their system correctly, <a href="https://remnote.com/">RemNote</a> could also do something similar with its &rsquo;everything is a Rem&rsquo; model.</p>
<p>This ties into the post I previously wrote about <a href="https://kewbi.sh/blog/posts/211114/">metadata</a> - here, the aspect of metadata I&rsquo;d be looking to enrich is time. As it stands, the most time-related metadata available for most notetaking systems is the date created, and date last updated. History, another facet of metadata involved in the above system, is also usually not very <em>rich</em>. It can&rsquo;t be manipulated very easily, nor is it intended usually as anything more than a simple worst-case-scenario-only backup system. But in writing, time usually plays a pretty big role, at least in my thought processes. I felt this a lot this past term, especially in some of my writing classes, as I mentioned in the introduction. Sometimes I need to step away from an essay for a couple days, or to keep writing page after page of useless drivel to finally hone down my actual point. With current systems, I&rsquo;m not able to cherry-pick paragraphs from a couple paper versions ago, or use time, in a way, as best as I could be.</p>
<p>There are trade-offs for this model - a major one would be the insane amount of data storage needed to keep infinite undo trees. Even if the scope was limited to the last couple edits, having each individual atomic block have its own history would add many orders of magnitude of both code complexity and storage required. Power users likely have tens of thousands of blocks - storing diffs that may never be used for each of those is likely unfeasible. But I can dream, and I can come up with slightly shoddy systems to replicate behaviour like this with my current systems. As I mentioned, with LibreOffice, I first work on an outline, then copy that entire outline above itself, and work on drafting that into coherent language. Then, when I edit, I either use Track Changes, or if that gets too messy, just make notes of old sentences that might be useful with comments. It gets a bit messy sometimes<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>, but it works, and it&rsquo;s pretty satisfying to finally delete everything right before I hand in an essay.</p>
<h2 id="recency-bias">Recency Bias</h2>
<p>In terms of what I write about, one thing I&rsquo;ve noticed about the role of time is that whenever I write, there&rsquo;s a strong influence of wanting to write about what&rsquo;s currently on the top of my head. Over the past few blog posts, I think this&rsquo;s really manifested - I connect to recent conversations I&rsquo;ve had with friends more frequently than, let&rsquo;s say, something I read in a tech book a couple months ago. Maybe that&rsquo;s to say that the books weren&rsquo;t as interesting or as memorable as discussing with friends. Regardless, that memorability, or lack thereof, lends itself to a recency bias that tends to show up in what I write about, and what I draw on to support it.</p>
<p>One example of this, bringing it back to the English essay, was the way I chose the angle from which I decided to study it. We&rsquo;d read several books throughout the term, and I was starting to think about the essay somewhere through finishing the third. That third book happened to be a <a href="https://en.wikipedia.org/wiki/The_Curious_Incident_of_the_Dog_in_the_Night-Time_(play)">play adaptation</a> of <a href="https://en.wikipedia.org/wiki/The_Curious_Incident_of_the_Dog_in_the_Night-Time">Mark Haddon&rsquo;s <em>A Curious Incident of the Dog in the Night-Time</em></a>. I remember feeling like I really wanted to analyze this play - especially as I&rsquo;d read it before for other purposes. I think that was the recency bias kicking in, because I eventually settled on a different work, <a href="https://en.wikipedia.org/wiki/The_Best_We_Could_Do">Thi Bui&rsquo;s <em>The Best We Could Do</em>.</a>. Even though I&rsquo;d initially planned to study the role of memory in <em>TBWCD</em>, I ended up focusing on the themes of family, which happened to be what we were discussing while reading <em>A Curious Incident</em>. Perhaps this is just a coincidence, and maybe I really did end up analyzing Bui&rsquo;s depiction of family simply because that was what I found to be the most interesting and held the most potential for a unique reading. I can&rsquo;t help, however, noticing examples of this throughout the rest of my writing, and wondering if this is one of the strongest effects of time in sharing ideas.</p>
<p>This focus on fresher topics isn&rsquo;t a problem per se, but I&rsquo;ve realized one way to combat this recency would be to have a resurfacing mechanism that brought up old thoughts for you to process again. I think this is why systems like the original Zettelkasten, then workflows like Roam&rsquo;s or Notion&rsquo;s, hold so much promise and are so frequently used. With bidirectional backlinks and endless possibilities for you to stumble onto an old idea that you&rsquo;ll see anew, there&rsquo;s more serendipity at play here to rediscover thoughts. As well, the danger of forgetting ideas that weren&rsquo;t fleshed out as much is diminished with systems like this. Users can be gently reminded of them in due time, when they explore adjacent subjects. I think this is why so many people feel so attached to their personal knowledge management systems - they offer an assurance that their ideas won&rsquo;t be relegated to the bottom of the proverbial bucket. Write up a quick synopsis on this or that, link it and/or tag it up, and forget about it until you magically need it again. For me, I don&rsquo;t use typical PKM software, so I have to do this a bit more manually, but there&rsquo;s a certain sense of happy chance that happens when I go through old folders in search of something. With my system, I suppose this resurfacing is a lot more active rather than passive, but who knows - maybe that strengthens the ideas&rsquo; connections in my own brain.</p>
<h2 id="editing-is-writing">Editing is Writing</h2>
<p>On the other hand, I can also definitely see how time plays a role in editing of my writing as well. Of course, the point of editing is that you can take the literal time to improve your work, and see how it changes over time, but I think time also works to hone your ideas. I&rsquo;ve talked about my <a href="https://kewbi.sh/blog/posts/210516/">outlining system</a> before, but again, going back to the English essay, I&rsquo;ve noticed how my ideas significantly changed over time and through editing. At first, I had ideas all over the place - tying in memory, identity, race, culture, second-hand experiences through a very wobbly thesis. When I first tried to edit things down, I blindly picked the subcategory that had the most sub-points and that I thought would be most relevant (second-hand memory) and chopped up my essay to deal with only that. My thesis got no stronger, and my writing was frankly even more messy - there just weren&rsquo;t that many examples that I could tie into a single overarching statement. But as time went on, as I edited more and pulled in different examples, I somehow had several eureka moments, and I finally realized what it was that I was trying to say.</p>
<p>Part of this comes down to the feeling that sometimes I have an idea, but I can&rsquo;t express it, and I don&rsquo;t even know what it is. It took time (a lot of it, might I add) to distill my analysis down to one line of argument. The thing is, all of my examples <em>had</em> been connected by a central theme (family and identity), but without taking loads of time to write and rewrite, I couldn&rsquo;t come up with the words to put it down, nor the realization of what I actually wanted to write about. When there&rsquo;s a lot of ideas swirling around in my head, it takes time for me to compartmentalize and learn to actually communicate them. In Ahren&rsquo;s famous <a href="https://takesmartnotes.com/"><em>Take Smart Notes</em></a>, he writes that &lsquo;writing is the thinking&rsquo;. My version, I guess, is &rsquo;editing is the writing&rsquo;. My point is that throughout coming up with an idea, and refining that idea, there&rsquo;s always a role of time: of iteration and of interconnection.</p>
<h2 id="conclusion">Conclusion</h2>
<p>I&rsquo;m happy to say, I did much better on the English essay than I expected, and hey, writing it even sparked a long chain of thoughts about the role of time in writing, which turned into yet more writing! Even as I edit this article, I&rsquo;m realizing it is itself a result of the recency bias, and that I went through much the same process of turning a messy brain dump into something slightly more coherent while writing it. I think exploring how time shapes ideas, and how it lets them mull and grow and change completely, is an interesting aspect of thought to consider with regards to the whole tools for thought phenomenon.</p>
<p>While, as I mentioned in the arrangement section, most Zettelkasten-like software already brings some backlinking feature, I&rsquo;d like to see software that plays better with time, history, and resurfacing context. For the semantic version control system I proposed, this might be possible with a very deeply-nested Git-like model. There&rsquo;re possible applications of NLP in doing better backlinking and idea retrieval, in a more serendipitous way. As well, there might be other possibilities in NLP in summarizing thoughts and suggesting potential &lsquo;what was it that you really wanted to say&rsquo;, to help users distill their messages. Those are just a few of the posible ways more time-sensitive tools for thought could be built - and I might start looking into proofs-of-concepts for some of these.</p>
<p>I wrote this post while procrastinating on all the work that&rsquo;s hit with the start of Term 2 here at UBC. It&rsquo;s been kind of crazy the last week - I feel like through the past 5 days of classes I&rsquo;ve done the equivalent of 10+ days of break-time work. It&rsquo;s felt slightly overwhelming - there&rsquo;s been a lot of information and syllabi and course structures thrown at me at once, but we&rsquo;ll get through it. I&rsquo;m enjoying all my classes so far, and all my professors have been super wholesome. We&rsquo;ll see what this new term brings, but my goal is to reach out more - to friends, to professors, to peers. We&rsquo;ll see how that goes.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Interestingly enough, all the parts that I thought my TA would criticize for weak analysis were the parts that they liked, and the introduction, something I was pretty happy with, ended up being the main point of commentary.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Binary files notwithstanding, which according to the <a href="https://git-scm.com/docs/git-diff">git-diff documentation</a> use something like 64-byte chunks.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Cue the <a href="https://www.reddit.com/r/UBC/comments/rpxyo1/essayfinalnomoreeditsv2/">essayfinalnomoreeditsv2.docx</a> memes.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>Least Common Denominators</title>
      <link>https://kewbi.sh/blog/posts/211219/</link>
      <pubDate>19 Dec 2021</pubDate>
      <author>Emilie Ma (Kewbish)</author>
      <description>On an atomicized software culture.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>A while ago, I finally bit the bullet and picked up a copy of <a href="https://www.calnewport.com/books/digital-minimalism/"><em>Digital Minimalism</em></a> at the library. It&rsquo;s a hugely famous book by Cal Newport, a popular (and for decently good reason) productivity author and CS professor. If you haven&rsquo;t heard of his philosophies surrounding technology usage and mindful technologism, you might want to go take a look at his work yourself. <em>Digital Minimalism</em> specifically focuses about social media and connecting with others virtually. A quick summary of <em>Digital Minimalism</em>: social media is often used for things that aren&rsquo;t useful, but &lsquo;could be&rsquo; - and that &lsquo;could&rsquo; is what hooks us to our devices in spite of ostensibly more productive and meaningful activities. The book centers around how to be more intentional about your digital footprint, media consumption, and online interactions with others.</p>
<p>At a very high level of abstraction, Newport discusses how we can make the tools we use less noisy, and to adapt them to work for us, not the other way around. You might recognize this phrasing, for good reason - it&rsquo;s often used when discussing tools for thought. That&rsquo;s what I wish to think about in this article. His philosophy is applicable not only to online connections and the <a href="https://kewbi.sh/blog/posts/201108/">endless maze of pings</a>, but with software as a whole as well. I&rsquo;ve discussed <a href="https://kewbi.sh/blog/posts/210124/">hyperpersonalized</a> software quite a few times on this blog, but Newport distills many of the ideas that orbit around that sort of artisan-created, artisan-facilitating software. However, while his advice centers around the <em>use</em> of social media and tools, I think there&rsquo;s a lot to be said about how we can <em>create</em> software in a way that pushes more worthwhile consumption and creation.</p>
<h2 id="but-what-if">But What If?</h2>
<p>In the book, Newport discusses a sort of &lsquo;but what if?&rsquo; logic that people tried to apply in order to push him towards getting on social media (which he had not, and has not done). He recalls that people often told him that he should consider joining social platforms, and when he asked why, they sort of universally came up with a &lsquo;but what if it&rsquo;s useful?&rsquo;, and a &lsquo;I&rsquo;m sure you&rsquo;ll discover something on [XYZ] that you&rsquo;ll find really meaningful&rsquo;.</p>
<p>I&rsquo;ve felt this way a lot myself. Two big examples of this that come to mind are Twitter and Notion / Roam / Obsidian. With Twitter, a lot of people in my friend circle&rsquo;ve hinted at the magical wonders of Twitter, and let me know that I&rsquo;d likely like it there. This is a bit tricky to unpack, because I know I would find it useful. I really do want to immerse myself in communities of people who are doing things much cooler than what I&rsquo;m working on, and be inspired<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. On Twitter, there&rsquo;s a sense of having a space to share my thoughts and ideas with a further group of people that I&rsquo;d really value. But on the other hand, I have a feeling I&rsquo;ll be constantly pulled into the discussion there. Right now, I do consume a decent bit of Twitter material with my <a href="https://kewbi.sh/blog/posts/210110/">Nitter</a> setup. It&rsquo;s sort of like having write lock - I can &lsquo;read&rsquo; from these systems fine for now, so I&rsquo;m trying to wean myself off the feeling I want to contribute directly there too yet. As Newport points out, I don&rsquo;t know what I&rsquo;d like to get out of Twitter. There&rsquo;s a bit of a circular dependency: without having been on the platform, I can&rsquo;t gauge what value it&rsquo;d return for me, and so I don&rsquo;t use it.</p>
<p>The same goes for Notion or other notetaking software. Each system has its own benefits: Notion, its centralizability; Roam, its database and linking model; Obsidian, its file-based focus. With notetaking tools, I feel less of what Newport describes, less of the &lsquo;what-if&rsquo; effect. I think it&rsquo;s because I&rsquo;ve felt the inertia of wanting to stay with my own systems, and because I&rsquo;m passionate about and know what I&rsquo;m doing with my knowledge management workflows, at least for now. Notion, for examples, brings a lot of maintenance and workaround effort - sure, I&rsquo;ve seen people organize their whole lives in it, but for their aesthetic setups they have to bring in custom icons and lots of layered filters. Could the same thing not be accomplished with separate notetaking, calendar, finance management, and habit apps? This isn&rsquo;t to say I don&rsquo;t have the same issues in the terminal, but I feel like I&rsquo;ve learned to cut down on it and recognize when there are better tools, cutting down on my &lsquo;what-if&rsquo; effect.</p>
<p>I recently had a conversation with a friend about <a href="https://github.com/Eeems/oxide">Remarkable launchers</a> or something - he was discussing with another friend why they&rsquo;d want to add a launcher to their tablet. Someone asked him what the point of the launcher was, and what he&rsquo;d personally used it for. He couldn&rsquo;t come up with any concrete examples (which is fine), but I jumped in and pointed this out. He was recommending a software without having an actual real use case for it, with the same &lsquo;but what-if&rsquo; thinking that seems to be so pervasive when working with software workflows.</p>
<h2 id="least-common-denominators">Least Common Denominators</h2>
<p>All this is to say that these recommendations are definitely valid, but in a different way. In <em>Digital Minimalism</em>, Newport specifically advocates for digging deeper into why you want to use a specific social media platform - for example, he notes a discussion he had with a client who realized he wanted to use Instagram to keep connections going with his childhood friends and see visually what they were up to. But Newport pushed him further, and asked if Instagram was the most productive and efficient way to engender that connection. Text messages and SMS photos would accomplish virtually the same thing, cutting out the distractions and even encouraging more meaningful discussion over thoughtless &rsquo;like&rsquo; interactions. And though messages were good, an in-person visit would add even more! Newport guided the client to go from what they wanted (connection) to a less distracting and more personable way to achieve the same thing (visits and text messages). This isn&rsquo;t applicable to all uses of social media and software, but there&rsquo;s an interesting idea I want to pull out in there.</p>
<p>What if we created software in a way such that we could get to exactly wanted, without all the other distraction and features that aren&rsquo;t useful to us. I thought of the term least-common-denominator software for this, trying to capture the idea of having the least possible component to do what you wanted while being composable and not having other features that you weren&rsquo;t interested in. Users can then select the exact feature set that matters to them, instead of having to settle for something that does too many things without optimizing for their own use-case. This form of software would break components and systems down into their least common denominator bits - atomic scripts that can do their job and just that. In my Twitter example, I&rsquo;d retain some ability to interact with people that I Twitter-simp over, without the noise and addiction and spam that&rsquo;s also on there. With Notion, I&rsquo;d be able to have a centralized source of truth database while having multiple views for everything, but also avoiding the performance issues and annoying workarounds people often have to use. There&rsquo;s got to be some reasonable limit for this granularity and control, but beyond that, why can&rsquo;t we have more composable systems?</p>
<p>For the developers among you, this&rsquo;d sort of reflect an extended view of the Unix philosophy applied to modern-day software systems. Do one thing, and do it well: so the Unix philosophy reads. Currently, most software is focused on doing multiple things, and doing them all decently. I hate to rag on Notion so often, but it&rsquo;s the perfect example - infinite capabilities, but each complicated system requiring many workarounds and hours of setup. Least-common-denominator software would optimize for its one use case and for sending its output to other tools. What I envision is something like the complicated multi-command pipes in bash, with commands passing data back and forth, only with a much better user interface.</p>
<p>Another idea that I&rsquo;d like to touch on is that least-common-denominator software wouldn&rsquo;t be like SaaSes or the constantly updating tools we use today. Of course, they&rsquo;d be maintained, and bug-fixes would pop up when needed, but I think having more atomicized software incentivizes it to be <em>done</em> much more quickly. Nowadays, if a product or tool or library is declared <em>done</em>, with work remaining only for issues and such, people tend to mentally mark it as deprecated and move onto the next shiny thing. But because these atomicized components wouldn&rsquo;t have a lot of interaction to do, there wouldn&rsquo;t be an endless backlog of features to work through, nor an increased surface area for bugs to pop up - again, they&rsquo;d do one thing, and do it well.</p>
<p>I won&rsquo;t go too far into what this sort of software system would look like, mostly because I haven&rsquo;t really figured that out. As to what technologies might be used: I&rsquo;ve mentioned <a href="https://www.inkandswitch.com/cambria/">Cambria</a> in a previous post about moving metadata around and translating data between sources. Tools like this to automate and facilitate data manipulation&rsquo;d likely be a central factor of this type of system, as well as open-source and transparent ways to organize and host data.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Though we can dream, I also don&rsquo;t realistically think a large-scale shift to this sort of minimal yet composable software&rsquo;ll get very far. For one, companies will certainly not stick to their core products - each big tech company:tm: has its countless failed / attempted / halfhearted ventures out into side businesses. Why wouldn&rsquo;t they want to monopolize user&rsquo;s time, and find new ways to appeal to their software / tool inertia to keep them with their existing software? In Newport&rsquo;s book, he writes that it&rsquo;s not very profitable for businesses to reduce their distractions. In this case, it&rsquo;s not very profitable to keep things minimalist and stop mashing new features into the app. As I&rsquo;ve mentioned before, standardization and the data processing aspects of these components would also get pretty complicated, and it&rsquo;s not clear what such a distributed model of software would look like without having a more open culture of software usage, where people can control their own tools and information.</p>
<p>I used to laugh at my friends when they&rsquo;d talk about whatever apps or whatever aspect of their systems being &ldquo;bloat&rdquo;<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, but I&rsquo;ve started to appreciate why they&rsquo;re pointing it out. It&rsquo;s a pattern (I won&rsquo;t say problem) that&rsquo;s pretty widespread across software as we use it today, and I&rsquo;d like to think that reinventing how we work with tools and software could help change that.</p>
<p>I&rsquo;m writing this during finals season, in between studying for separate exams and cramming practice papers. (Well, now that I&rsquo;m editing this conclusion, I&rsquo;ve got through nearly all of my exams already, but oh well.) I had a weird realization a month and a half before the end of term that it was <em>a month and a half before the end of term</em>, and I started trying to work on my study guides and come up with a plan of attack. I&rsquo;m glad I started early - I don&rsquo;t think I properly realized just how much there was to do, nor how much time it&rsquo;d take to get through all the practice materials I&rsquo;d want to tackle. I backloaded things weirdly, having a lot of my review and study sessions planned just before finals week started, and now I&rsquo;m slightly suffering my way through all of it. But lessons learned for next time, and I got through most things fine.</p>
<p>I also recently had my first couple of software engineering intern interviews ever, which was both really exciting and incredibly nerve-wracking. (If anyone working at a company I&rsquo;m interviewing at right now reads this, I assure you that I&rsquo;m probably overly enthuastic about the possibilities of working there! I assure you I&rsquo;m not normally this&hellip;rambly about software.) It&rsquo;s kind of scary that an actual company is considering taking me on to do work for them, and that I could potentially be making a very real impact to products. Things seem to be going well, but the sunk-cost is kind of kicking in, and with every round, I keep getting more and more nervous that they&rsquo;ll just decide I&rsquo;m not a good fit and not consider me. But such is life, and I&rsquo;m looking forward to my next couple rounds. Fingers crossed!</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>One friend in particular describes it as an online <a href="https://kk.org/thetechnium/scenius-or-comm/">scenius</a>. We&rsquo;ve (me and this particular group of friends) talked a decent bit about finding IRL sceniuses (ironically on an online chat). I guess this pull towards online groups stems from the fact that I don&rsquo;t really have that experience in-person right now. Oh well, perhaps I&rsquo;ll stumble into some quirky CS friend groups soon, but for now, I do have my digital scenius influences (you know who you are).&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>If you know, you know. I can literally hear the person I&rsquo;m thinking of saying this as I write it.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>Metadata and You</title>
      <link>https://kewbi.sh/blog/posts/211114/</link>
      <pubDate>14 Nov 2021</pubDate>
      <author>Emilie Ma (Kewbish)</author>
      <description>On metadata, its dimensions, and its impacts.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Recently, I&rsquo;ve started pondering the tools-for-thought phenomena again, but perhaps in a more meta way. From what I&rsquo;ve noticed on Twitter (at least for the short amount of time that I&rsquo;ve been checking content from it), there&rsquo;s been a trend over the past couple years with the explosion of popularity of apps like <a href="https://roamresearch.com/">Roam Research</a>, <a href="https://obsidian.md/">Obsidian</a>, <a href="https://logseq.com/">Logseq</a>, <a href="https://www.notion.so/">Notion</a>, <a href="https://workflowy.com/b/">Workflowy</a>, <a href="https://ankiweb.net/">Anki</a>, or the myriad of other apps that solve a major pain point for a lot of people: the management, and meta-management of knowledge. Here, I&rsquo;d like to focus on the meta-management aspect of this a bit. I&rsquo;ve noticed a lot of the tools rapidly gaining popularity and (appear to be) pushing the forefront of human-computing interfaces have a shared trait in common: they&rsquo;re all in essence ways to manipulate metadata.</p>
<p>Metadata and knowledge can sometimes blend together, likely a result of metadata-deficient systems where a majority of the knowledge was spent on cross-labelling itself in lieu of proper metadata. When I reference metadata, I&rsquo;m talking about not the juicy knowledge itself, but the trail of breadcrumbs leading to it - the labels, the categorizations, and the aliases. Things like &rsquo;time last modified&rsquo;, &rsquo;time created&rsquo;<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, and other custom labels all fall under the umbrella of metadata. There&rsquo;re also actions associated with that metadata, like advanced regex-based searching, filtering by tag or label, alternate &lsquo;slices&rsquo; of data. The apps I&rsquo;ve mentioned above all extend or have significantly innovated and built features connected to metadata. Roam, likely one of the more popular examples, has their famous tag and linking system. Click on a little hashtagged bit of context, and you&rsquo;re teleported to a page full of other mentions of the same topic - the very alternate &lsquo;slicing&rsquo; of data I&rsquo;ve just mentioned.</p>
<p>Metadata is the apparent solution to the categorization problem, and what appeals so powerfully to the part of the human brain that craves neat little boxes. The issue with software is that it&rsquo;s generally not <a href="https://kewbi.sh/blog/posts/210124/">hyperpersonalizable</a>. Settings can&rsquo;t be tweaked in every single little way that all the different workflows and people using the app demand - core feature development would stall to a standstill, and devs would spend more time implementing personalizations than concrete tools. But people still want to create their own little classifications, and make the app <em>theirs</em>. Metadata here is the fix - at least, for most people. Some users might feel overwhelmed (look at the hours of &lsquo;get started with Notion&rsquo; videos and the pages of questions about certain tweaks), but on average, I&rsquo;d say that most people generally approve of this ability. Less personalization-focused people can simply overlook the metadata, or implement just the core basics. Power users, on the other hand, can use custom fields and metadata to their heart&rsquo;s content - just look at the magic people pull off with Notion databases.</p>
<p>I&rsquo;ve been thinking a lot recently about tools for thought, and their first principles. What are the majority of lauded apps rooted in? What trends and tendances is society looking for in popular apps? In this post, I&rsquo;d like to share some of my realizations (well, that&rsquo;s a strong word) about the space and the role metadata plays in our knowledge interactions.</p>
<h2 id="manipulating-the-meta">Manipulating the Meta</h2>
<p>Apps do some combination of three key things with metadata: write it, connect it, and allow it to be synthesized and searched for and molded. For example, take something like <a href="http://trello.com/">Trello</a>. You can tag and label your to-dos (writing metadata), link them to one another across boards (connecting it), and have high level overviews and statistics about your project (synthesis). This last bit about super-synthesis is important, as it creates a new level of meta. With that, there&rsquo;s a possibility to invest in linking up metadata about the metadata, and that&rsquo;s where the potentials for new tools and new innovations lies.</p>
<p>In a sense, the true innovation factor of these popular apps lies in the meta-levels that they provide. Roam and Notion and whatnot aren&rsquo;t the power tools they are because of some revolutionary new way to interact with data itself: it&rsquo;s the refreshing levels of interactions they offer with the metadata instead that boosts the quality of life when working with the knowledge and data itself. Most tools offer a good baseline of data management, and some level of interconnectedness and tagging and such, but when software offers a new way to interact with the metadata, that&rsquo;s when you get interesting workflows and environments. Roam, Notion, and other apps with a block or bullet model appeal to people because of their fine-grained connections, something that was lacking in the common sterilized, document-driven approach.</p>
<p>A good example of this is what inspired me to write this whole blog post: the <a href="https://roamjs.com/extensions/discourse-graph">Discourse Graph</a> extension available in Roam. I don&rsquo;t use Roam, nor plan to daily drive it in the near future, but there&rsquo;s something about this extension in particular that really tempted me to switch. It works with your existing Roam graph to provide a formalized note model, composed of questions, evidence, claims, conclusions, and all the connections in between. It&rsquo;s suited well for a research ecosystem, with tools to denote that this note &lsquo;supports&rsquo; or &lsquo;refutes&rsquo; or &lsquo;weakly correlates&rsquo; with another. This in itself is a powerful metadata model - the codified structure takes away decision paralysis, and creates a very regimental and logical order of graphs. However, where this extension goes above and beyond is its playground feature, where you can query your model for &lsquo;all evidence that supports X and refutes Y&rsquo;, for example, and find &lsquo;all claims that connect to such and such question&rsquo;. That&rsquo;s a truly new model of interaction with metadata, and a great example of how having a structure and tools to manipulate metadata can lead to connecting together key insights in your knowledge.</p>
<h2 id="data-dimensions">Data Dimensions</h2>
<p>Besides having tools built into software to analyze metadata, it&rsquo;s also important to allow users to implement their metadata to fit their workflow. People&rsquo;ll always want more dimensions to categorize and subcategorize and fit all their data into - I remember when I was trying to implement Trello into my workflow, I was pretty frustrated at the lack of sub-bullets and sub-tasks, and had to make do with a very scuffed checklist model instead. The same goes with nesting - Workflowy&rsquo;s popular for a lot of reasons, but one is the infinite hierarchy that it provides. That infinity is also something interesting to note: as long as something&rsquo;s not unlimited (to a reasonable depth), a subset of users will feel like they&rsquo;re hitting a wall with what the software can help them do. Whatever depth of metadata is available, unless unlimited, will feel like some construct to at least some people, if not a significant portion. This creates essentially infinite &lsquo;axes of belonging&rsquo;, or categories with which to split data up. Axes of belonging are like possible views and queries into the data, and if the number of axes can be maximized, having all these new variables and possibilities makes software ostensibly more powerful for those power users.</p>
<p>But there&rsquo;s still a fine balance to walk between too complicated with too many dimensions and axes of data, and too few. Excel, arguably perhaps one of the original tools for thought and innovators in the metadata space, does this quite well. The spreadsheet and table interface is immediately intuitive for many, but hidden deep into menus and tabs that the average user will never explore, are the hidden gems that probably makes someone&rsquo;s workflow click. There are infinite ways to associate metadata to a cell through other cells or formatting or conditional formulas, and an infinite canvas full of columns and rows to stack all your data in. This is, I think, how people think connections work best - through categories, expressed here in table associations. Maybe it&rsquo;s not the best and most efficient way to work, because sometimes constraints are what you need, after all. But with its many metadata dimensions and built-in ways to explore data, Excel feels so open, and that&rsquo;s a trait in software that I think people are almost universally drawn to.</p>
<h2 id="standardizing-might-will-fail">Standardizing Might (Will?) Fail</h2>
<p>So far, I&rsquo;ve discussed the two main elements of what makes metadata integration so appealing. However, to get multiple people on the same page, all benefiting from the same metadata innovations, tools will need to have some shared standard, or some sort of schema. Schemas are inherently limiting though, unless they&rsquo;re explicitly <em>un</em>-limiting. But in that case, the point of standardization and extension is a bit moot. To some extent, I think you can get schemas to be flexible and agree, but there&rsquo;s a fine line between too much structure and the sort of spec that goes &rsquo;literally any field can be literally anything, go have fun&rsquo;. How are new features proposed and added across so many apps? Will a central standard have to be patched? How can we make it backwards compatible without &lsquo;de-dimensionalizing&rsquo; or flattening metadata?</p>
<p>There&rsquo;s a <a href="https://xkcd.com/927/">relevant XKCD</a>, as there always is, but there&rsquo;s also a couple relevant tools that might work to bridge the gap between different tools. <a href="https://www.inkandswitch.com/cambria/">Cambria</a> by Geoffrey Litt (and Peter Van Hardenberg and Orion Henry) is a tool to move data between schemas. In essence, it&rsquo;s a Google Translate for tools for thought, where &rsquo;lens&rsquo; can be defined to rename fields, convert between datatypes, and extract certain labels. It partially solves the issue of differing standards, as apps can define ways to translate and transmute metadata between each other. This does assume that software companies are willing to cooperate with each other (a stretch), or that there&rsquo;ll be dedicated users who drive this system forward (definitely more likely). Something like this doesn&rsquo;t solve the issue of how rich interactions and media based on metadata that&rsquo;s not quite supported should resolve, but it&rsquo;s a great step to linking all these tools and towards a more metadata-friendly future<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<h2 id="conclusion">Conclusion</h2>
<p>For software being developed today, I think there are some decently clear trends in users becoming more tech-savvy, and in power users craving even more functionality. In the new field of knowledge work, maximum efficiency is glamorized and lauded. While that has its own caveats that I won&rsquo;t get into now, it&rsquo;s key to consider that, in general, people seem to be trending towards wanting more options and more connectivity through the meta. It&rsquo;s taken me this long to realize that I&rsquo;ve essentially written a SEO boost for the company-that-must-not-be-named, but oh well. We&rsquo;ll roll with it, because that appears to be what the zeitgeist is moving towards: a future of connectivity and linking<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. Maybe in this case, I&rsquo;m thinking of it more in terms of knowledge management than magical VR metaverses, but the case in point holds.</p>
<p>In other news, I&rsquo;ve been up to more work with <a href="https://fullstackopen.com/">Full Stack Open</a>, and I&rsquo;ve also tried to start the <a href="https://cses.fi/problemset/">CSES problem set</a> while bodging my way through C++. It&rsquo;s been a while since I&rsquo;ve felt this beginner - it took me a solid hour and half to write a basic <a href="https://en.wikipedia.org/wiki/Collatz_conjecture">Collatz sequence</a> simulator. Granted most of that was looking up syntax and tweaking a basic Vim setup, but it was certainly a knock to my Python tendencies. CSES is kind of one of those things that&rsquo;s good for you, and somewhat fun, but only in small doses, not when trying to grind certain amounts of problems per session or whatever. It&rsquo;s been a good brainteaser so far, so I think I might enjoy (very) small amounts of it in the future. I also feel like I&rsquo;m starting to spread myself too thin - I&rsquo;ve started doing some stuff at the <a href="https://ubccsss.org/">UBC CSSS</a> as well. I&rsquo;m halfway certain it&rsquo;s something to do with being immersed in an entirely new world of uni, and surrounding myself with people who all seem much more experienced and cooler than I am. It&rsquo;s a good ego check, but it&rsquo;s also been inspiring me to explore and look into a lot of new things. Halfway a good thing (exploring new areas of CS, doing fun things), halfway a not-so-great thing (occasionally self-overwhelm and loads of imposter syndrome), but I think I&rsquo;ll be able to manage, so we&rsquo;ll see what happens.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>There&rsquo;s also a lot to say about the fact that the most common, and most baseline metadata is often that of time. It&rsquo;s something that&rsquo;s easy to connect to a physical moment, a decently useful bit of information for most contexts, and straightforward to record in server logs. It&rsquo;s also an universal constant - people across different cultures can&rsquo;t interpret time differently, nor can users argue over the best way to tweak it. It&rsquo;s just time. (More on this maybe in another post soon - I&rsquo;ve got some ideas brewing that I&rsquo;d like to investigate further.)&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>It&rsquo;d be interesting if something like this got centralized / listed into some graph. It&rsquo;d then be a link full of linked ways to link (and manipulate) links in linking software! (Infinite metadata recursion, and it&rsquo;s turtles all the way down.)&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Besides the unintentional reference to Facebook, the title of this post is a cute nod to <a href="https://thesephist.com">Thesephist&rsquo;s</a> project <a href="https://thesephist.com/you/">&lsquo;a piano and you&rsquo;</a>.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>Starting Anki</title>
      <link>https://kewbi.sh/blog/posts/211031/</link>
      <pubDate>31 Oct 2021</pubDate>
      <author>Emilie Ma (Kewbish)</author>
      <description>On proper SRS, GTK hacks, and the Leitner system.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Something that I&rsquo;ve realized throughout my first month and a half at university, is the sheer volume of information and responsibility placed on a student&rsquo;s shoulders. I love it. Being able to manage myself and have full(-ish) control of my courses, my performance, and the rest of my life has been great. I like being immersed in content that I truly don&rsquo;t understand, and that challenges me a bit to actually understand and make sense of the material. However, there&rsquo;s a balance: I like that there&rsquo;s exciting and decently interesting information, but it&rsquo;s, of course, annoying that there&rsquo;s a lot of it. I&rsquo;ve had to, and am still in the process of, figure out how to get through all of this material, and get through it decently well. My courses are very spread out in terms of specialization, and some are quite memorization focused. Improving and optimizing my recall and comprehension are now starting to become interests and priorities of mine.</p>
<p>Well, it&rsquo;s Halloween, and I don&rsquo;t know how spooky flashcards and memory management are, but here I am to discuss <a href="https://en.wikipedia.org/wiki/Spaced_repetition">spaced repetition software</a> and <a href="https://apps.ankiweb.net/">Anki</a> in particular. (I won&rsquo;t go into what these are right now, but go ahead and take a look at those resources if you&rsquo;re interested.) I&rsquo;ve been dabbling with <a href="https://kewbi.sh/blog/posts/201025/">a variation of SRS</a> for a while now, and have recently even been working on a <a href="https://github.com/kewbish/liberty">free-response version of the philosophy</a>. I&rsquo;ve tried both <a href="https://quizlet.com">Quizlet</a> and terminal UI versions of flashcard apps, like <a href="https://github.com/proycon/vocage">vocage</a> and <a href="https://github.com/Yvee1/hascard">hascard</a>. I&rsquo;ve known about Anki for a long while now, but I&rsquo;ve never really looked into it. I&rsquo;d always discounted their SuperMemo2 algorithm, and thought its applications didn&rsquo;t overlap with my needs.</p>
<p>However, what finally pushed me over the edge was taking ATSC113 at UBC. It&rsquo;s a course about atmospheric sciences, and goes into the weather in relation to flying, snow sports, and sailing. It&rsquo;s a course with lots of vocabulary and concepts that I&rsquo;d never thought about or learned about before: plenty of memorization heavy work in areas that I&rsquo;m completely unfamiliar with. I&rsquo;d tried to review notes on a regular basis, and made my own flashcards in an attempt to test myself. However, I kept needing to schedule review sessions and assess my level of comprehension myself, which introduced a lot of mental friction. I knew that I was probably going to start looking for something new to try, so I decided to be proactive and take a look at Anki.</p>
<p>When I started looking into Anki, I was initially a bit turned away by the aesthetic. It&rsquo;s very utilitarian, and I hadn&rsquo;t really considered it as it looked so archaic and decidedly unaesthetic. Eventually, the power outweighed the cons. Anki abstracts away all the meta-work that I have to do, so I don&rsquo;t have to worry about balancing my cards, or about the Leitner system. I figured out how to theme and make Anki look pretty and shiny anyways, so that&rsquo;s fine. What matters is that I&rsquo;ve decided that Anki is a good experiment to try, at least for the time being, and that it&rsquo;s been quite comfortable and helpful to use.</p>
<p>In this post, I&rsquo;d like to talk about a couple SRS-related things I&rsquo;ve been thinking about at the moment. This might continue into a series as I continue to learn my way around Anki, but for now, here&rsquo;re my thoughts.</p>
<h2 id="the-glamourization-of-srs">The Glamourization of SRS</h2>
<p>I&rsquo;d like to inject a short interlude about SRS and the SRS community in general here - I think there&rsquo;s a lot of comparisons to be drawn here between Anki and #roamcult, and maybe the knowledge management community too. There&rsquo;s a lot of similarly passionate people, and people who&rsquo;ve kind of attached a large part of their identity to the app. There&rsquo;s absolutely nothing wrong with it - I guess people have an intrinsic desire to know and particularly magically know all the things they want to do. Anki is a bit of a stopgap to the be-all and end-all of understanding, and I think it has something to do with the idealization and romanticization of knowledge in general. Maybe this is because I&rsquo;ve been spending a decent amount of time in circles where discussing knowledge management is a normal and interesting thing to do, but I see that people keep trying to reinvent themselves and keep searching for the one silver bullet to their knowledge needs. And I suppose I&rsquo;ve succumbed to that a bit as well - look at my Zettelkasten series and the amount of time I spend tweaking my various configs in an attempt to become that student. However, I guess I&rsquo;ll just leave this in here - SRS is lightly glamourized. There&rsquo;s nothing wrong with that, but in my opinion, it is. Perhaps my opinion as to the magnitude of glorification will change as time goes on, and I, too, become An Anki User, but we&rsquo;ll see.</p>
<h2 id="a-diy-night-mode">A DIY Night Mode</h2>
<p>Something interesting I&rsquo;d like to also touch on is getting themes to work properly in Anki. One of the main issues I ran into when trying to set Anki up was getting my GTK theme to work with Anki night mode. I currently use <a href="https://github.com/ddnexus/equilux-theme">Equilux</a> as my GTK theme, which I really like for its flat grey design and nice integration with my boring, monochrome aesthetic. Anki, however, works on QT, which I&rsquo;m not entirely familiar with, but it&rsquo;s a different theming and development engine than GTK. Unfortunately, Anki night mode by default doesn&rsquo;t take the GTK theme as a QT theme, so I apparently had to do some <code>/etc/environment</code> magic to set <code>QT_QPA_PLATFORMTHEME</code> to <code>qt5ct</code> and within that, <code>gtk2</code>. Spoiler alert: Don&rsquo;t bother looking into that, because if your GTK theme is already a dark mode and you&rsquo;d prefer a night mode within Anki, it won&rsquo;t work out. It <em>does</em> work with light mode, but then the entire reviewing interface is by default light and blinding.</p>
<p>What happens is that <a href="https://forums.ankiweb.net/t/toggling-night-mode-appears-to-change-qt-gtk-theme/14404">Anki uses their own QT theme</a> when night mode is toggled, since &ldquo;not all themes will work correctly with dark colors&rdquo;. Entirely fair, but that meant I had to do some serious CSS styling edits in order to simulate night mode within &rsquo;light mode&rsquo; Anki. To save you a lot of work, my edits to apply night mode colours (lightly edited to use Equilux colours where appropriate) to light mode Anki are available on my dotfiles, at <a href="https://github.com/kewbish/dotfiles">kewbish/dotfiles</a>. The key directories to look in are <code>/usr/lib/python3.9/site-packages/aqt/data/web/</code>, and <code>~/.local/share/Anki2/addons21</code>, if you want to edit any addons. This might differ due to installation method (I used <code>anki-bin</code> from the AUR), but this is what worked for me.</p>
<p>A more universal fix is creating an addon in the <code>~/.local/share/Anki2/addons21</code> directory, which is as easy as making a directory and creating an <code>__init__.py</code>. Alternatively, edit an existing addon. As long as you include:</p>
<pre tabindex="0"><code>from aqt import mw
from PyQt5.Qt import QStyleFactory

mw.app.setStyle(QStyleFactory.create(&#34;gtk2&#34;))
</code></pre><p>somewhere, you&rsquo;ll correctly force the GTK theme into any theme of Anki. This way, you can switch to dark mode within the Anki settings, as well as using a proper GTK theme that integrates with the rest of your desktop environment.</p>
<h2 id="my-settings">My Settings</h2>
<p>I don&rsquo;t think I really did much research as to what settings were idea for use with Anki - my config at the moment is some mix of <a href="https://chuff.wordpress.com/2018/01/08/article-how-to-use-anki-as-a-leitner-box-game/">this Anki Leitner setup</a> and the <a href="https://refold.la/roadmap/stage-1/a/anki-setup#Low-key-Anki">Lowkey Anki setup</a>. The two things I knew going into configuration was that I somewhat unconventionally wanted to partially integrate the <a href="https://ncase.me/remember/">Leitner system</a> in my learning steps, and that I wanted a Quizlet-like pass / fail option. Yes, people have gone on about how you should just trust the algorithm (à la the natural recursion), and how it&rsquo;s not really that much cognitive friction and that it really helps with the ease settings and how cards are tuned to your level of memory. But I don&rsquo;t like it - I now have to build a mental model of what differentiates hard from good cards, and likewise for good / easy, again / hard. This might be something I revisit later on in my SRS journey, but I&rsquo;d rather keep things simple and close to the softwares I&rsquo;ve used in the past for now.</p>
<p>I have my settings at 20 new cards a day - a bit much for now, but I&rsquo;d like to quickly get back up to pace with some of my class cards. I&rsquo;ve set my learning steps to <code>1d 2d 4d 8d 16d 32d</code>, which might seem inordinately long for the quick <code>5m 25m 1440m</code> people, but I&rsquo;ve found the Leitner system works quite well for me, so I kept it. My graduating and easy intervals are both at 32d, and my insertion order&rsquo;s random. Similarly to the &lsquo;harsh&rsquo; Leitner system, I have my lapses at 1d with a minimum interval of 1d. My leech threshold&rsquo;s at 8, and I&rsquo;ve set this to tag only. It&rsquo;ll be some time before I even get all my cards out of learning, but I hope that this&rsquo;ll work well enough for me. I bury related siblings and review siblings, which reduces redundant review time. My maximum interval&rsquo;s at 100 years, which is a bit long now that I think about it, but oh well. My starting ease is where it starts to get interesting - I&rsquo;ve set it to 2.50, with my easy bonus, interval modifier, and hard interval all at 1.00. This essentially disables the easy and hard buttons, which is nice, since I can&rsquo;t see them visually anyway.</p>
<p>Speaking of the pass / fail setup proposed by Refold&rsquo;s Lowkey Anki, I use this <a href="https://github.com/lambdadog/passfail2">Pass Fail addon</a>, updated for use with Anki 2.1. I resonate with the goals of the addon, and so far, I&rsquo;d say I definitely recommend it. Keep in mind that this all is just a snapshot for future reference, and that I&rsquo;m still very much tweaking everything around.</p>
<h2 id="conclusion">Conclusion</h2>
<p>With Anki, I&rsquo;m looking forward to learning more fluently, and taking away the friction of review. I like that I have one place to return to and one place to manage all the things I should review now, and that that one place is something I can turn to daily. I think this daily habit, while much less focused than specific sprints every other couple days, is more sustainable in maintaining and always brushing up on knowledge. I&rsquo;m still figuring out how to best use Anki to recall and more deeply learn new information. I&rsquo;d like to find, for example, a way to review cards in advance without them piling up in the future. I know I can custom study and toggle off the &rsquo;re-date cards&rsquo; or whatever, but I think my purpose right now is trying to get ahead of reviews, so I don&rsquo;t think there&rsquo;s really a way to review without pushing more cards together in the future. As well, I&rsquo;d like to learn the card and note management windows better. As of now, I&rsquo;ve just figured out how to do what I need to do (which, prior to <a href="https://ankiweb.net/shared/info/385888438">Edit Field During Review (Cloze)</a> was just editing cards and tagging them), and haven&rsquo;t explored much else. Maybe that&rsquo;s alright - maybe I shouldn&rsquo;t overcomplicate things for myself before I&rsquo;ve even gotten started.</p>
<p>In other news, I&rsquo;m currently in the process of overly studying for midterm wave number two. I&rsquo;m glad that I made it through the first wave, and while I&rsquo;m not too ecstatic about some of what I&rsquo;ve been able to do, I&rsquo;ve learned a couple lessons that I&rsquo;ll try to apply in the future. It is what it is. As well, I&rsquo;ve been meaning to start looking into internships and applying to some, but I feel like I&rsquo;ve hit a wall. The imposter syndrome, particularly with the interview side of things, is definitely kicking in. I don&rsquo;t know how I feel describing myself as a &lsquo;web developer&rsquo; while looking at some of those DSA questions, or even web framework questions, and not really having a clue with how to approach them. But it&rsquo;s something that I&rsquo;ve also decided that I need to work on, so I&rsquo;m looking into that CS handbook<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> and trying to figure out a way to systematically learn and improve.</p>
<p>As well, I&rsquo;ve been working through <a href="https://fullstackopen.com/">Full Stack Open</a>, which is an open source course about web development, and React / Typescript / Express / MongoDB in particular. I&rsquo;ve realized that I&rsquo;ve kind of let my Vue experience go a bit, and that, looking through interviews and such, people tend to look for React more. I&rsquo;ve never really systematically learned any part of web development, and I think FSO has been a good introduction to this whole curriculum thing. It also helps that Typescript and the React design patterns are topics that I&rsquo;ve never really covered before. Part of what makes me demotivated and pushes me to stop working on something is that I feel that I know half of whatever a course is teaching, but I don&rsquo;t know the other half: I end up skimming most of the material, and retain not a lot of new information. I&rsquo;m on the third module at the moment, and while I don&rsquo;t have a lot of time to tackle their exercises, I do really enjoy the examples they&rsquo;ve set up and how the problems apply the material super well. I&rsquo;ve found working through the course really informative and honestly fun, and would recommend the course so far. I&rsquo;m excited to see what the rest of the course ends up being like, and hopefully, I&rsquo;ll report back with more progress soon.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Ironically, I very vividly remember making fun of my friends for printing out their own copy and putting it together into a binder and reading it every chance they got in class. At this point, <em>I&rsquo;ve</em> become that kid, but hey, maybe I should have listened to their CS fanboy ramblings the first time around.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>More Linux Tweaks</title>
      <link>https://kewbi.sh/blog/posts/211003/</link>
      <pubDate>03 Oct 2021</pubDate>
      <author>Emilie Ma (Kewbish)</author>
      <description>On troubleshooting many Linux-related issues.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>It&rsquo;s been a while since I had the time to write anything for YK - I&rsquo;ve been getting settled into the quintessential university freshman experience at UBC, and loving it so far. Every upper-year I meet keeps telling me about the rose-coloured glasses apparently all freshmen have, with something along the lines of &lsquo;well, I hope you can keep that mindset up!&rsquo;. It&rsquo;s not particularly encouraging, but to be fair, I&rsquo;ve only been here a month, and haven&rsquo;t had any major quizzes or midterms yet, so maybe I do still have my pink spectacles quite firmly on. More on this some other time: this isn&rsquo;t some university reflection, after all, it&rsquo;s a tech blog!</p>
<p>So, in other news, I got a new laptop, and have been juggling close readings and annotations with setting it up. I&rsquo;ve finally got everything installed and configured to my liking, however, so I thought I&rsquo;d write a bit about the process of getting everything together, similar to my <a href="https://kewbi.sh/blog/posts/210711/">previous one about my monitor and Bluetooth setup</a>. While I was able to follow my past notes for display and Bluetooth configuration, I encountered my fair share of struggles along the way. It&rsquo;s just mostly stuff I&rsquo;d set up on my old laptop but without writing notes about how I got myself through the messes in the first place, I had no idea where to go this time round. However, I managed to work things out, very slowly and steadily, and I&rsquo;m now writing this very post with my new setup.</p>
<p>Some specs:</p>
<ul>
<li>It&rsquo;s the Lenovo Yoga 7 14ITL5 - got it off Best Buy for a half decent deal.</li>
<li>It&rsquo;s got a 11th Gen Intel i7-1165G7, with 8 cores, Intel Iris Xe graphics, a very nice 16GB of RAM that I didn&rsquo;t even have to dismantle the chassis for, and 1TB of NVMe SSD. It&rsquo;s currently running 5.13.19-2MANJARO, and everything (minus fingerprint reader and sound) works really well.</li>
<li>One of the immediate improvements over my old laptop (well, besides weight) is how quickly it boots and runs things. My old laptop had to go for at least half a minute to boot up, which was certainly an annoyance when it&rsquo;d suddenly crash in the middle of Zoom class. Granted, it had to go through Grub and a shabbily patched together Windows dual boot configuration, but this laptop is so much quicker. It boots from completely shut down in what I can perceive to be a second or two, which is crazy fast compared to my past habits of staring blankly at a HP loading circle for ages.</li>
<li>Speaking of weight, the laptop&rsquo;s quite thin and light, something I appreciate when I have other binders and things to shove in my bag. It&rsquo;s about half a kg lighter than my old laptop, which isn&rsquo;t entirely much, but it makes a nice difference that I value.</li>
<li>The battery life&rsquo;s also pretty amazing - the first time I took this laptop to school, I hadn&rsquo;t yet ordered a second charger cable, and my packaged one was still at home. It managed to churn through three hours of Zoom meetings, and three hours of a physics lab, all while still having a good 30% left. It&rsquo;s amazing what this 71Wh battery can do - it can definitely hold tonnes more than my old 38Wh.</li>
<li>Something else I really like about the laptop so far is that I have one (1!) cable to plug in when I get home. I have my power, monitor, printer, webcam and various other little cables all hooked into a singular USB-C hub, which then plugs right into my laptop. I used to have to plug each individual cable in, and because each port was on different sides of my old HP, it was a bit of a mess, visually. This does present the problem of always having to be charging the laptop if I want to use my monitor, but it&rsquo;s somehow configured itself to only charge when not full, and then run off what I assume is AC after that.</li>
<li>And it looks amazingly cute - I&rsquo;m not usually one to fuss over the colour of laptops, but it&rsquo;s just such a <em>nice</em> slate grey.</li>
</ul>
<p>Overall, definitely a big improvement over my old HP, and very grateful I&rsquo;m able to have a better machine.</p>
<p>Similarly to my other Linux-setup-tweaks post, this article will cover a bunch of my troubleshooting, thought processes, and eventual solutions to my various issues: getting my printer drivers working, fixing screen tearing, duplicating Chrome history and passwords, tweaking fonts, and more. This is mostly to keep a record for myself of all the little problems that I&rsquo;ve had, just in case I have to reinstall or do this all over again for some other machine. I don&rsquo;t think this post&rsquo;ll be of much use if you don&rsquo;t have the exact same model of laptop, but maybe some of the more general tweaks, like for fonts and such, will help someone out there. Throughout this guide, I assume you&rsquo;ll be running Manjaro or some Arch relative, and have a basic knowledge of how to run commands and install packages - I&rsquo;ll only be going into what I did to fix my issues, so you&rsquo;ll have to look at more general advice elsewhere.</p>
<h2 id="cups-and-printing">CUPS and Printing</h2>
<p>I&rsquo;m going go start with CUPS troubleshooting first, because I went through a whole struggle to set up my printer. I specifically have the Brother-MFC7340 laser printer, which is a rather old black and white model that comes without any of the fancy modern QoL features like double sided printing or wireless connections. It even has a fax - that&rsquo;s how old it is. But Brother does a pretty decent job of keeping their drivers backward compatible and up to date, even for older models like mine, so I thought I&rsquo;d have a pretty easy time at it here. I installed <a href="https://github.com/OpenPrinting/cups">cups</a>, and trawled the internet a bit for the Brother driver I thought would work, which ended up being <a href="https://aur.archlinux.org/packages/brother-mfc7340/">this one (brother-mfc7340)</a> on the AUR. Spoiler alert: it doesn&rsquo;t work.</p>
<p>I tried uninstalling sane, disabling usblp, and manually editing PPD files, all things listed in the <a href="https://wiki.archlinux.org/title/CUPS">Arch Wiki</a> troubleshooting page. Long story short - the driver doesn&rsquo;t work, wasn&rsquo;t the one I thought I had installed on my old laptop, and you shouldn&rsquo;t bother trying any of the above. Instead, install <a href="https://github.com/pdewacht/brlaser">brlaser</a> from the AUR, and set up your printers to use that driver instead. I&rsquo;m not going to go into the specifics of how to set printers up, but know that you&rsquo;ll have to select something like &lsquo;Brother MFC7360N&rsquo; instead of MFC7340. It&rsquo;s not an exact model match, but it works perfectly fine for me, so I&rsquo;m not going to ask questions.</p>
<p>An aside: to configure scanning properly, you&rsquo;ll have to install <a href="https://aur.archlinux.org/packages/brscan3/">brscan3</a>, or install brother-mfc7340 as above. Then, install <code>simple-scan</code> or another scanning tool, which should automatically register your printer / scanner combo.</p>
<h2 id="screen-tearing">Screen Tearing</h2>
<p>When madly scrolling through tonnes of forum pages in an attempt to resolve the above CUPS issue, I also encountered a bunch of screen tearing issues. I&rsquo;m running Chrome<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, and found that without hardware acceleration on (which for some reason, didn&rsquo;t work properly in Chromium) and another fix I&rsquo;ll detail below, I had pretty significant screen tearing while scrolling. It wasn&rsquo;t anything too severe, but annoying to notice when trying to read pages and scroll nicely.</p>
<p>What got my screen to stop tearing was, for one, enabling hardware acceleration in Chrome settings (see chrome://settings), and messing around with Intel graphics files.
I created <code>/usr/share/X11/xorg.conf.d/20-intel.conf</code>, and put the following in it:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>Section &#34;Device&#34;
</span></span><span style="display:flex;"><span>  Identifier  &#34;Intel Graphics&#34;
</span></span><span style="display:flex;"><span>  Driver      &#34;intel&#34;
</span></span><span style="display:flex;"><span>  Option      &#34;AccelMethod&#34;  &#34;sna&#34;
</span></span><span style="display:flex;"><span>  Option      &#34;TearFree&#34;     &#34;true&#34;
</span></span><span style="display:flex;"><span>EndSection
</span></span></code></pre></div><p>When I rebooted, the boot was getting stuck at &lsquo;/dev/[something] clean, &hellip; files, &hellip; blocks&rsquo;, so I thought I&rsquo;d have to start over and reinstall my system again. But don&rsquo;t panic - hit <!-- raw HTML omitted -->CTRL<!-- raw HTML omitted -->-<!-- raw HTML omitted -->ALT<!-- raw HTML omitted -->-<!-- raw HTML omitted -->F2<!-- raw HTML omitted --> to get into a recovery terminal. From here, you can either <code>pacman -Syu</code> and see if that fixes it; or, as that didn&rsquo;t work in my case, run <code>sudo mhwd -f -i pci video-linux</code>, which did some scary things for a while and then popped me back into my login screen. The above command is the equivalent of <code>sudo mhwd -r pci video-linux</code> followed by <code>sudo mhwd -i pci video-linux</code>, if that helps.</p>
<h2 id="retracing-history">Retracing History</h2>
<p>It turns out Chrome conveniently stores all of its data in <code>~/.config/google-chrome/</code>. Most of what you&rsquo;ll want will be in the <code>~/.config/google-chrome/Default/</code> directory, which is home to the History, Cookies, Extension Cookies, and Shortcuts databases. I&rsquo;m going to be honest - I don&rsquo;t entirely know which have carryover effects and which don&rsquo;t, but I essentially copy-pasted all the databases I could find to my new laptop. This let me retain my history, but I still had to log into each website and refresh cookies where necessary on my own. Note that the Extensions directories don&rsquo;t appear to do anything - I thought that this would copy over all of my extensions, but alas, I had to do that manually as well, which is surprising, since the directory contains what appears to be the raw extension files.</p>
<p>Speaking of logins, I&rsquo;ve saved most of my passwords into the Chrome password manager (not very secure, I know), and wanted to transfer those over as well<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. On my origin / old laptop, I went into <code>chrome://settings/passwords</code>, and clicked the hamburger menu near Saved Passwords. This let me export all my passwords into a very dangerous, unencrypted CSV. Try not to let anyone get access to that. On my new installation, I went to <code>chrome://flags</code>, and searched for and enabled Password Import. Navigate back to the <code>chrome://settings/passwords</code> page, and it&rsquo;ll let you import a whole bunch of passwords in the same menu.</p>
<p>Also, a cute tip if you, like me, enable login to Gmail without login to Chrome but still want a custom profile pic in Chrome: click on your profile, click edit, and select some other profile pic. This&rsquo;ll spit out the photo in <code>~/.config/google-chrome/Avatars/</code>. Take whatever photo you want to use as a profile pic, rename it to whatever&rsquo;s in the Avatars directory, and replace the Chrome avatar with your custom pic.</p>
<h2 id="font-tweaking">Font Tweaking</h2>
<p>I had two major font tweaks to do - one with my preferred emoji font, and one with my type hinting in LibreOffice.</p>
<p>First, the emojis. I prefer to use <a href="https://twemoji.twitter.com/">Twemoji</a>, which is Twitter&rsquo;s custom emoji font - also the same one used in Discord. I like how round and flat and cohesive it is, but maybe that&rsquo;s just an acquired taste from spending much too long on Discord. Regardless, to make Twemoji work at a baseline level, you&rsquo;ll have to install <a href="https://aur.archlinux.org/packages/ttf-twemoji/">ttf-twemoji</a>. There&rsquo;s another package, <a href="https://aur.archlinux.org/packages/ttf-twemoji-color">ttf-twemoji-color</a>, that provides the colour version of the font, as well as the B&amp;W files. However, Chrome apparently doesn&rsquo;t have full support for SVG colour by default, and the colour version wasn&rsquo;t appearing in other apps like my terminal. So, I had to do some finagling, and install <a href="https://archlinux.org/packages/ttf-bitstream-vera">ttf-bitstream-vera</a>, <em>then</em> ttf-twemoji-color. Finally, I had to create <code>~/.config/fontconfig/fonts.conf</code>, and put the following inside:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>&lt;fontconfig&gt;
</span></span><span style="display:flex;"><span>  &lt;alias&gt;
</span></span><span style="display:flex;"><span>    &lt;family&gt;serif&lt;/family&gt;
</span></span><span style="display:flex;"><span>    &lt;prefer&gt;
</span></span><span style="display:flex;"><span>      &lt;family&gt;Twemoji Color&lt;/family&gt;
</span></span><span style="display:flex;"><span>    &lt;/prefer&gt;
</span></span><span style="display:flex;"><span>  &lt;/alias&gt;
</span></span><span style="display:flex;"><span>  &lt;alias&gt;
</span></span><span style="display:flex;"><span>    &lt;family&gt;sans-serif&lt;/family&gt;
</span></span><span style="display:flex;"><span>    &lt;prefer&gt;
</span></span><span style="display:flex;"><span>      &lt;family&gt;Twemoji Color&lt;/family&gt;
</span></span><span style="display:flex;"><span>    &lt;/prefer&gt;
</span></span><span style="display:flex;"><span>  &lt;/alias&gt;
</span></span><span style="display:flex;"><span>  &lt;alias&gt;
</span></span><span style="display:flex;"><span>    &lt;family&gt;monospace&lt;/family&gt;
</span></span><span style="display:flex;"><span>    &lt;prefer&gt;
</span></span><span style="display:flex;"><span>      &lt;family&gt;Twemoji Color&lt;/family&gt;
</span></span><span style="display:flex;"><span>    &lt;/prefer&gt;
</span></span><span style="display:flex;"><span>  &lt;/alias&gt;
</span></span><span style="display:flex;"><span>  &lt;alias&gt;
</span></span><span style="display:flex;"><span>    &lt;family&gt;Apple Color Emoji&lt;/family&gt;
</span></span><span style="display:flex;"><span>    &lt;prefer&gt;
</span></span><span style="display:flex;"><span>      &lt;family&gt;Twemoji Color&lt;/family&gt;
</span></span><span style="display:flex;"><span>    &lt;/prefer&gt;
</span></span><span style="display:flex;"><span>  &lt;/alias&gt;
</span></span><span style="display:flex;"><span>&lt;/fontconfig&gt;
</span></span></code></pre></div><p>Run <code>fc-cache -f -v</code> to clear the font cache, log out and back in for good measure, and you should be graced with some very quirky, iconic emojis.</p>
<p>I also had a small problem with <a href="https://aur.archlinux.org/packages/ttf-ms-fonts">ttf-ms-fonts</a>, which is a package that includes all the default Windows fonts, like Times New Roman. I&rsquo;m required to use it for school, so I&rsquo;d have to be staring at it quite regularly. A pity then, that LibreOffice wasn&rsquo;t displaying it properly - it was a bit too short and compressed, and there were some spacing issues that came up occasionally. So, I had to further edit my <code>fonts.conf</code>, and add some bitmap edits:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>&lt;match target=&#34;font&#34;&gt;
</span></span><span style="display:flex;"><span>	&lt;edit name=&#34;embeddedbitmap&#34; mode=&#34;assign&#34;&gt;
</span></span><span style="display:flex;"><span>		&lt;bool&gt;false&lt;/bool&gt;
</span></span><span style="display:flex;"><span>	&lt;/edit&gt;
</span></span><span style="display:flex;"><span>&lt;/match&gt;
</span></span></code></pre></div><p>Put this inside the <code>&lt;fontconfig&gt;</code> tag, and run <code>fc-cache -f -v</code> again.</p>
<p>The final <code>~/.config/fontconfig/fonts.conf</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>&lt;fontconfig&gt;
</span></span><span style="display:flex;"><span>  &lt;alias&gt;
</span></span><span style="display:flex;"><span>    &lt;family&gt;serif&lt;/family&gt;
</span></span><span style="display:flex;"><span>    &lt;prefer&gt;
</span></span><span style="display:flex;"><span>      &lt;family&gt;Twemoji Color&lt;/family&gt;
</span></span><span style="display:flex;"><span>    &lt;/prefer&gt;
</span></span><span style="display:flex;"><span>  &lt;/alias&gt;
</span></span><span style="display:flex;"><span>  &lt;alias&gt;
</span></span><span style="display:flex;"><span>    &lt;family&gt;sans-serif&lt;/family&gt;
</span></span><span style="display:flex;"><span>    &lt;prefer&gt;
</span></span><span style="display:flex;"><span>      &lt;family&gt;Twemoji Color&lt;/family&gt;
</span></span><span style="display:flex;"><span>    &lt;/prefer&gt;
</span></span><span style="display:flex;"><span>  &lt;/alias&gt;
</span></span><span style="display:flex;"><span>  &lt;alias&gt;
</span></span><span style="display:flex;"><span>    &lt;family&gt;monospace&lt;/family&gt;
</span></span><span style="display:flex;"><span>    &lt;prefer&gt;
</span></span><span style="display:flex;"><span>      &lt;family&gt;Twemoji Color&lt;/family&gt;
</span></span><span style="display:flex;"><span>    &lt;/prefer&gt;
</span></span><span style="display:flex;"><span>  &lt;/alias&gt;
</span></span><span style="display:flex;"><span>  &lt;alias&gt;
</span></span><span style="display:flex;"><span>    &lt;family&gt;Apple Color Emoji&lt;/family&gt;
</span></span><span style="display:flex;"><span>    &lt;prefer&gt;
</span></span><span style="display:flex;"><span>      &lt;family&gt;Twemoji Color&lt;/family&gt;
</span></span><span style="display:flex;"><span>    &lt;/prefer&gt;
</span></span><span style="display:flex;"><span>  &lt;/alias&gt;
</span></span><span style="display:flex;"><span>    &lt;match target=&#34;font&#34;&gt;
</span></span><span style="display:flex;"><span>        &lt;edit name=&#34;embeddedbitmap&#34; mode=&#34;assign&#34;&gt;
</span></span><span style="display:flex;"><span>            &lt;bool&gt;false&lt;/bool&gt;
</span></span><span style="display:flex;"><span>        &lt;/edit&gt;
</span></span><span style="display:flex;"><span>    &lt;/match&gt;
</span></span><span style="display:flex;"><span>&lt;/fontconfig&gt;
</span></span></code></pre></div><h2 id="and-finally-sync">And, Finally, Sync</h2>
<p>No actual problems here - I&rsquo;d just like to keep a note of the rsync command I use to back things up for further reference:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>rsync -r --info=progress2 --exclude=.git --exclude=node_modules --exclude=archive --exclude=&#34;*.mp4&#34; /home/kewbish/Downloads/dev/* /media/kewbish/Seagate\ Basic/dev/
</span></span></code></pre></div><h2 id="conclusion">Conclusion</h2>
<p>Every time I set up a laptop, or break something and need to fix it, I&rsquo;ve realized that even though there&rsquo;ll inevitably be trials and tribulations, I end up learning a lot - not just about how to fix said bugs, but in general how computers work. The first time I installed Linux, I had to learn about dual booting, what partitions even were, and how to make partition schemes, and port all my hacky Windows experience over into a whole new world of Unix. When I was trying to troubleshoot my monitor last time, I learned a whole lot about displays and profiles, and likewise when I sorted out my Bluetooth connections. These processes of constantly breaking, then fixing and learning, are what&rsquo;s kind of fun about Linux. Sure, you won&rsquo;t ever have to run into major issues on Windows (as long as you don&rsquo;t do anything too crazy), but it ends up abstracting all the interesting system files and configurations away from the user. Sometimes diving into the <code>/etc/</code> or <code>/usr/</code> directories can lead to a whole lot of research and fun findings - that&rsquo;s why I still bodge my way through Linux, even though I&rsquo;ve been &lsquo;haha linux kid&rsquo;ed so many times.</p>
<p>There&rsquo;s still some things that refuse to work - the fingerprint scanner, and the speakers, for some reason, are either not supported on Linux, or require some <a href="https://bugzilla.kernel.org/show_bug.cgi?id=208555">complicated kernel patches</a> that for the sake of my sanity I don&rsquo;t think I&rsquo;ll attempt. As well, now comes the process of wiping my old HP and figuring out how to reinstall Windows, and then handing it off to a family member. I also need to sort out some small little config problems here and there, but I don&rsquo;t run into them enough to bother. For the most part, what I need to work works fine: I wasn&rsquo;t planning on using the scanner, I have headphones or earbuds for when I need audio, and my programming environment works pretty much identically to my HP anyways. It&rsquo;s good that I managed to set everything up so quickly - I don&rsquo;t have to tote around such a bulky laptop anymore, and the extra battery life&rsquo;s definitely a boon for uni.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>The Googled one, as I found Chromium was a bit buggy in some respects, such as not letting me sign into my Gmail because the &lsquo;browser was insecure&rsquo;, and also not letting me save my passwords properly. I haven&rsquo;t found any performance differences, and to be honest, I&rsquo;m okay with not crusading the whole privacy narrative if I&rsquo;ll be able to check my bloody email.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>I never got around to enabling and really using Google Account sync, which apparently takes all your locally saved passwords and associates them with your Google Account instead. This has the added benefit of syncing extensions as well - something that would have been nice to maintain my preferences from. Having sync enabled makes switching installations and machines a lot easier, but I don&rsquo;t know if I want to bother.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
