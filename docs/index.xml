<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Yours, Kewbish - a collection of articles on tech and thought.</title>
    <link>https://kewbi.sh/blog/</link>
    <description>Latest Yours, Kewbish posts</description>
    <managingEditor>(Emilie Ma (Kewbish))</managingEditor>
    
	<atom:link href="https://kewbi.sh/blog/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Snakes and Lagrangian Ladders</title>
      <link>https://kewbi.sh/blog/posts/241229/</link>
      <pubDate>29 Dec 2024</pubDate>
      <author>Emilie Ma (Kewbish)</author>
      <description>On secret sharing and interpolation.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>I recently had the pleasure of reading a dense crypto paper, emailing the corresponding author in frustration to ask questions, then double emailing two hours later to say I&rsquo;d figured it out. The paper in question was Yurek et al.&rsquo;s <a href="https://eprint.iacr.org/2022/971">&ldquo;Long Live The Honey Badger: Robust Asynchronous DPSS and its Applications&rdquo;</a>: it describes how to split up a secret into shares that can be distributed among a set of people and how to change the set of people and refresh shares while keeping the original secret intact. Secret sharing is important for sensitive but critical information: information that no single party should be able to read on their own, but that we can&rsquo;t afford to lose — think nuclear launch codes and encryption keys.</p>
<p>The problem I was emailing with was me not being able to understand the refresh process. Algorithm 3 in the paper opaquely says a few times to &ldquo;interpolate&rdquo; the desired new secrets. I&rsquo;m sure seasoned cryptographers can recalculate secrets from shares in their sleep, but detangling these three lines took more time than reading the entire rest of the paper. Typical secret sharing via <a href="https://en.wikipedia.org/wiki/Shamir%27s_secret_sharing">Shamir secret sharing</a> is very cleverly constructed already, and as I mentioned in <a href="https://kewbi.sh/blog/posts/241020/">my previous post</a>, crypto papers generally don&rsquo;t do a good job of explaining the basics in favour of just laying down novel material. This is great for brevity and conciseness in academia, but not so great for beginners to the field.</p>
<p>Once I&rsquo;d gotten more background on the very finicky math, the actual interpolation and refreshing wasn&rsquo;t hard to understand and implement. Still, I wish they&rsquo;d linked to a layman&rsquo;s explainer, although from a few cursory searches, none quite seems to exist. This post is the explainer I wish I&rsquo;d had before tackling this paper. I&rsquo;ll cover the very basics of Shamir secret sharing before summarizing the Honey Badger protocol&rsquo;s use of interpolation, explaining the key three lines of Algorithm 3 in the scheme, so you can hopefully save yourself the three hours of puzzling.</p>
<h2 id="eating-the-frog">Eating the Frog</h2>
<p>In my opinion, the hardest part of Shamir Secret Sharing is the Lagrange interpolation, so I&rsquo;m going to put it up front here while your brains are still fresh. Lagrange interpolation is a way of estimating the value of a polynomial at some input, given some points on the polynomial. Let&rsquo;s consider a bog-standard function: <code>f(x) = x^2 + 2</code>.</p>
<figure><img src="/img/241229/interpolation-1.png"
         alt="Figure 1. f(x) = x² &#43; 2."/><figcaption>
            <p><em>Figure 1. f(x) = x² + 2</em></p>
        </figcaption>
</figure>

<p>If we have no knowledge of the polynomial, we&rsquo;ll need three points to estimate the value of <code>f(x)</code> for any <code>x</code> back. This is because <code>f(x)</code> is a polynomial of degree two (highest power term is <code>x^2</code>). There&rsquo;s no <code>x</code> term in the function as I&rsquo;ve written it, but we can also write it as <code>f(x) = 1 * x^2 + 0 * x + 2</code>.</p>
<p>In order to recover <code>f(x)</code>, we need to solve for the coefficients of the terms — the 1, 0, and 2. If we know the values <code>y_i = f(x_i)</code> taken at few different points <code>x_i</code>, then we&rsquo;ll get something like the below:</p>
<pre tabindex="0"><code>y_1 = a * x_1^2 + b * x_1 + c
y_2 = a * x_2^2 + b * x_2 + c
y_3 = a * x_3^2 + b * x_3 + c
</code></pre><p>In this case, we know three values of <code>x</code>, and their associated <code>y</code> values, and we&rsquo;re solving for <code>a</code>, <code>b</code>, and <code>c</code>. Because we have three unknown coefficients, we&rsquo;ll need a system of three equations, based on three evaluated points. In general, for a polynomial of degree <code>t</code>, you&rsquo;ll need <code>t+1</code> points to solve for the polynomial — the extra <code>+1</code> comes from the unknown value of the constant term itself.</p>
<p>To figure out how to reconstruct the polynomial, let&rsquo;s start with a slightly easier function, <code>f(x) = 2x</code>. Let&rsquo;s say we know that <code>f(1) = 2</code> and <code>f(3) = 6</code>.</p>
<figure><img src="/img/241229/interpolation-2.png"
         alt="Figure 2. f(x) = 2x."/><figcaption>
            <p><em>Figure 2. f(x) = 2x</em></p>
        </figcaption>
</figure>

<p>We want to derive a polynomial that takes on the value 2 at <code>x = 1</code> and 6 at <code>x = 3</code>. This is one such polynomial:</p>
<pre tabindex="0"><code>y = (x - 3)/(1 - 3) * 2 + (x - 1)/(3 - 1) * 6
</code></pre><p>Note that when <code>x = 1</code>, the first fraction cancels out to 1 and the second fraction evaluates to 0, so <code>y = 1 * 2 + 0 * 6 = 2</code>, as expected. Similarly, when <code>x = 3</code>, the first fraction evaluates to 0 and the second fraction to 1, making <code>y = 0 * 2 + 1 * 6 = 6</code>. Also note that when we rearrange the terms:</p>
<pre tabindex="0"><code>y = (x - 3)/(-2) * 2 + (x - 1)/(2) * 6
  = -(x - 3) + 3(x - 1)
  = -x + 3 + 3x - 3
  = 2x
</code></pre><p>So we can recover our original function! As another example, let&rsquo;s take our original function <code>f(x) = x^2 + 2</code>. Because this polynomial is of degree 2, we&rsquo;ll need 3 points to reconstruct it. Let&rsquo;s take <code>f(1) = 3</code>, <code>f(2) = 6</code>, and <code>f(3) = 11</code>.</p>
<p>Let&rsquo;s look at the form of our fractional coefficients. We want each fraction to evaluate to 1 at the associated x-value, so the overall term evaluates to the right y-value. We also want the fraction to evaluate to 0 anywhere else. The general form of such a term (let&rsquo;s say, for the point <code>(x_i, y_i)</code>) is to have the numerator of the fraction be a product of <code>(x - x_m)</code>, and the denominator be <code>(x_i - x_n)</code>, for all of the other <code>x_m</code>. We exclude the <code>(x - x_i)/(x_i - x_i)</code> fraction, because when this is evaluated at <code>x_i</code>, we&rsquo;d get <code>0/0</code>, which is undefined. When we evaluate this fraction at <code>x_i</code>, the terms in the numerator all cancel with the terms in the denominator, so we get 1, as desired. When we evaluate this fraction at any of the other input x-values, however, one of the numerator terms <code>(x - x_m)</code> will be 0, preventing this particular y-value from affecting the overall polynomial&rsquo;s value. This fractional coefficient is also called the <a href="https://en.wikipedia.org/wiki/Lagrange_polynomial#Definition">Lagrange basis polynomial</a>, and we&rsquo;ll denote it <code>λ_i(x)</code>. Each y-value is multiplied by a different Lagrange basis polynomial, since the x-value changes, so the <code>i</code> in the basis polynomial also has to change.</p>
<figure><img src="/img/241229/interpolation-3.png"
         alt="Figure 3. Form of the Lagrange basis polynomial."/><figcaption>
            <p><em>Figure 3. Form of the Lagrange basis polynomial.</em></p>
        </figcaption>
</figure>

<p>For our example, our polynomial is thus:</p>
<pre tabindex="0"><code>y = (x - 2)(x - 3)/(1 - 2)(1 - 3) * 3 +
    (x - 1)(x - 3)/(2 - 1)(2 - 3) * 6 +
	(x - 1)(x - 2)/(3 - 1)(3 - 2) * 11
</code></pre><p>If we plug in <code>x = 4</code>, for example, we&rsquo;d expect to get <code>4^2 + 2 = 18</code>. Indeed, we get:</p>
<pre tabindex="0"><code>y = (4 - 2)(4 - 3)/(1 - 2)(1 - 3) * 3 +
    (4 - 1)(4 - 3)/(2 - 1)(2 - 3) * 6 +
	(4 - 1)(4 - 2)/(3 - 1)(3 - 2) * 11
  = 1 * 3 - 3 * 6 + 3 * 11
  = 3 - 18 + 33 = 18
</code></pre><p>Feel free to skip this expansion, but if you&rsquo;re interested, this is what we get when we rearrange the terms:</p>
<pre tabindex="0"><code>y = (x^2 - 5x + 6) * 3/2 +
    (x^2 - 4x + 3) * -6 +
    (x^2 - 3x + 2) * 11/2
  = (3/2 - 6 + 11/2) * x^2 +
    (-15/2 x + 24 x + - 33/2 x) +
	(9 - 18 + 11)
  = x^2 + 0x + 2 = x^2 + 2
</code></pre><p>Even when evaluating this interpolated polynomial at an x-value that wasn&rsquo;t originally provided, like <code>x = 4</code>, for example, we were still able to get the correct result. We can use the same interpolated polynomial to evaluate the y-value at <code>x = 2.5</code>, for example. Using the Lagrange interpolation to recover the y-value at an x-value not included in the list of input points is critical to how Shamir secret sharing works, as I&rsquo;ll describe in the next section.</p>
<h2 id="here-there-be-snakes-sss">Here There Be Snakes (SSS)</h2>
<p>Shamir Secret Sharing starts with the secret <code>s</code> that you want to be able to reconstruct. This might be something like a recovery key or private message. For the sake of explanation, assume this secret is a number. In more realistic scenarios, the secret is likely some series of bytes which need to be transcoded into, or at least interpreted as, large integers, but this is outside of the scope of the SSS algorithm.</p>
<p>We then define a polynomial <code>SSS(x) = s + a * x + b * x^2 + ... + z * x^t</code>. The degree of the polynomial (the choice of <code>t</code>) determines how many shares you&rsquo;ll need to reconstruct the secret <code>s</code>. The coefficients <code>a, b, ...</code> are all randomly chosen numbers. Note that the constant term is the secret <code>s</code> that we&rsquo;re trying to split into shares, and that <code>SSS(0) = s</code>. The SSS shares of the secret are now simply the point <code>(i, SSS(i))</code> for some index <code>i</code> — we need to keep track of the actual index from which a SSS share was calculated for the reconstruction later on. Many such shares can then be calculated and distributed to the other parties who want to be able to help reconstruct the secret.</p>
<p>To recover the secret, at least <code>t+1</code> parties must submit their shares. Using Lagrange interpolation, we can then recover the original <code>SSS</code> polynomial, or in particular, evaluate it at index 0. This gives us the value of <code>s</code> back.</p>
<p>As you might have guessed, SSS&rsquo;s primary application is data recovery. For example, <a href="https://www.preveil.com/">PreVeil</a> is an E2EE platform focusing on email and file collaboration. It <a href="https://www.preveil.com/wp-content/uploads/2024/06/PreVeil_Security_Whitepaper-v1.6.pdf">supports the notion of Approval Groups</a>, a recovery scheme that makes use of SSS, requiring some threshold of designated contacts to recover the user&rsquo;s encryption key. Trezor, the hardware cryptocurrency wallet, <a href="https://trezor.io/learn/a/what-is-shamir-backup?srsltid=AfmBOoq2BNTogcvxxynrx-o49LEF0cLjtxPRuM2F0kTxkLVIu5ZwNry_">makes use of SSS</a> to back up the user&rsquo;s recovery key. The HN- and GitHub-famous <a href="https://github.com/jesseduffield/horcrux">Horcrux</a> tool is one of my favourite SSS applications, just because it&rsquo;s such a fun concept: Horcrux lets you split up a file into encrypted shares, much like Voldemort did with his soul. I mentioned above that the SSS secret is usually some series of bytes interpreted as a large integer — here, Horcrux splits up the file into chunks of bytes to avoid integer overflow and repeats the SSS once per chunk, collecting the i-th shares of each chunk into the overall i-th share.</p>
<p>With SSS, you can choose both your recovery threshold, <code>t</code>, and your total number of shares, <code>n</code>. Having a large <code>n</code> is appealing, since you have more options for who can help reconstruct your secret — in the case where you&rsquo;re trying to recover a key from unreliable P2P nodes, for example, the higher availability from a large <code>n</code> might be desirable. On the other hand, an adversary also has more options for who to compromise in order to learn <code>s</code>. Having <code>n</code> be much larger than <code>t</code> can be problematic in this case.</p>
<p>One key caveat of SSS is that when these applications directly recover data (e.g. split up a recovery key directly into shares), they&rsquo;re vulnerable to malicious parties colluding to recover your data. SSS is usually applied in E2EE contexts as an alternative to the platform servers storing the user&rsquo;s key, so it&rsquo;s problematic if adversaries can compromise a threshold of nodes and directly recover your key. In the case when these parties are social recovery contacts (read: real people), social engineering is also a risk, since there&rsquo;s no easy way to authenticate reconstruction requests, so your contacts might accidentally send their share to a malicious impersonator and leak your recovery data. PreVeil&rsquo;s whitepaper doesn&rsquo;t mention any protections against this, and neither does Trezor. I think the collusion and social engineering concerns are fundamental to vanilla SSS without any additional security considerations.</p>
<p>One solution to these issues is the approach I took in my recent research project, <a href="https://github.com/kewbish/kintsugi">Kintsugi</a>, which involves adding an <a href="https://kewbi.sh/blog/posts/241020/">Oblivious Pseudo-Random Function exchange</a> that protects against the risk of collusion by requiring an additional brute-force step, and against social engineering by not requiring recovery request authentication. The math is quite neat, and this project is what made me look at dynamic proactive secret sharing in the first place. Speaking of, let&rsquo;s get into the Honey Badger protocol now that we have the background on the base SSS protocol.</p>
<h2 id="resharing-is-caring">(Re)sharing is Caring</h2>
<p><a href="https://eprint.iacr.org/2022/971">Honey Badger</a> is the dynamic, proactive secret sharing (DPSS) protocol proposed by Yurek et al. in their 2022 paper &ldquo;Long Live The Honey Badger: Robust Asynchronous DPSS and its Applications&rdquo;. DPSS is used to refresh the secret shares that SSS outputs while keeping the same overall secret <code>s</code>. DPSS both allows users to update the set of parties that possess shares, and invalidates former secret shares, preventing them from being used to reconstruct the secret <code>s</code>.</p>
<p>Recall that the SSS polynomial has the form <code>SSS(x) = s + a * x + b * x^2 + ... + z * x^t</code>. I&rsquo;ll use the term &ldquo;party&rdquo; to refer to the party that holds a secret share, be that a service provider, server, or friend. The core idea of Honey Badger is for each party at index <code>i</code> that has the old SSS secret share <code>s_i = SSS(i)</code> to generate a new SSS polynomial:</p>
<pre tabindex="0"><code>SSS&#39;_i(x) = s_i + a&#39; * x + b&#39; * x^2 ... + z&#39; * x^t&#39;
</code></pre><p>The constant term of this new polynomial is the former SSS secret share. The coefficients <code>a', b', ... z'</code> are new random coefficients, and the degree of the polynomial can change to a new threshold <code>t'</code>. The new secret share that results from the DPSS process will be different than <code>s_i</code>, and if they differ, a threshold of <code>t'</code> will be required to reconstruct <code>s</code> instead of a threshold of <code>t</code>. (The Honey Badger paper uses the notation <code>χ_i(x)</code> instead of <code>SSS'_i(x)</code>, <code>[s]^i_d</code> instead of <code>s_i</code>, and <code>d</code> instead of <code>t</code>.)</p>
<p>The party at index <code>i</code> then sends an evaluation to each other party at their respective index, <code>SSS'_i(j)</code>. which allows the party at index <code>j</code> to interpolate their new share <code>s_j'</code> (line 208 in Algorithm 3 of the paper). These new party shares can be further interpolated to recover the original secret <code>s</code>. Recall that each original <code>s_i = SSS(i)</code> and <code>s = s_0 = SSS(0)</code>. Similarly, each party&rsquo;s new share <code>s'_i</code> is interpolated as <code>SSS'_0(i)</code>, or alternative shares of <code>s_0 = s</code>. Once the party at index <code>i</code> has collected a threshold of shares of the form <code>SSS'_1(i)</code>, <code>SSS'_2(i)</code> and so on, they can interpolate :</p>
<pre tabindex="0"><code>s&#39;_i = λ_1(0) * SSS&#39;_1(i) + λ_2(0) * SSS&#39;_2(i) + ... + λ_{t&#39;}(0) * SSS&#39;_{t&#39;}(i)
    = SSS&#39;_0(i)
</code></pre><p>Thus, when these <code>s'_i</code> are interpolated again (during a normal SSS recovery operation, for example), the original <code>s</code> is recovered.</p>
<pre tabindex="0"><code>  λ_1(0) * s&#39;_1 + λ_2(0) * s&#39;_2 + ... + λ_{t&#39;}(0) * s&#39;_{t&#39;}
= λ_1(0) * SSS&#39;_0(1) + λ_2(0) * SSS&#39;_0(2) + ... + λ_{t&#39;}(0) * SSS&#39;_0(t&#39;)
= SSS&#39;_0(0)
= s_0 = s
</code></pre><p>Intuitively, consider that the original secret <code>s</code> is split into shares once, with each of those party shares being split up again. This broadcast changes which parties hold which sub-shares of the original secret, although the underlying shared data remains the same. <code>SSS'_i(x)</code> can have a different degree, and therefore a different reconstruction threshold <code>t'</code>, than <code>SSS(x)</code>, allowing users to add or remove recovery parties. This secret refresh can also be configured to run at some desired interval (e.g. once per day) to protect against recovery parties&rsquo; shares being leaked over time.</p>
<p>Alternatively, the paper describes this interpolation process more formally by framing the new <code>SSS'_i(x)</code> as a bivariate polynomial, or a polynomial with two variables, denoted as <code>SSS'(i, x)</code> (the paper uses <code>B(x, y)</code>, see line 207 of Algorithm 3 in the paper). This bivariate polynomial still has the form <code>SSS'(i, x) = s_i + a' * x + b' * x^2 ... + z' * x^t'</code>. Note that <code>SSS'(0, 0) = s_0 = s</code>. The idea of resharing is then to interpolate in one variable first, <code>i</code>, over the evaluations at <code>x = i</code> (this is the tricky part!) to get the various <code>SSS'(0, i) = s'_i</code> shares. Then, to reconstruct the original secret, you can interpolate in the other variable, <code>x</code>, to recover <code>SSS'(0, 0) = s</code>. If looking at concrete code makes this easier to understand, feel free to also take a look at <a href="https://github.com/kewbish/kintsugi/blob/master/src/polynomial.rs#L77">my Rust implementation</a> and its <a href="https://github.com/kewbish/kintsugi/blob/master/src/polynomial_tests.rs#L44">tests</a>.</p>
<h2 id="commitment-issues">Commitment Issues</h2>
<p>The above section focused on how the core resharing of Honey Badger worked. However, you might have noticed that the actual paper also mentions checking <a href="https://en.wikipedia.org/wiki/Commitment_scheme"><em>commitments</em></a>. Commitments hide an actual value and later prove, upon revealing the value, that you haven&rsquo;t changed it in the meantime. In Honey Badger, the commitments serve to prove that the nodes are resharing correct secret shares <code>s'_i</code> to the new set of parties.</p>
<p>Honey Badger uses <a href="https://www.rareskills.io/post/pedersen-commitment">Pedersen commitments</a>, of the form <code>s_i * G + ŝ_i * H</code> (see <a href="https://kewbi.sh/blog/posts/241020/#background-elliptic-curves">my other post</a> if you&rsquo;re less familiar with elliptic curves.) Here, <code>s_i</code> is the secret you&rsquo;re committing to, <code>G</code> is the generator point of the elliptic curve, <code>ŝ_i</code> is a random blinding factor used to keep <code>s_i</code> secret even in the case of brute-force attacks, and <code>H</code> is similarly a random elliptic curve point. Points <code>G</code> and <code>H</code> are public to both the old and new resharing parties. During refresh operations, <code>ŝ_i</code> is also calculated the same way <code>s_i</code> is reshared, with a new <code>SSS'_i(x)</code> polynomial with <code>ŝ_i</code> as its secret constant.</p>
<p>In Honey Badger, parties broadcast their new shares alongside a new commitment, which is then verified at various points (lines 205 of Algorithm 1, 303 of Algorithm 2, and 202 of Algorithm 3). Nodes must also keep track of other parties&rsquo; commitments, though, so they have something to verify new shares against: note that in Algorithm 1, the commitments of all shares are broadcast and stored by all parties. This means that the resharing process requires an additional step to interpolate the new commitments (line 209 of Algorithm 3). This isn&rsquo;t trivial (in contrast to the paper&rsquo;s rather flip suggestion to simply &lsquo;interpolate&rsquo;), because we don&rsquo;t know the other parties&rsquo; new secret shares <code>s'_i</code> or their blinding factors <code>ŝ'_i</code>, so we need to somehow interpolate their <em>new</em> commitments only based on public information.</p>
<p>Let&rsquo;s say that the party <code>j</code> wants to interpolate these new commitments. To set the scene, we start out by knowing <code>c_i = s_i * G + ŝ_i * H</code> for all of the other parties (without knowing their <code>s_i</code> or <code>ŝ_i</code>, or any of the new shares or blinding factors), as well as our new share <code>s'_j</code> and new blinding factor <code>ŝ'_j</code>. We want to figure out <code>c'_i = s'_i * G + ŝ'_i * H</code>.</p>
<p>The key here is to recall how <code>s'_i</code> is calculated with Lagrange interpolation, which we can also apply on the commitments themselves to interpolate the new <code>c'_i</code>. Recall that the Lagrange basis polynomial, <code>λ_i(x_i)</code>, is used as a coefficient for the point at index <code>i</code> to interpolate for the value of the polynomial at <code>x = x_i</code>. Now, instead of interpolating for <code>s_0</code> at <code>x = 0</code>, we want to interpolate for the commitment at index <code>i</code>. This can be written as:</p>
<pre tabindex="0"><code>  λ_1(i) * c_1 + λ_2(i) * c_2 + ... + λ_{t+1}(i) * c_{t+1}
</code></pre><p>We can then expand the form of the commitments <code>c_1</code>, <code>c_2</code>, etc.:</p>
<pre tabindex="0"><code>= λ_1(i)     * (s_1 * G     + ŝ_1 * H) +
  λ_2(i)     * (s_2 * G     + ŝ_2 * H) + ...
  λ_{t+1}(i) * (s_{t+1} * G + ŝ_{t+1} * H)
</code></pre><p>Then, multiply out the Lagrange basis polynomials and collect like terms to move <code>G</code> and <code>H</code> outside. You can then notice that the coefficients of <code>G</code> and <code>H</code> are then exactly the forms of the reshared <code>s'_i</code> and <code>ŝ'_i</code><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<pre tabindex="0"><code>= (λ_1(i) * s_1 * G + λ_2(i) * s_2 * G + ... + λ_{t+1} * s_{t+1} * G) +
  (λ_1(i) * ŝ_1 * H + λ_2(i) * ŝ_2 * H + ... + λ_{t+1} * ŝ_{t+1} * H)
= (λ_1(i) * s_1 + λ_2(i) * s_2 + ... + λ_{t+1} * s_{t+1}) * G +
  (λ_1(i) * ŝ_1 + λ_2(i) * ŝ_2 + ... + λ_{t+1} * ŝ_{t+1}) * H
= s&#39;_i * G + ŝ&#39;_i * H
= c&#39;_i
</code></pre><p>This lets us determine the value of <code>c'_i</code> from the other received commitments without actually needing to learn the secret shares of other parties. You can see this commitment interpolation in code <a href="https://github.com/kewbish/kintsugi/blob/master/src/dpss.rs#L82">here</a>, with tests <a href="https://github.com/kewbish/kintsugi/blob/master/src/dpss_tests.rs#L64">here</a>.</p>
<p>I&rsquo;ll admit I couldn&rsquo;t have figured out how to interpolate these commitments without leaning on ChatGPT — I was barely familiar with single-variable Lagrange interpolation and couldn&rsquo;t fathom how I could manage both the secret sharing and the blinding factor. This term was the first where I bothered to try using it as a learning tool, and I was pleasantly surprised by how decent it was for crypto in particular. Sometimes, it&rsquo;d get the explanation itself wrong, but while walking through it, I&rsquo;d manage to get a key insight that let me fill in the rest correctly. Its errors were fairly easy to spot, especially when translating its output into code: things like mixing up scalar and point addition that an additional reprompt was often enough to fix.</p>
<h2 id="conclusion">Conclusion</h2>
<p>I hope this explainer has highlighted that the crypto isn&rsquo;t as scary as it sounds, and that the primitives of a paper aren&rsquo;t so hard to understand either. I think it&rsquo;d be neat if we required authors to submit a &ldquo;from the basics&rdquo; guide alongside their work, even if just to call out what commonly-understood terms like &ldquo;interpolate&rdquo; mean and where to look for further information<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<p>This isn&rsquo;t intended at all as a dig to the paper, which was otherwise very thorough and well-written. It was easy to read, and I appreciated the various comparison tables and practical benchmarking sections. The paper was a fundamental building block to my recent research work, and I&rsquo;d definitely recommend <a href="https://eprint.iacr.org/2022/971.pdf">giving it a read</a>. There&rsquo;s plenty more aspects of the paper that I haven&rsquo;t covered here, including how high-threshold secret sharing is handled and how they optimize resharing by amortizing batched operations. I wanted to focus this blog post on the main interpolation mechanisms in Algorithm 3, but here&rsquo;s a few more less-fleshed-out notes about Honey Badger:</p>
<div class="grid-element" style="margin-bottom: 0.5em">
  <details>
    <summary>More pointers on Honey Badger&#39;s properties</summary>
    <hr />
    <p>In the resharing section, we learned how we can recover the original secret <code>s</code>, but we still need to distribute the shares to the various parties. The paper outlines two options: one is to assume the presence of some trusted dealer, with full (albeit perhaps temporary) knowledge of the secret, and the other is to use a <em>distributed key generation</em> function (DKG), which can be used for a trustless setup. If we want to go the DKG route, however, the hitch is that Honey Badger is <em>asynchronous</em>, meaning that it&rsquo;ll continue to operate in face of arbitrary network delays. This therefore means that the DKG we choose also needs to be asynchronous. This is tricky, since you&rsquo;d usually expect DKGs to require some element of synchronization and consensus over which nodes to &rsquo;listen to&rsquo; during key generation. Honey Badger calls out <a href="https://eprint.iacr.org/2021/1591">Das et al.&rsquo;s asynchronous DKG</a> in particular — if you&rsquo;re interested in detangling this, I&rsquo;d recommend watching <a href="https://www.youtube.com/watch?v=A-3ZhG-7SI0">their conference presentation</a> for the same paper first.</p>
<p>Besides asynchrony guarantees, Honey Badger is also Byzantine-fault tolerant up to a third of nodes. This means that even if up to a third of nodes go rogue and actively try to submit incorrect shares for resharing or otherwise misbehave, Honey Badger can still continue with its resharing and refreshing operations. This is thanks to the multi-valued validated Byzantine agreement (MVBA) protocols used to agree on which nodes have emitted correct shares. I&rsquo;ll discuss this more in the next section on polynomial commitments. You can find their example implementations <a href="https://github.com/tyurek/dpss/blob/main/dpss/broadcast/tylerba2.py">here</a> — I didn&rsquo;t fully get my MVBA prototype working in time.</p>
<p>One last note I&rsquo;ll make is a distinction between the types of failures that Honey Badger can tolerate. One failure, as I&rsquo;ve just mentioned, is a Byzantine-fault failure, with actively malicious nodes. Honey Badger can handle up to a third of total nodes being Byzantine. On the other hand, it can only tolerate <code>t</code> honest-but-curious nodes that follow the protocol correctly (e.g. don&rsquo;t submit false <code>s_i</code>) but collude. Any more, and they&rsquo;ll be able to reconstruct <code>s</code> due to how SSS works. Otherwise, the SSS can instead tolerate having <code>n - t - 1</code> nodes being offline, since only <code>t+1</code> nodes are required to successfully recover <code>s</code>. I ran into these differences while trying to describe the overall fault-tolerance of a protocol I was developing that made use of Honey Badger, so I think it&rsquo;s worth considering here.</p>

  </details>
</div>

<p>If you&rsquo;re interested in other types of secret sharing, there are so, so many extra offshoots built off the same primitives that you can explore:</p>
<ul>
<li><em>Proactive</em> secret sharing, as mentioned above, is secret sharing where the shares are refreshed while keeping the secret itself the same. <a href="https://link.springer.com/chapter/10.1007/3-540-44750-4_27">Here&rsquo;s one of the seminal papers on this topic.</a></li>
<li><em>Dynamic</em> secret sharing, as mentioned above, lets the set of parties holding shares to change. Usually, this requires proactive secret sharing — otherwise, former shareholders could collude to reconstruct the secret. This is sometimes also called <em>mobile</em> secret sharing. <a href="https://dl.acm.org/doi/10.1145/1880022.1880028">Here&rsquo;s another paper on this.</a></li>
<li><em>Computationally secure</em> secret sharing limits the computational resources required to store shares. Secret sharing can require a lot of storage — growing linearly in the number of shares created. <a href="https://www.cs.cornell.edu/courses/cs754/2001fa/secretshort.pdf">This approach allows for more efficient sharing.</a></li>
<li>Similarly, <em>batched</em> secret sharing makes sharing multiple secrets more space-efficient. The Honey Badger paper includes a batch-amortized variant, and <a href="https://www.sciencedirect.com/science/article/abs/pii/S0020025510004536?via%3Dihub">here&rsquo;s another paper on the topic.</a></li>
<li><em>Verifiable</em> secret sharing allows parties to validate the correctness of a share. <a href="https://eprint.iacr.org/2023/1196.pdf">Here&rsquo;s one paper on this.</a></li>
<li><em>Threshold signatures</em> are slightly different than threshold secret sharing, but there are <a href="https://eprint.iacr.org/2022/1656">interesting papers on this as well</a>.</li>
</ul>
<p>I&rsquo;m sure there are plenty more variants I&rsquo;ve missed above — it became a bit of a game during my initial literature review to figure out which magic keyword combination I needed to find relevant papers.</p>
<p>In other news, my workshop paper on decentralized E2EE key recovery was accepted! It builds on the concepts I&rsquo;ve mentioned in this post and <a href="https://kewbi.sh/blog/posts/241020/">my previous one on OPAQUE</a>. The project is called <a href="https://en.wikipedia.org/wiki/Kintsugi">Kintsugi</a>, a play on how the protocol mends together encryption key backups from distributed shares. The demo implementation is <a href="https://github.com/kewbish/kintsugi">on GitHub</a>, and I&rsquo;ll update with a copy of the paper when that&rsquo;s available. I&rsquo;m pretty proud of the fact I was able to go from knowing very little about cryptography to defining an interesting research direction to implementing and writing a whole paper in less than ten weeks. I&rsquo;m happy that the project had a solid implementation portion and a focus on applied work, since historically I haven&rsquo;t sat well with theory-only projects. Nonetheless, I appreciated the stretch to dive into the technical bits of cryptography, as opposed to glossing over it and reaching for existing libraries. There&rsquo;s still plenty of design considerations to be finalized and details to be polished on the demo, which I&rsquo;ll be working through next, but this project has been an extremely fun and challenging exercise in cryptography and protocol design.</p>
<p>There are still a few papers that I&rsquo;ve waded through and think could be explained much better, so I might make this type of cryptography/systems paper explanation a running series here. Wrestling with papers more deeply more often has been a goal of mine for a while, and distilling it into explainers <a href="https://muratbuffalo.blogspot.com/">à la Murat</a> is helpful for keeping you accountable for carefully reading all the details. I have my eye on a few <a href="https://eprint.iacr.org/2024/887">systems</a> <a href="https://pdos.csail.mit.edu/~petar/papers/maymounkov-kademlia-lncs.pdf">implementations</a> <a href="https://www.usenix.org/conference/osdi20/presentation/dauterman-safetypin">papers</a>, or perhaps a more theoretical look at a <a href="https://eprint.iacr.org/2021/1591">distributed key generation</a> protocol, so we&rsquo;ll see what I come up with.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>The keen-eyed reader might notice that we&rsquo;ve been mixing <em>scalar</em> and <em>point</em> addition for the interpolation process. This has been termed doing the interpolation <a href="https://eprint.iacr.org/2017/363">&lsquo;in the exponent&rsquo;</a> — although the elliptic curve notation used here implies group multiplication, the result is equivalent to as if we&rsquo;d done the commitment interpolation in field arithmetic via exponentiation, then multiplied by the appropriate points.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>It&rsquo;s occurring to me that this might just be what appendices are commonly used for, but ideally this explainer should be in more casual language! We don&rsquo;t need thirty pages of proofs and all these Greek symbols to understand how to connect the dots to find a polynomial.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>Making OPAQUE Clear</title>
      <link>https://kewbi.sh/blog/posts/241020/</link>
      <pubDate>20 Oct 2024</pubDate>
      <author>Emilie Ma (Kewbish)</author>
      <description>On becoming less oblivious to OPRFs.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>I am not a cryptographer. I&rsquo;ve participated in enough CTFs to have had the refrain &ldquo;never roll your own crypto&rdquo; drilled into my head. Leave the group-modulo-n wrangling to the professionals, as the undertone went. While I won&rsquo;t encourage you to do so, today I&rsquo;ll give you enough of the basics to implement your own key exchange protocol. I&rsquo;ll leave the deploying it to prod to you.</p>
<p>To build up some backstory: I recently met someone, who, when I asked to exchange contacts, told me to add them on Signal, instead of Discord or Whatsapp. This was new. A few years earlier, I&rsquo;d had the same reaction when I needed to join some group chats that were hosted on Telegram<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. I&rsquo;ve just finished <a href="https://mitpress.mit.edu/9780262548182/tor/">a book about Tor</a> for a reading group, and attended a couple talks about <a href="https://en.wikipedia.org/wiki/Mix_network">mixnets</a>. While it might have something to do with the fact that my office is in the <a href="https://www.cambridgecybercrime.uk/">Cybercrime Centre</a>, recent news like the <a href="https://en.wikipedia.org/wiki/Pavel_Durov#2024_arrest_and_indictment">arrest of the Telegram founder</a> and the <a href="https://docs.google.com/document/d/1iWCqmaOUKhKjcKSktIwC3NNANoFP7vPsRvcbOIup_BA/edit?tab=t.0">viral AR I-XRAY glasses</a> have been bringing up topics around privacy and security.</p>
<p>To me, a core thread running through these concerns is the question of who we can trust with our data. There are growing movements to <a href="https://en.wikipedia.org/wiki/DeGoogle">abandon big-tech platforms</a> and use dumb phones without internet. Folks hop between the secure messaging platforms de jour, or at least onto the incumbent, which appears to be Signal. Signal, Protonmail, even something as familiar as Whatsapp: they&rsquo;re all end-to-end encrypted. This means the servers that run these platforms only store encrypted copies of your data, and the only people who can read your funnier-in-your-head texts are you and the intended recipient. This encryption usually uses big random numbers instead of passwords, since passwords are lower-entropy. These platforms also (usually) won&rsquo;t store your keys for you, since that&rsquo;d undermine the whole point of end-to-end encryption. You&rsquo;re therefore the only person in the world who can correctly decrypt any messages that are sent — nice and safe. But what happens when you lose your phone? Sure, the app prompted you to save a recovery file, store a twelve-word recovery phrase, or back up your keys somewhere before proceeding, but like most folks, you&rsquo;d probably just skipped past that step.</p>
<p>Let&rsquo;s use passwords, you say — folks know how to use them, they generally do a better job at recording them somewhere, and there are password manager tools readily available so it&rsquo;s rare you&rsquo;ll forget them. However, even besides the myriad misconceptions about strong passwords and the dangers of people reusing passwords, there&rsquo;s the fundamental problem that the platform&rsquo;s server will have to store your password. Nowadays, it&rsquo;s usually stored hashed, so that&rsquo;s mostly fine — should someone break into the service&rsquo;s database, their only choice is to brute-force or <a href="https://en.wikipedia.org/wiki/Rainbow_table">rainbow-table</a> their way to steal your original password.</p>
<p>Now consider what happens in between you hitting &lsquo;Log in&rsquo; on the auth page and the server processing your password — or more precisely, how the bytes of your password are transmitted. Yes, hopefully in this day and age it goes through TLS, so no one should be able to read it. But the core problem is that <em>it&rsquo;s still in plaintext</em>. Once the server receives your password via HTTP, it still needs to read and process it into a hash, to compare against the stored hash. And how is this processing done? In plaintext. That puts a lot of trust on the server to behave honestly. Even if the server behaves as expected, the hardware it runs on might be vulnerable to attacks: because the password is transmitted in plaintext, it&rsquo;s also in the RAM and cache in plain text. When I was at Cloudflare, I learned about some of the ways the team architected the Workers platform explicitly for better isolation on all levels, guarding against <a href="https://developers.cloudflare.com/workers/reference/security-model/">speculative execution bugs</a>, for example. If the server&rsquo;s using plain passwords, it&rsquo;s vulnerable to SPECTRE and other lower-level attacks like it.</p>
<p>It seems, then, that there&rsquo;s no safe option. Either you have better security at the risk of fallible users losing access to their data, or you get a more familiar user experience at the expense of many layers of security concerns. However, there&rsquo;s a way to augment passwords with some neat cryptography so that you can effectively get the best of both worlds. Enter OPAQUE: a password-based key exchange protocol that lets the end user input a password and save their keys on the server, while not allowing the server any access to the password. It retains the server-has-zero-knowledge properties that we&rsquo;d expect in an E2E setting and requires both user and server to participate in any login attempts, reducing the feasibility of brute-force attacks. OPAQUE was selected for standardization by the IETF over several other similar password-based protocols, including the other top contender <a href="https://en.wikipedia.org/wiki/Secure_Remote_Password_protocol">SRP-6a</a>, because of this defense against brute-force attacks. It&rsquo;s also been implemented by several companies, including <a href="https://blog.cloudflare.com/opaque-oblivious-passwords/">Cloudflare</a> and <a href="https://github.com/facebook/opaque-ke">Facebook</a>.</p>
<p>In my current research internship, I&rsquo;ve been working with variants of OPAQUE as applied to key recovery for E2E services. We&rsquo;re figuring out how to adapt OPAQUE to store and retrieve a user&rsquo;s private key via a password: for example, in cases where they&rsquo;ve lost access to their old devices. We needed OPAQUE&rsquo;s properties: we don&rsquo;t want the server to be able to reconstruct the password and read the user&rsquo;s keys, nor do we want the user to keep the only copy of keys locally and end up susceptible to brute-force if any encrypted info leaks. We&rsquo;re adding some other goodies on top, too, but I had to implement a vanilla version first. When I was doing so, I had to trawl through tens of pages of dense crypto papers and cross reference the <a href="https://www.ietf.org/archive/id/draft-irtf-cfrg-opaque-17.html">OPAQUE spec</a> with the myriad <a href="https://github.com/search?q=opaque+protocol&amp;type=repositories">example repos</a>, but I think the core ideas boil down much more intuitively.</p>
<p>Protocols like OPAQUE shouldn&rsquo;t be secure-by-obscurity, and certainly not secure-by-lack-of-good-high-level-public-explanations. This post aims to give you the walkthrough of OPAQUE I wish I had when I embarked on this project. I&rsquo;ll assume some general CS knowledge, but otherwise I&rsquo;ll provide the context you need if you&rsquo;re starting from scratch. I&rsquo;ve chosen to gloss over some of the related crypto concepts to focus on just what&rsquo;s needed to understand the OPAQUE exchange, but I&rsquo;ve included links and mentioned other keywords if you&rsquo;d like to dive deeper.</p>
<p>I hope this post will serve to make OPAQUE clear and you less wary about crypto — reams of LaTeX can be scary, but I promise this won&rsquo;t be.</p>
<h2 id="background-elliptic-curves">Background: Elliptic Curves</h2>
<p>One of the concepts I&rsquo;ll assume some background in is <a href="https://en.wikipedia.org/wiki/Public-key_cryptography">public-key cryptography</a>. The basic idea is that you store two keys: one public and one private. You share your public key with others, and keep your private key to yourself, as the names may suggest. To encrypt something, you take your private key and the public key of the intended recipient together and do some operations on it, which ensures that only the recipient can decrypt your message, since only they have the private key associated with their public key. The idea is that public keys are derived from private keys using some hard-to-reverse operation, and that you can easily derive the public key from the private key, but not vice versa. For example, multiplying numbers is easy, but factoring numbers is hard, so part of what underlies the <a href="https://en.wikipedia.org/wiki/RSA_(cryptosystem)">RSA cryptosystem</a> is that you can&rsquo;t easily factor numbers to derive the secrets that are used to create the private key<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<p>Elliptic-curve cryptography works on the same principle. The hard-to-reverse operation here is multiplying a point on the curve with a number. Let&rsquo;s build up to multiplying with points on the curve by first adding a point to itself, which we can then repeat <em>n</em> times to get multiplication by <em>n</em>.</p>
<p>We first have to start with the given point on the curve. The formula of this curve will change depending on the particular curve you use (and there are plenty), so I won&rsquo;t go into detail about it now. Pick a point <code>P</code>. To add the point to itself, you&rsquo;ll need to draw the tangent line to the curve at <code>P</code> — this is the line that follows the shape of the curve at point <code>P</code>. Extend the tangent line far enough, and you&rsquo;ll find that it intersects exactly one point on the curve. Call this point the tangent line intersection.</p>
<figure><img src="/img/241020/adding-points-1.png"
         alt="Figure 1. Drawing the tangent line to P."/><figcaption>
            <p><em>Figure 1. Drawing the tangent line to P.</em></p>
        </figcaption>
</figure>

<p>This tangent line intersection point is then reflected in the x-axis to get the doubled point, <code>2P</code>.</p>
<figure><img src="/img/241020/adding-points-2.png"
         alt="Figure 2. Reflecting across the x-axis to get 2P."/><figcaption>
            <p><em>Figure 2. Reflecting across the x-axis to get 2P.</em></p>
        </figcaption>
</figure>

<p>We can then repeat this again, with the tangent line for the point <code>2P</code>, to get <code>3P</code>, and so on. There are several optimizations to do this multiplication faster, including the <a href="https://en.wikipedia.org/wiki/Elliptic_curve_point_multiplication#Double-and-add">double-and-add</a> algorithm.</p>
<figure><img src="/img/241020/adding-points-3.png"
         alt="Figure 3. Repeating to find 3P."/><figcaption>
            <p><em>Figure 3. Repeating to find 3P.</em></p>
        </figcaption>
</figure>

<p>The private key in elliptic curve cryptography is the choice of a number <code>n</code>, and the public key is the curve&rsquo;s basepoint multiplied by <em>n</em>. The curve&rsquo;s basepoint is a point that&rsquo;s defined along with the curve (technically, the generator of the group we&rsquo;ll be operating in) — just consider it as a constant that&rsquo;s handed to you together with the definition of the curve. It&rsquo;s easy to compute the public key given the private key, since the multiplications aren&rsquo;t hard. However, if you want to recover the private key given just the resulting public key point, you&rsquo;ll have to try every single possible value of <code>n</code>, which is assumed to be infeasible. This multiplication is called the <a href="https://en.wikipedia.org/wiki/Discrete_logarithm_records#Elliptic_curves">elliptic curve discrete log problem</a>, as an analogy to the discrete log problem of finding the number <code>x</code> such that <code>g^x = y</code> for some public number <code>y</code>.</p>
<p>One thing to note is that if you know the number <em>n</em> that you multiplied by, it&rsquo;s easy to &lsquo;undo&rsquo; a multiplication as well. You can multiply by its inverse — think of it like doing <code>n * P * 1/n</code> — to recover just your original point. You might wonder why we can&rsquo;t do something similar to recover the number <code>n</code> from our public point, since the curve&rsquo;s basepoint is a public parameter. However, you can&rsquo;t analogously &lsquo;invert&rsquo; a point, so you&rsquo;ll still have to end up trying all the possible values of <code>n</code>. The core takeaway of all this is that division by a number is easy, and division by a point to get the number back is hard.</p>
<h2 id="metam-oprf-asis">Metam-OPRF-asis</h2>
<p>Let&rsquo;s put that elliptic curve theory to work, starting with the main building block of OPAQUE: the oblivious pseudo-random function (OPRF). We can build up what an OPRF is step by step:</p>
<ul>
<li>A function is a mapping from a domain of inputs to a range of outputs.</li>
<li>A <a href="https://en.wikipedia.org/wiki/Pseudorandom_function_family#Motivations_from_random_functions">random function</a> is an entirely random mapping from inputs to outputs. This means the random outputs should be <a href="https://en.wikipedia.org/wiki/Randomness_test">uniformly distributed</a>, which means there&rsquo;s no obvious bias in the outputs.</li>
<li>A pseudo-random function <em>looks</em> like it&rsquo;s an entirely random mapping, but is actually deterministically mapping inputs to outputs. It&rsquo;s very important that it <em>looks</em> like a random function, so it also needs to have a uniform distribution of outputs.</li>
<li>An oblivious pseudo-random function is a random function that requires two people to evaluate, where neither person learns what the other put in — they&rsquo;re <em>oblivious</em> to the other party&rsquo;s input.</li>
</ul>
<p>More concretely, let&rsquo;s describe the two people as a client and a server and define an OPRF as a function <code>F(server_input, client_input)</code> such that the client never learns the server input, and the server never learns the client input nor even the final result of the function. I think <a href="https://en.wikipedia.org/wiki/Oblivious_pseudorandom_function">Wikipedia</a> and other resources do a terrible job of explaining how this is possible, because intuitively, it isn&rsquo;t. How can the server, who&rsquo;s evaluating the function, not know its own output? How can the server also not know the client&rsquo;s input, if it was required as part of the function&rsquo;s inputs in the first place?</p>
<p>The answer lies in the elliptic-curve operations I touched on before. Here&rsquo;s how the OPRF actually works:</p>
<figure><img src="/img/241020/oprf-exchange.png"
         alt="Figure 4. The full OPRF exchange."/><figcaption>
            <p><em>Figure 4. The full OPRF exchange.</em></p>
        </figcaption>
</figure>

<ul>
<li>The client has <code>x</code>, which is a number it wants to keep secret. It multiplies this point by the curve&rsquo;s basepoint, so now we have a point. Let&rsquo;s call this point <em>x_point</em>.</li>
<li>To keep it secret, the client generates a random number <code>r</code>, which is called the <a href="https://en.wikipedia.org/wiki/Oblivious_pseudorandom_function#EC_and_conventional_Diffie%E2%80%93Hellman"><em>blinding factor</em></a>. The client calculates <code>r * x_point</code> and sends that to the server as <code>client_input</code>. The server doesn&rsquo;t know <code>r</code>, so it can&rsquo;t get the original <code>x_point</code> back. This protects the server from learning what the client inputted, but it&rsquo;s easy for the client to undo this later.</li>
<li>The server receives <code>client_input</code>, and multiplies it by its own secret number, <code>server_input</code>. (In the diagram, I called this secret number <code>key</code> to save space.) This makes the output <code>= client_input * server_input = r * x_point * server_input</code>. The server can&rsquo;t learn the actual execution output without the blinding factor <code>x_point * server_input</code> nor the <code>x_point</code> itself, because that pesky <code>r</code> is there.</li>
<li>The client receives <code>output</code>, and multiplies it by <code>1/r</code>. This lets it recover <code>server_input * x_point</code> — remember that division by a number is easy. However, the client still can&rsquo;t learn <code>server_input</code> either — remember that division by a point (<code>x_point</code>) is hard.</li>
</ul>
<p>To summarize:</p>
<ul>
<li>We want to end up with the client&rsquo;s input multiplied by the server&rsquo;s input, without either party knowing the other&rsquo;s value.</li>
<li>The property that dividing by points is hard prevents the server from learning the client&rsquo;s blinding factor or secret point and similarly prevents the client from learning the server&rsquo;s secret.</li>
<li>The property that dividing by numbers is easy enables the client to unblind the result to get the required <code>client_input * server_input</code>.</li>
</ul>
<h2 id="registration">Registration</h2>
<p>That&rsquo;s actually all the hard crypto out of the way! Let&rsquo;s now cover the three main phases of OPAQUE: the registration, login, and key exchange. The first phase is registration, where the user will use their password (a string) in an OPRF exchange to get an encryption key that only they know that they then can use to encrypt their keypair data.</p>
<p>The registration revolves around an OPRF exchange.</p>
<figure><img src="/img/241020/opaque-registration.png"
         alt="Figure 5. OPAQUE registration."/><figcaption>
            <p><em>Figure 5. OPAQUE registration.</em></p>
        </figcaption>
</figure>

<ul>
<li>The client transforms their password into a point on the curve. This is done via <a href="https://datatracker.ietf.org/doc/rfc9380/">&lsquo;hash-to-curve&rsquo; functions</a> that allow you to take arbitrary inputs to points on the curve. This is the <code>x_point</code> in the OPRF explanation above. The client then blinds their password point with some random blinding factor, <code>r</code>.</li>
<li>The server receives this <code>client_input</code>. The server generates a user-specific keypair that&rsquo;ll just be used for this user. This user-specific private key will be the <code>server_input</code> in the OPRF explanation above. It multiplies the <code>client_input</code> with its <code>server_input</code> and returns this value to the client, along with the server&rsquo;s public key.</li>
<li>The client receives this <code>output</code> and unblinds it. The client now has <code>x_point * server_input</code>. Let&rsquo;s call this unblinded output <code>rwd</code> — it&rsquo;ll be used as the key to (symmetrically) encrypt what we call the <em>envelope</em>.</li>
<li>The client generates an <em>envelope</em>, which includes a new keypair that the client will use in communications with this server. It also puts the server&rsquo;s public key into this envelope. The client then encrypts all of this with the <code>rwd</code> and sends the encrypted envelope to the server.</li>
<li>The server saves the encrypted envelope so the user can access it again later. It can&rsquo;t decrypt this envelope, since it has no way of unblinding its output to recover the <code>rwd</code>.</li>
</ul>
<p>Note that in every step of this process, the server will never learn the password nor the <code>rwd</code> used to encrypt the envelope, so the client can safely trust the server to store its information. This is critical for end-to-end encryption systems.</p>


<div class="grid-element" style="margin-bottom: 0.5em">
<details>
<summary>
A fun challenge: given the protocol as specified above, can you find the DOS attack vector?
</summary>
<hr>
<p>
It's possible for a malicious user masquerading as the client to intercept the server's output and unblind it with some random number, making the `rwd` that the malicious user calculates meaningless. This doesn't matter, though, because it can then encrypt jibberish with the `rwd` or just directly send junk to the server, which will happily store it under the original user's identifier. When the original user tries to log in, they won't be able to decrypt the envelope they retrieve from the server, which effectively DOSes their account for any future use. This means there needs to be some way of checking that the original user is the same one who provides the blinded input and the encrypted envelope.
</p>
<p>
My supervisor pointed this out in my initial implementation of OPAQUE, and I was pretty confused when I saw that none of the other implementations on GitHub handled this in any way. I ended up asking <a href="https://github.com/expede">Brooke Zelenka</a> about it, and she pointed me to <a href="https://www.ietf.org/archive/id/draft-irtf-cfrg-opaque-17.html#name-registration">this section of the OPAQUE spec</a>, which states that registration requires some other method of the server authenticating the client to ensure that it's talking to the right one. I think you can get past this if you assume enough things about the communication channel on which messages are exchanged, but I just slapped some signatures onto the messages to ensure authenticity and called it a day.
</p>
</details>
</div>


<h2 id="logging-in">Logging in</h2>
<p>Logging in also relies on a similar OPRF exchange. This time, the user needs to recover <code>rwd</code> so it can decrypt the encrypted envelope that the server returns.</p>
<figure><img src="/img/241020/opaque-login.png"
         alt="Figure 6. OPAQUE login."/><figcaption>
            <p><em>Figure 6. OPAQUE login.</em></p>
        </figcaption>
</figure>

<ul>
<li>The client transforms their password into the same point on the curve, and chooses a new random blinding factor <code>r</code>. The client blinds their password point and sends it over.</li>
<li>The server receives this <code>client_input</code> and multiplies it with the same <code>server_input</code> secret that it used during registration. The server sends this <code>output</code> along with the stored encrypted envelope to the client.</li>
<li>The client receives this <code>output</code> and their envelope. It unblinds the <code>output</code> to recover the <code>rwd</code> and uses the <code>rwd</code> to decrypt the envelope. The user has now recovered their service-specific keypair and can move onto a key exchange or further communications with the server, since the decrypted envelope will include the server&rsquo;s public key.</li>
</ul>
<p>The two main benefits of OPAQUE were that it prevents the client and server from learning anything about what the other party&rsquo;d stored or used to compute the function and that it avoids offline brute-force attacks. We&rsquo;ve previously discussed how the OPRF provides this first property via the blinding factors and elliptic-curve cryptography, but let&rsquo;s also briefly touch on the brute-force attack part. Without the OPRF, you might just encrypt the envelope with your password directly and send that to the server to store. This is both less secure than using a key, which is likely longer and has more entropy, but also allows any malicious parties, including a dishonest server, to intercept your encrypted envelope and mount an offline brute-force attack. In theory, the cryptography should ensure this takes a very long time, but the OPRF gives you the additional guarantee that any attacker must interact with the server to get their password guess multiplied by the <code>server_input</code>. This means the server is able to detect and rate-limit password attempts, making brute-force much slower than it would be otherwise.</p>
<h2 id="the-ke-rry-on-top">The KE-rry On Top</h2>
<p>The final piece of OPAQUE is the key exchange that needs to follow in order to derive a shared secret with which to encrypt all following communication. I focused less on this part of the protocol, since for my project we were only interested in the recovery of the client&rsquo;s keypair from the envelope. As well, once you&rsquo;re done with the OPRF exchanges, you&rsquo;re in some sense &lsquo;back in safe territory&rsquo; — there are plenty of key exchange protocols proposed, and you could probably plausibly choose any one of them. <a href="https://eprint.iacr.org/2005/176.pdf">HMQV</a>, a Diffie-Hellman variant, was chosen in the original <a href="https://eprint.iacr.org/2018/163.pdf">OPAQUE paper</a> for its performance. However, <a href="https://blog.cloudflare.com/opaque-oblivious-passwords/">Cloudflare&rsquo;s OPAQUE explainer</a> leverages TLS as an AKE, and the <a href="https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-opaque-01">specification</a> mentions another variant using a <a href="https://www.iacr.org/cryptodb/archive/2003/CRYPTO/1495/1495.pdf">SIGMA-I</a> Diffie-Hellman variant.</p>
<p>For the sake of completeness, I&rsquo;ll briefly cover the HMQV calculation used in the original OPAQUE paper. We want to derive a shared secret that both the client and server can calculate, and use this as a key going forward.</p>
<ul>
<li>As part of the login (or in a separate round-trip message), the client chooses a random number <code>c</code>. It multiplies the curve&rsquo;s basepoint with this to get a public point <code>C</code>. This is sent to the server.</li>
<li>Likewise, the server chooses a random number <code>s</code> and multiplies it with the curve&rsquo;s basepoint to get <code>S</code>.</li>
<li>Both client and server then compute a couple of hashes (remember that these are effectively numbers, not points.) First, the client computes <code>blinded_session_identity = H(client_identity, server_identity, r)</code> with its blinding factor. Then, let <code>e_u = H(C, server_identity, blinded_session_identity)</code> and <code>e_s = H(s, server_identity, blinded_session_identity)</code>. The exact details of this are less important to the overall protocol — this just ensures you&rsquo;re talking to the right person.</li>
<li>Then, the client computes its shared secret as <code>H((S + e_s * server_public_key) * (c + e_c * client_private_key))</code> and the server computes <code>H((C + e_c * client_public_key) * (s + e_s * server_private_key))</code>. The first parenthesis of each expression contains the public parameters that are known from the other party, and evaluates to a point; the second parenthesis contains the private parameters from the party computing the hash and evaluates to a number.</li>
</ul>
<p>If we expand both sides, we&rsquo;ll see that what&rsquo;s being hashed is the same. Feel free to skip the proof if you&rsquo;re happy to just trust that the above are equal, but it&rsquo;s neat looking at variants of Diffie-Hellman to prove how both client and server derive the same secret. I&rsquo;d encourage you to give it a go — it&rsquo;s very satisfying to see everything fall into place and the base math isn&rsquo;t challenging, though keeping all the variables in line and recognizing when to factor terms in and out can be a nice puzzle. Let <code>G</code> be the curve basepoint:</p>
<pre tabindex="0"><code>  (S + e_s * server_public_key) * (c + e_c * client_private_key) // what the client hashes
= (s * G + e_s * server_public_key) * (c + e_c * client_private_key)
= (s * G) * (c + e_c * client_private_key) + (e_s * server_public_key) * (c + e_c * client_private_key) // distribute the multiplication
= (s * G * c) + (s * G * e_c * client_private_key) + (e_s * server_public_key) * (c + e_c * client_private_key)
= (s * G * c) + (s * G * e_c * client_private_key) + (e_s * server_private_key * G) * (c + e_c * client_private_key)
= (s * G * c) + (s * G * e_c * client_private_key) + (e_s * server_private_key * G * c) + (e_s * server_private_key * G * e_c * client_private_key)
= (s * G * c) + (e_s * server_private_key * G * c) + (s * G * e_c * client_private_key) + (e_s * server_private_key * G * e_c * client_private_key) // rearranged terms
= (s * G + e_s * server_private_key * G) * c + (s * G + e_s * server_private_key * G) * e_c * client_private_key // factor out c and e_c * client_private_key
= (s + e_s * server_private_key) * c * G + (s + e_s * server_private_key) * e_c * client_private_key * G // factor out G
= (s + e_s * server_private_key) * (c * G + e_c * client_private_key * G) // factor out first term
= (s + e_s * server_private_key) * (C + e_c * client_public_key)
= (C + e_c * client_public_key) * (s + e_s * server_private_key) // what the server hashes
</code></pre><p>This completes the authenticated key exchange, and in turn, the OPAQUE protocol!</p>
<h2 id="conclusion">Conclusion</h2>
<p>When I was first looking into implementing OPAQUE, my supervisor sent me the original paper as some helpful reading, but after seeing the PDF was 61 pages long, I bailed out to go read through the <a href="https://blog.cloudflare.com/opaque-oblivious-passwords/">Cloudflare explainer</a> instead. The original paper only has the full protocol laid out on page 47! The rest of the paper, and even the protocol description itself, is very dense — I suppose it&rsquo;s nicely concise for those who have been in the field for long enough that they can parse the math at first glance, but trudging through all of that isn&rsquo;t fun for a first-timer. I found the crypto explainers in general to also be at weird levels of abstraction that didn&rsquo;t immediately make it clear how primitives built together, particularly for topics like elliptic-curve cryptography or OPRFs.</p>
<p>In general, I&rsquo;ve noticed that theoretical crypto papers always start with a sea of security games where they prove the correctness and security of their protocol, but not actually explain the protocol until later, as if the protocol is an afterthought that derives naturally from the security games. Again, this probably makes sense for the cryptographers who are focusing on the security, but you&rsquo;d think you&rsquo;d put your major contribution up front in the paper. Another thing I&rsquo;ve noted about crypto papers is how little discussion they usually have. Unless it&rsquo;s an applied crypto paper where the system has been fully implemented and benchmarked, there&rsquo;s usually at most a page or so of discussion, which mostly consists of re-explaining that they protocol is better than the others that currently exist. The conclusion is also usually on the order of a paragraph or two, which is a far cry from the systems/HCI papers that I&rsquo;d read. As well, one thing that&rsquo;s nice is that the modern crypto papers in major journals are mostly all available freely via the <a href="https://iacr.org/publications/">IACR</a>. Having almost all my references centralized on the IACR archives and the IACR having a sequential numbering scheme have had the side effect of me memorizing the numbers of the key papers I&rsquo;ve been referencing over and over again, to the point that I can type in the first digit out of the three or four digit ID and have Chrome autofill the rest as the first search result. Such is crypto research.</p>
<p>I came into this project not really having much crypto background besides understanding the basics of public key cryptography and what an elliptic curve was. The main resources I used to hack my way through were Wikipedia entries, which were usually less notation-heavy than papers, Cloudflare explainers published via their blog, and, in an unusual-for-me turn, ChatGPT. It&rsquo;s surprisingly decent at explaining crypto and math topics — I was very wary of it getting things wrong, but it turns out even if it&rsquo;s messing up some of the details, it&rsquo;s good enough at imparting the intuition that lets you get a skeleton of an implementation done, enough to check it against the expected outputs from Wikipedia or the actual paper. I did write the code myself, since it wasn&rsquo;t quite understanding the libraries I needed to use, but it did a fair job of pointing me in the right direction or mentioning keywords (even if they were explained in the wrong contexts) that I could check against other resources. I&rsquo;m decidedly less anxious about using ChatGPT as an assistant now, and I expect it to keep giving me enough nudges to make my way to the end of my project.</p>
<p>Anyways, I hope this explainer has been clearer than the usual seas of math notation that never tell you how things fit together or how the intuition works. I&rsquo;m thinking of doing the same sort of explainer for some other cryptography topics that I&rsquo;ve had to wrestle with for my project recently, like Shamir secret sharing. As my supervisor said, warning people not to roll their own crypto is a bit patronizing when you think of it: it discourages folks from really understanding the protocols they&rsquo;re relying on and creates this out-group of folks who think they&rsquo;re not good enough to do crypto. It emphasizes this mindset where people aren&rsquo;t trusted to understand any crypto well enough to not mess it up. While this was probably trying to prevent people from writing their own very easily breakable ciphers and things of that more trivial nature, I think it&rsquo;s a bit of a gatekeep-y refrain. Go forth and implement your own OPAQUE — hopefully this explainer has made you a little less oblivious to OPRFs and OPAQUE and how to build them!</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Admittedly, not generally known for being truly secure, particularly against malicious Telegram employees, but it marketed itself as something different to the conventional chat platforms I&rsquo;d used before then.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>When I say &lsquo;hard&rsquo; in this post, I generally mean &lsquo;currently no one thinks it&rsquo;s possible&rsquo;, but that doesn&rsquo;t roll off the tongue quite so well. If it makes you happier, replace &lsquo;hard&rsquo; with &lsquo;requires exponential-time brute-force&rsquo;, and &rsquo;easy&rsquo; with &lsquo;polynomial-time or less&rsquo;.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>Truth or DARE</title>
      <link>https://kewbi.sh/blog/posts/241006/</link>
      <pubDate>06 Oct 2024</pubDate>
      <author>Emilie Ma (Kewbish)</author>
      <description>On Darmstadt and distributed systems.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>I think distributed systems are neat, but I also don&rsquo;t think I understand them well enough to warrant my level of interest in them. I&rsquo;ve never taken a distributed systems course, have only stumbled my way through the <a href="https://fly.io/dist-sys/">Fly.io distributed systems challenges</a>, and have never wrestled with more complicated design constraints while architecting something from the ground up. Whenever folks ask me about what particular subfield I&rsquo;m interested in, I don&rsquo;t have a concrete answer. Sure, I&rsquo;ve read <a href="https://books.google.co.uk/books/about/Designing_Data_Intensive_Applications.html?id=zFheDgAAQBAJ"><em>Designing Data-intensive Applications</em></a> and have covered the concepts behind different consistency models at least three times, but I still tended to butcher their distinctions when explaining them. If distributed systems was a band, I&rsquo;d definitely be labelled as a fake fan. I&rsquo;ve wanted to do something about this for a while — I&rsquo;ve wanted to finally <em>understand</em> distributed systems and be a <em>real</em> distsys engineer. So when a great opportunity cropped up, I shipped myself off to Germany to spend a week among brutalist buildings, looming castles, and most importantly, a cohort of grad students and eminent researchers, to see if I could make some progress.</p>
<p>The <a href="https://dare-summer.github.io/">Second ACM Europe Summer School on Distributed and Replicated Environments</a> is not only a bit of a mouthful — it&rsquo;s also a summer program that invites grad students interested in distributed systems together to participate in lectures and labs taught by leading professors in the area. Last year, it took place in Brussels, Belgium, and this year, we were hosted in Darmstadt, Germany, at the <a href="https://www.tu-darmstadt.de/index.en.jsp">Technische Universität Darmstadt</a>. Each day of the program consisted of several chunks of either lectures, which were more formal presentations about some new research, or labs, which were structured like a programming assignment and set us free to tackle some extension problem related to the lecture. A list of the speakers is available <a href="https://web.archive.org/web/20240929083858/https://dare-summer.github.io/speakers/">here</a>, and the program for this year is <a href="https://web.archive.org/web/20240929083842/https://dare-summer.github.io/program/">here</a>.</p>
<p>I&rsquo;m currently an undergrad, so I would normally be ineligible. However, Professor Kleppmann (author of aforementioned <em>Designing Data-intensive Applications</em>) was supervising me for my current research internship and also speaking at DARE, so he encouraged me to apply. I&rsquo;m very grateful that I was invited to apply, and immensely surprised that my prior research experience and quickly-put-together motivation letter sufficed to make up for my lack of a degree.</p>
<p>My previous experience in distributed systems work was primarily through a <a href="https://www.cs.ubc.ca/students/undergrad/courses/specialty">CPSC448 directed studies</a> that touched on formal verification, though I didn&rsquo;t do any actual proving. As well, my work at Cloudflare and at Stripe were fairly relevant: while I wasn&rsquo;t directly working on quote-unquote distributed systems, I was doing infrastructure work that was appropriately flavoured as such. My motivation letter listed these, as well as name-dropping an <a href="https://kewbi.sh/blog/posts/240526/">unrelated paper</a> I&rsquo;d worked on in the software engineering space. I don&rsquo;t think the applications were particularly selective, since the <a href="https://tuda-dare24.hotcrp.com/">HotCRP</a> listed 32/36 submissions accepted, and I&rsquo;d assume at least a couple of those were test submissions. I was still very nicely surprised when I&rsquo;d heard back about my acceptance, though, and I&rsquo;m again very thankful I was granted the opportunity.</p>
<p>I&rsquo;d taken notes during each lecture and wanted to revisit them, so this is a post summarizing my main takeaways from the talks and from the program overall. Each of the subsections here could be a blog post in itself — this will be the longest post on my blog to date, but it&rsquo;s also because it&rsquo;s covering a packed week of dense lecture material. I&rsquo;ve written up my notes in the same order as the lectures occurred, so you&rsquo;ll get to relive the learning at DARE as it happened. Professors and other students: I might&rsquo;ve made some mistakes or simplifications in the below — please forgive me!</p>
<h2 id="martin-kleppmann--bft-decentralized-access-control-lists">Martin Kleppmann — BFT Decentralized Access Control Lists</h2>
<p>Professor Kleppmann&rsquo;s lecture turned out to be very closely aligned to what I&rsquo;ll be working on this term with him, and it was nice getting the high-level summary lecture at DARE before diving into extension work. His talk revolved around a decentralized access control list protocol, where members can be added and removed from a group chat. This seems simple, but there are tricky edge cases when concurrency gets involved: if two people concurrently remove each other, what happens? What happens if a user who is removed by another user concurrently adds their own guest user? The protocol also needs to be Byzantine-fault tolerant, meaning that the system must continue to provide a consistent access control model despite having nodes that don&rsquo;t correctly follow the protocol. All in all, not as easy as it seems.</p>
<p>We started by covering some of background — how can you implement a BFT system. One approach is a version vector, where every replica keeps track of its idea of the state of the other replicas. However, they aren&rsquo;t safe in a Byzantine context, since a malicious replica can send different updates to different nodes with a version vector crafted to confuse the two nodes into thinking they&rsquo;re in sync when they&rsquo;ve actually diverged.</p>
<p>You can solve this, though, by using a hash graph, like in Git. Each &lsquo;commit&rsquo; or operation contains the hash of the previous one, which implicitly will transitively include the hash of the predecessor&rsquo;s predecessor, and so on. If the hashes of the most recently received operation are the same, you can assume that two nodes are in sync, relying on the property that cryptographic hash collisions are hard to find. Otherwise, you can keep moving backwards in the graph until the two nodes converge on a common point, then exchange all updates after that point. This is good for a Byzantine context, but is inefficient in the number of roundtrip exchanges, since both nodes will have to continuously communicate as they &lsquo;move backwards&rsquo; in the hash graph. It&rsquo;d be nice if we could figure out the set of updates to share more efficiently.</p>
<p>Generally, you can speed up set membership checks by using <a href="https://en.wikipedia.org/wiki/Bloom_filter">Bloom filters</a> to approximate things. <a href="https://martin.kleppmann.com/2020/12/02/bloom-filter-hash-graph-sync.html">A paper</a> he worked on applied that exact idea, which seems like such an obvious idea once you think about it. Both nodes can send a Bloom filter of the operations they currently know about and walks backward to check if operations were in the other side&rsquo;s Bloom filter, and if not, sends them over. This repeats until the nodes are in sync again — no more redundant updates. Someone once told me that the best research is the intuitively obvious stuff, and I think this paper is no different. Applying a relatively basic CS concept to a new problem space in exactly the way the concept should be used makes the work both easier to understand and to defend. I hope that I&rsquo;ll be able to hone my &lsquo;finding obvious directions&rsquo; skills more to take advantage of this.</p>
<p>Returning to the hash graph, Professor Kleppmann also went a bit into detail about blockchains. They&rsquo;re overall similar to hash graphs, but instead have a total ordering of blocks, which requires Byzantine-fault tolerant consensus. This is needed in blockchain settings to prevent double-spending of currency, but in this decentralized ACL group application we don&rsquo;t need there to be only one ordering of operations — we just need some sort of convergence. This taught me about the difference between consensus and collaboration: in consensus, nodes decide on one alternative; in colaboration, we can keep all alternatives and need to figure out how to put them together.</p>
<p>With all this background, we then worked through some design decisions around how to handle the ACL group edge cases mentioned above (spoiler alert: handling them is still active research). Some alternative systems were brought up, like Matrix&rsquo;s <a href="https://matrix.org/docs/older/moderation/#:~:text=better%20to%20redact.-,Power%20levels,-The%20next%20line">power levels</a> or some sort of seniority-based system. We ruled out user voting, since that&rsquo;s susceptible to Sybil attacks, where malicious nodes control a coordinated group of users, and social engineering. For our lab, we were given some starter code implementing one such ACL CRDT and were given freedom to explore implementing some of these ideas.</p>
<h2 id="elisa-gonzalez-boix--crdt-fundamentals-and-ambienttalk">Elisa Gonzalez Boix — CRDT Fundamentals and AmbientTalk</h2>
<p>Professor Gonzalez Boix presented their work on AmbientTalk, an actor-based programming language that supports communication across peers without any explicit infrastructure. In particular, it supports volatile connections that might drop any time.</p>
<p>The language is service-based, with events for when services are discovered and when messages are received or sent. You can define custom services in these languages and use the asynchronous messaging and lease primatives to communicate between them. The <a href="https://soft.vub.ac.be/amop/at/introduction">intro page</a> has more examples, but here&rsquo;s an overview of what it looks like:</p>
<pre tabindex="0"><code>whenever: Service
discovered: {
	|ref| // far reference
	when: message_type@FutureMessage()
	becomes: {
		|ref|
		...rest
	}
}

def service := object: {
    def message() {
	...
	return value
	}
}

export: service as: Service;
</code></pre><p>The language has a concept of <a href="https://soft.vub.ac.be/amop/at/tutorial/actors#actors_and_far_references">far references</a>, which as far as I can tell originated in <a href="https://en.wikipedia.org/wiki/E_(programming_language)">E</a>. These far references are like pointers across machine boundaries that work asynchronously. Messages that are sent to far references when connectivity drops are buffered in a queue and will be resent eventually later. Objects are passed between actors as far references, but primitive data is shared via isolates, working in a pass-by-copy fashion.</p>
<p>Most of the talk was about some CRDT fundamentals, which I won&rsquo;t go into here, as well as AmbientTalk&rsquo;s unique features, but I also wanted to mention the fun lab we had working in it. We were playing around with an interactive shopping list example app based on a CRDT that was handling its network communication via AmbientTalk&rsquo;s runtime. It was nice not having to code any of that and seeing things &lsquo;just work&rsquo;. The idea was that the shopping list app was collaborative, so if you specified the same <code>Service</code> name, you&rsquo;d be able to dynamically discover peers on the same network and listen to their messages. This ran into a slight challenge, because there were thirty of us in a small room all competing for the same messages, but we worked around this by all declaring services with slightly different names. Some of us managed to get collaboration to work across devices, but something to do with my firewall or the eduroam network we were on wasn&rsquo;t letting me try that particular feature out. This was still one of my favourite labs of the program, though.</p>
<h2 id="antonio-fernandez-anta--amecos">Antonio Fernandez Anta — AMECOS</h2>
<p>One of Professor Anta&rsquo;s students actually presented a poster about his lecture&rsquo;s research project at our poster session on the first day, so I had seen a bit of the background before the talk, although I&rsquo;ll admit I still didn&rsquo;t fully follow. The work presented was called AMECOS: A Modular Event-based framework for Concurrent Object Specification, In a similar vein to Professor Kaki&rsquo;s talk below, they noted that currently, concurrent objects are generally specified sequentially and assume some way to keep track of the object&rsquo;s state.</p>
<p>They define events as &lsquo;opex&rsquo;es, or &lsquo;operation executions&rsquo;, then define the various consistency models around the properties that these opexes would hold. For example:</p>
<ul>
<li>Linearizability implies a &lsquo;realtime&rsquo; ability to globally read a value written anywhere else immediately after the write finishes, so there can&rsquo;t be simultaneous opexes and any reads will return the latest write opex&rsquo;s value.</li>
<li>For sequential consistency, the process order must be respected, but the action doesn&rsquo;t necessarily have to take effect instantaneously between its invocation and its response.</li>
<li>For causal consistency, we also require process order to be respected, but each process can have its own order of the other read/write opexes as long as it respects causal order between order events. This allows opexes to be executed locally without requiring communication or agreement with other processes.</li>
</ul>
<p>It&rsquo;d be difficult to specify these consistency models sequentially, so they specify the system modularly as an execution. An execution is correct if there&rsquo;s an opex ordering that satisfies the consistency property required. The advantages of this approach are that they don&rsquo;t assume some omniscient power knowing the total arbitration order, that the object&rsquo;s state doesn&rsquo;t need to be tracked, that the object is described only via its interface, and that the object&rsquo;s specification can then be separated from the consistency definitions it needs.</p>
<h2 id="gowtham-kaki--novel-consensus-proof-techniques">Gowtham Kaki — Novel Consensus Proof Techniques</h2>
<p>Professor Kaki&rsquo;s talk centred around novel ideas for proving distributed systems, particularly for modelling consensus via convergence and monotonicity. The current standard approach to modelling distributed systems is via asynchronous message passing, but it&rsquo;s hard to, say, specify a safety invariant for a leader election with message passing. A first attempt might look like checking &lsquo;if A has elected B as its leader and C has elected D as its leader, then B = D&rsquo;, but induction doesn&rsquo;t work for this. This invariant also allows some invalid state transitions.</p>
<p>The key takeaway I had from his talk was his point that strengthening invariants is a valid proof strategy that can unlock the final proof. Counterintuitively, you now need to prove more properties, but you can also assume more starting points in the inductive step. The final inductive invariant in the leader election might include all of the following properties:</p>
<ul>
<li>if a leader has been assigned to a node, a quorum of votes should exist for that leader</li>
<li>if a leader message exists, a quorum of votes should exist for that node</li>
<li>a node can only vote for one node</li>
<li>if a node has voted for some node and a vote message exists, the message and the leader should be the same</li>
<li>if two voting messages came from the same node, they should be the same</li>
<li>if a voting message exists, the node should have voted</li>
</ul>
<p>We can also avoid using an induction approach for the proof and instead rely on the convergence of leadership — the fact that it doesn&rsquo;t matter in what order the voting messages are processed so long as they vote for the same leader — and monotonicity of leadership — you can either vote for no one or the same leader, which you can model via a lattice. This monotonicity and convergence gives you consensus.</p>
<p>You can then model consensus with some way of replicating state, and place these two invariants on it. They found that while this replicated state approach required more messages than the async message passing approach, it instead performed better in terms of throughput. They&rsquo;re still working on this, as they&rsquo;ve mentioned they haven&rsquo;t implemented garbage collection or crash recovery in the evaluation system, but I think the gist is that this is a viable alternative to the typical message passing implementations for consensus.</p>
<p>I had a bit of trouble following these two specification/verification talks, mostly because of the formality of the details introduced, but I&rsquo;ll come back to revisit the material should they come in handy.</p>
<h2 id="carlos-baquero--state-based-crdt-performance">Carlos Baquero — State-based CRDT Performance</h2>
<p>One of the other lectures (I don&rsquo;t recall who) cited the <a href="https://inria.hal.science/inria-00609399v1/document">2011 CRDT paper</a> that Marc Shapiro wrote. I&rsquo;ve read the paper for my past research internship around formal verification, and I know the paper&rsquo;s a foundational one. I didn&rsquo;t realize, though, that Professor Baquero was also a co-author on the paper — I only put two and two together when the speaker was listing out the authors and gestured towards him. A very small world.</p>
<p>Professor Baquero&rsquo;s talk was on CRDTs and an optimization that can be applied to reduce the size of state you need to send back and forth. The general idea is that state-based CRDTs can be inefficient. For example, take an <a href="https://crdt.tech/glossary#:~:text=Add%2Dwins%20set%20(AWSet)%3A">add-wins set</a>: it keeps track of a set of tombstones for removed elements to guard against cases where an element&rsquo;s &lsquo;remove&rsquo; operation is processed before its &lsquo;add&rsquo;. As well, removed elements are typically also kept in the main set, duplicating the storage needed.</p>
<p>This talk had some background about <a href="https://en.wikipedia.org/wiki/Semilattice">join semi-lattices</a> and partially-ordered logs. A join semi-lattice is a type of structure that defines a join operation that you can apply to two states to get the joint state: the join is a bit like a set union with extra spice. Intuitively, the &rsquo;lattice&rsquo; part of the name connects to the fact that when you draw up all the possible joins and states, you end up with a lattice-like shape, with an empty state at the bottom and a final state at the top. This final state is the result of joining all the possible inputs together — there&rsquo;s some maths calling it the &rsquo;least upper bound&rsquo;, but you can think of it like &rsquo;the state that contains all of these other states joined together&rsquo;. Very frequently, this looks like a union if you squint. These states can be defined by applying this join operation based on the operations in a partially-ordered log, or a polog, for short. If you model state for a <a href="https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type#G-Counter_(Grow-only_Counter)">grow-only counter</a> in the polog like <code>{A(1), B(2)}</code>, for example, then you could define your join operation to take two states and do a member-wise max. If we had <code>{A(1), B(2)}</code> and <code>{A(3), B(1)}</code> to join, we would then get <code>{A(3), B(2)}</code>, then sum across all members to get the final grow-only counter value of of 5. Each replica keeps a local view of this polog to derive its state.</p>
<p>This state ends up getting pretty big, however, when you consider <a href="https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type#2P-Set_(Two-Phase_Set)">tombstone sets</a> and more complicated CRDTs. Instead of sending over the whole state, the approach presented therefore relies on deltas and mutations. For example, if you get an update from replica A that it&rsquo;s been incremented twice, just send that as a delta mutation instead of sending over all the extra state for replicas B, C, and D. You can fragment your state into so-called <em>irredundant join decompositions of state</em>, turning something like <code>{A(1), B(2)}</code> into <code>{A(1)}, {B(2)}</code> and send the deltas based on these smaller bits instead.</p>
<p>One of his students&rsquo; PhD thesis builds this out more, explaining how you can decompose a state, hash the members to assign their values to buckets, then calculate what buckets to send over to each replica to limit sending duplicated state. You can also apply bloom filters to efficiently test what needs to be sent over. However, there are some issues with false positives in Bloom filters, so both bucket and bloom filters need to be combined for a robust system. There&rsquo;s a whole four-round system of sending bloom filters, their differences, and buckets across that you can read more about <a href="https://vitorenes.org/publication/enes-efficient-synchronization/enes-efficient-synchronization.pdf">here</a>.</p>
<p>Aside from the content of the talk, there were two sort of offhand points about communication times that&rsquo;ve instead really stuck. One was that the whole world can be connected at a latency supporting FPS games and other real-time applications just because the Earth&rsquo;s diameter is small. It&rsquo;s neat to think about: if the Earth was larger, there could&rsquo;ve been whole classes of apps that would&rsquo;ve never been invented. The other is that communication round-trips to space colonies (e.g. Mars) will dramatically increase from what we&rsquo;re used to on Earth, and could be up to a 20 minute RTT. It was very thought-provoking to consider space tech as a field where CRDTs will likely become necessary — when you have such long response times and occassional periods where communication isn&rsquo;t possible, you can&rsquo;t really lean too heavily on the classic client-server model. I want to learn more about the state of space tech in the future, especially since a fellow Rise Global Winner has just gotten into YC with their <a href="https://bifrostorbital.com/">satellite startup</a>. I think there&rsquo;s a lot of interesting potential with critical systems that&rsquo;ll need certain consistency and consensus guarantees.</p>
<h2 id="mira-mezini--algebraic-rdts">Mira Mezini — Algebraic RDTs</h2>
<p>Professor Mezini talked about replicated data types, particularly the concept of an ARDT, or an Algebraic Replicated Data Type. Her talk presented how ARDTs can be used for decentralized state management, can be used reactively for propagating changes, and can be used for coordination with various consistency and availability guarantees.</p>
<p>An interesting point she brought up was that Brewer, the inventor of the <a href="https://en.wikipedia.org/wiki/CAP_theorem">CAP theorem</a>, now states that the CAP theorem&rsquo;s application should be relaxed in modern systems to fit with the specific system&rsquo;s requirements. Sometimes, we can relax consistency a little bit to allow for more availability, and vice versa. It doesn&rsquo;t have to be an all-or-nothing choice given today&rsquo;s technologies. I liked this take, particularly because I could see it in the previous internship work I&rsquo;ve done — we used <a href="https://en.wikipedia.org/wiki/Isolation_(database_systems)#Serializable">serializable</a> transactions just for this part for high consistency, but for better performance left everything else in <a href="https://en.wikipedia.org/wiki/Isolation_(database_systems)#Read_uncommitted">read-uncommited transactions</a>.</p>
<p>ARDTs were presented as a standard library of composable, primitive data-type RDTs that you could really customize but still have working together. They differ from CRDTs because off-the-shelf CRDTs have fixed design decisions baked into them, like assumptions about various network models, and causes a bit of an impedance mismatch when trying to actually program with data. Using ARDTs allows you to decouple the state of data from its dissemination, so that the communication and network becomes irrelevant to the actual application logic. They felt a bit like Jacky&rsquo;s <a href="https://jzhao.xyz/posts/bft-json-crdt">BFT JSON CRDT library</a>, and to be honest I can&rsquo;t quite articulate the difference between ARDTs and &lsquo;customizable CRDTs&rsquo; better than this.</p>
<p>She presented a subset of ARDTs called reactive ARDTs that would avoid callback hell as well as having to track data dependencies across systems. Right now, it&rsquo;s difficult to model and manage updates in data being propagated correctly to other places given a system with diverse nodes — I think the final chapter of DDIA covers this a little as well. They designed a language, <a href="https://dl.acm.org/doi/10.1145/3191697.3214337">REScala</a>, that operates with these reactive ARDTs and their dependency effect chains as first-class citizens in the runtime. This way, it&rsquo;s easier to understand, since events are modelled as happening instantaneously, but also lets the runtime handle making the effect chains strictly serializable and more consistent.</p>
<p>There was also some work on coordination ARDTs mentioned along this vein, to enforce application level invariants that are difficult to otherwise manage via consensus. They achieved this via adding a verifying compiler, <a href="https://dl.acm.org/doi/10.1145/3633769">LoRe</a>, to REScala. Their coordination works by making use of locking without requiring offline nodes to agree. The system models interactions between systems as having certain pre/post-conditions and actions, so the compiler can check that the overall invariants hold at each step.</p>
<p>Fun note about her slides is that I noticed one of the images she used on a slide about decentralized collaborative applications looked really familiar. Halfway through, it hit me that it was actually the title image from the Ink and Switch <a href="https://www.inkandswitch.com/crosscut/">Crosscut</a> article, which ironically is explicitly a &lsquo;personal thinking space&rsquo; and not a collaborative tool. Say you will about me being able to recognize the Ink and Switch blog post images on sight.</p>
<h2 id="annette-bieniusa--formalizing-broadcast-tla-and-erla">Annette Bieniusa — Formalizing Broadcast, TLA+, and Erla+</h2>
<p>Professor Bieniusa&rsquo;s talk was listed on the program as something to do with reactive datatypes and Elixir, which I was looking forward to finally learning a bit more about, but she actually spoke on specifying different broadcast models in TLA+! TLA+ is a specification language used in formal verification, and it&rsquo;s been used in the industry to verify and model many critical systems, notably including AWS&rsquo;s S3. I&rsquo;ve worked a little with TLA+ in my previous research internship on an extension to the <a href="https://distcompiler.github.io/">PGo</a> project, a compiler that translated Modular PlusCal (which itself compiles to TLA+) directly into production-ready Go systems.</p>
<p>So imagine my surprise when the project Professor Bieniusa talks about is about a project, Erla+, that&rsquo;s almost exactly that, just with Go replaced with Erlang! However, their work compiles from a subset of PlusCal directly into TLA+ and Erlang, whereas PGo requires the use of our custom Modular PlusCal extension language first, so they&rsquo;ve cut out the need to learn new syntax. Also, their compiler produces actor-based systems, which I don&rsquo;t know much about but seem to map quite naturally to having multiple distributed nodes that one needs to coordinate. It was neat to see how much the two projects naturally mirrored each other.</p>
<p>The bulk of her talk was primarily about broadcast, though. We talked through several variations, including best-effort broadcast, reliable broadcast, and uniform reliable broadcast. Best-effort broadcast is just a broadcast where you try your best to deliver messages with no retries or other guarantees. For a reliable broadcast, you force each node to forward its messages, and for a uniform reliable broadcast, you need to ensure all messages are also <em>delivered</em>, or received. Unfortunately, uniform reliable broadcast can&rsquo;t exist if the majority of nodes fail, for obvious reasons. We also defined a few properties that we used in these definitions:</p>
<ul>
<li>FIFO property → if you broadcast <code>m</code> from <code>p</code> then <code>m'</code>, then <code>m</code> is delivered before <code>m'</code></li>
<li>causal property → if you broadcast <code>m</code> from <code>p</code> then <code>m'</code> from <code>q</code>, then <code>m</code> is delivered before <code>m'</code></li>
<li>total order property → if <code>m</code> is broadcast from <code>p</code> before <code>m'</code>, then <code>m</code> is broadcast from <code>q</code> before <code>m'</code></li>
</ul>
<p>We then spent some time learning TLA+ syntax and primitives to formally model these properties. I learned that TLA+ uses something called linear-time logic, which gives you a set of executions that are considered correct. You can then define linear-time properties, like safety and liveness properties. A safety property requires that if any execution is incorrect, then there was a prefix of that execution where the remainder of the execution did not fulfill the property — intuitively, it requires that if something went wrong, there was a particular &rsquo;turning point&rsquo; where things went south. Safety can only be satisfied given infinite time, but can be violated in finite time.</p>
<p>On the other hand, a liveness property requires that for any prefix of an execution, there is a set of following executions for which the property is also satisfied — intuitively, that the execution &lsquo;keeps running&rsquo;. Liveness can conversely only be violated in infinite time and is satisfied in finite time.</p>
<p>To model these in TLA+, you need a couple operators: <code>[]F</code> denotes that <code>F</code> is always true, and <code>&lt;&gt;F</code> that <code>F</code> is eventually true. You can also combine these, so <code>[]&lt;&gt;F</code> states that at all times, <code>F</code> is either true or will be true, so intuitively this expresses that some progress will be made towards getting to <code>F</code> eventually. The reverse, <code>&lt;&gt;[]F</code>, expresses that eventually, <code>F</code> will always hold. Intuitively, this denotes stability.</p>
<p>The final concept I&rsquo;ll cover is how we apply these to express &lsquo;fairness&rsquo;. It&rsquo;s a property that states that if something happens &lsquo;often enough&rsquo;, it should eventually happen. There&rsquo;s variations: weak fairness can be expressed as <code>&lt;&gt;[]F → []&lt;&gt;F</code> and says that a step towards <code>F</code> must eventually occur. The implication reads that if <code>F</code> is eventually continually true, then it must eventually occur. Strong fairness, on the other hand, can be expressed as <code>[]&lt;&gt;F → []&lt;&gt;F</code>, which says that a step must eventually occur even if something is not eventually continually true. For more intuition about the difference between weak and strong fairness, think of a traffic light. If the traffic light is strongly fair, the car will eventually have to go, because it&rsquo;ll eventually be green before switching back to red. However, if the traffic light is weakly fair, then the car might never go, because the traffic light will eventually switch back to red and never has a point where it will continue to always be green. This was really mind-bending to wrap my head around, and I think the concepts of eventual-ness and the timing logic here is fun to dig into.</p>
<p>Another coincidence: I also learned that Professor Bieniusa will be collaborating with my supervisor for my research internship next term, so we might get to connect again soon!</p>
<h2 id="german-efficiency">German Efficiency</h2>
<p>In addition to the cold, hard, technical details, I also learned about the finer details of European education systems (the Belgian and German ones, in particular) and about Darmstadt and Germany as a whole. I speak no German, so I had to rely on the locals speaking English. I was a little self-conscious about being the classic clueless North American tourist who romps about Europe and is generally a nuisance. Granted, I don&rsquo;t think I bothered anyone, but I really felt like I was very uncultured and not well-informed before learning about any of this. I&rsquo;m starting to get why people recommend exchange programs and travelling so much — you learn so much by osmosis and vibes, even from a quick stay where you&rsquo;re not interacting much with the locals.</p>
<p>Some quick-fire notes:</p>
<ul>
<li>The Frankfurt airport seemed very empty, even though I was arriving on a weekend afternoon, when I&rsquo;d have expected it to be bustling. Maybe I was in a quieter terminal.</li>
<li>On the other hand, the smoking lounges seemed very full. I was mildly shocked to see a smoking lounge right out the gate, especially indoors. It was also odd to see people smoking right outside doors, young people smoking, and other indoor smoking areas. In Vancouver, it tends to be fairly rare and is almost always an older person huddled in an alleyway, not a well-dressed twenty-something strutting by with friends.</li>
<li>Darmstadt is literally translated as &ldquo;colon/intestines-city&rdquo;. Something about how if Germany was anthropomorphized into a human, Darmstadt would be smack where the bowels were. Apologies if you were enjoying a nice meal at this point in reading.</li>
<li>Trains seem to be consistently late. We took a train a bit closer to Frankfurt to hike, and our train there was almost ten minutes late, and our train back was closer to fifteen late. I was told that German efficiency only applies to cars.</li>
<li>Germans are pretty intense about their hiking. We went up a &lsquo;small hike&rsquo; to <a href="https://www.schloesser-hessen.de/en/schloss-auerbach">Auerbach Castle</a>, which was the better part of an hour up a fairly steep hill. I&rsquo;d assumed since they didn&rsquo;t ask about accessibility restrictions that the &lsquo;hike&rsquo; meant a flat walk, but no, this was really a hike. At some point we saw people <em>biking</em> down the very steep, narrow path, and at the top I was told that most Germans would not consider our trek anything near a hike.</li>
<li>Everything initially seems more expensive than Canada — for example, ramen might run you 14 euros. I was told that this was relatively cheaper than other parts of Europe, but the converted equivalent of ~$21 CAD seemed a little steep. Something closer to $12-15 CAD is what I&rsquo;m used to. However, when you factor in the lack of tip and the already-included tax, it&rsquo;s not far off from Vancouver prices. It was nice that most prices were round numbers too, which helped with sorting out change.</li>
<li>It rains a lot, and people are used to it. One of the highlights of the program was a walking tour around the city on our first day, during which it started thunderstorming and pouring. The lightning and thunder were just a few seconds apart, and we were huddling, trying to recall the conversion for time between lightning and thunder to how close the storm was to figure out screwed we might be. Our guide, completely unfazed, led us around into the main city centre castle and continued peppering us with facts.</li>
<li>Darmstadt has their own 9/11 story, albeit in 1944. The city was <a href="https://en.wikipedia.org/wiki/Bombing_of_Darmstadt_in_World_War_II">heavily bombed by the RAF</a> during WWII, destroying half the town&rsquo;s homes overnight. We were actually in town for the 80th anniversary memorial event, and all through Wednesday we heard the bells tolling across town.</li>
<li>The cafeteria food at TU Darmstadt is quite good. It felt a bit like Ikea standing in the cafeteria line and grabbing lunch, but there was solid variety. I will warn folks that when they translate something as dumplings, though, they mean American/Western-style dumplings — I was not expecting a dense dough ball.
<ul>
<li>The group I was with was very interested in having Asian food for our free meals, seeing as we had a German-cuisine dinner already scheduled. The <a href="https://www.moschmosch.com/">two</a> <a href="https://g.co/kgs/qaPbk23">places</a> we tried were great. It was very fun teaching them to use chopsticks and see them tank Szechuan peppercorns for the first time.</li>
<li>I feel obligated to also especially shout out <a href="https://g.co/kgs/3Fgcghn">this hole-in-the-wall Tibetan dumpling</a> takeaway place, which I tried on my last night. The staff offered me a free sample of mango lassi and were very sweet in explaining everything in English.</li>
</ul>
</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p>Besides all the distributed systems and multicultural learning, I also met many lovely people (who are probably reading this post — I can still see my blog analytics getting a suspicious number of views from Belgium). It was great making some new friends, since I think I was one of the only people who wasn&rsquo;t with a contingent from their home university and perhaps the only North American. I did my share of cultural exchange too: other than being the de-facto Asian-culture expert, I also taught folks some Canadian and American slang<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> and explained how university works here.</p>
<p>I&rsquo;m very happy that I was able to attend and feel very fulfilled in the new fields I&rsquo;ve gotten a tour of during DARE. I&rsquo;m proud to say that this was the longest continuous period in my life where I felt like I fully grasped the difference between strict, sequential, and causal consistency, and I can still mostly reason about the details now. Being exposed to different areas of research that I had no real background in was a challenge — once the LaTeX started flowing in a talk, I admit things generally started going over my head — but was also great to be able to build an idea of the different subareas within distributed systems. DARE was a shortcut in getting to the frontier of research in a short week, and I&rsquo;m excited to be able to continue thinking about some of these problems in my current and upcoming research internships.</p>
<p>The other students who took this program for ECTS credits were required to do another two-week research project following DARE, building on one of these lectures. They&rsquo;ll be doing a presentation soon in a few weeks, and I&rsquo;m really looking forward to seeing what they&rsquo;ve come up with. I didn&rsquo;t have to do one since the transfer credits aren&rsquo;t going to meaningfully affect my courseload next year, but in a way my current internship work is one big extension of Professor Kleppmann&rsquo;s talk and work, so I&rsquo;ll say it counts.</p>
<p>I&rsquo;d very much recommend the program to anyone even tangentially interested in distributed systems. I believe the talks will be different year-to-year, and I&rsquo;ve heard the next iteration is planned in Porto, Portugal. There&rsquo;s funding available for European students via the Erasmus program and no fee for the program itself otherwise. Stay on the lookout for DARE 2025 — I&rsquo;d strongly encourage other students to go for it!</p>


<style>
ul {
margin-bottom: 0.5em;
}
</style>


<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>I will never forget when we were standing in a circle with someone else from Portugal explaining Skibidi Toilet to the others and chatting about brainrot, when Professor Baquero joined the group and said something along the lines of, &ldquo;ah yes, &lsquo;brainrot&rsquo;, that must be what my daughter has&rdquo; and taking a look at a Skibidi Toilet episode. Oh, how I love the Internet.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>Magic! at the Mall</title>
      <link>https://kewbi.sh/blog/posts/240825/</link>
      <pubDate>25 Aug 2024</pubDate>
      <author>Emilie Ma (Kewbish)</author>
      <description>On new phones and new paradigms.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>I got my first phone sometime around 2018<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. I got my second last month.</p>
<p>My first phone was a silver iPhone 7. I remember my friends with iPhones had iPhone SEs or 6s back then, and having a 7 was a subtle step up. I was the one who’d take photos because I had the “best” camera. I rarely used it when I got it — our school had a phone-free policy, so I’d have to dump my phone into this wicker basket of phones at the start of the day. I’d rescue it at the end of the day, then not really do anything with it after I got home. I didn’t even get a phone plan until I started high school, and til then I didn’t use my phone for much besides Google Hangouts and the occasional FaceTime.</p>
<p>There was one classmate who had an iPhone 8, and I recall thinking the X was so excessive. What were the new features, besides a slightly bigger screen and no home button? Big deal. Fast forward a few years, and I’d think people who got the then brand-new iPhone 13 were on the bleeding edge, perhaps a little extravagantly so.</p>
<p>But it’s 2024, and the dynamic island and USB-C charger and three cameras of the iPhone 15 are all the rage. I’d still stubbornly stuck to my iPhone 7. All of a sudden, I was the only friend with a home button. I’d stopped getting updates a few years ago, but I didn’t really miss any of the new features. My 7 served me well — besides, well, Uber Eats not working with anything under iOS 16 and FaceTime and Discord calls starting to stutter out. I still didn’t really feel a need to upgrade, and having an old phone felt almost like a point of pride for me at this point: I’d taken such good care of my phone that here it was, six years later, with no scratches, a two day battery life, and working like a charm.</p>
<p>Unfortunately, the day came last month: I’m headed to Cambridge (UK) for the fall, and I’ll need both my Canadian SIM, to receive SMS verification codes and such, and a local SIM for calling. My iPhone 7 doesn’t support an eSIM, so I won’t be able to dual-SIM. With a heavy heart, I made my way to my local Apple Store.</p>
<p>There, I got an iPhone 14 (doubling my model number!). While I was there, a lot of casual magic happened. I’d never bought a phone in person before, or really spent significant time in an Apple Store, so I was pleasantly surprised by some of the little touches I noticed. This was also my first time switching phones, so I got to experience the sheer wizardry that is Quick Start. And while I was booking my pickup slot, I also noticed Apple Vision Pro demos available, and I impulsively booked a demo. The AVP isn’t something that I’ve really thought about, besides seeing a few tweets and video thumbnails, or considered for actual use, but I found the demo fairly enchanting.</p>
<p>I think there are a few aspects that <em>make magic</em>.</p>
<ul>
<li>Magic inverts expectations while building on them. It makes the hard things unexpectedly easy and the impossible things possible, but it has to do so in an incredibly intuitive way. There shouldn’t be a &lsquo;why does this work?&rsquo; — there can only be an ‘of course’.</li>
<li>Magic is embedded and composable, not standalone and sandboxed. It’s adaptable to whatever you need in the moment, and comprehensive in covering everything you might think about.</li>
<li>Magic is predictive but forgiving. It figures out the word on the tip of your tongue and the recurring patterns that make up your day. When you get something wrong, it gently nudges you to ask if that’s what you really want.</li>
<li>Magic can disappear. With more magic comes more responsibility. The more magical an experience, the more tiny flaws can quickly break the suspension of disbelief.</li>
</ul>
<p>The Quick Start and AVP demo experiences both captured these traits — I think that afternoon in the Apple store was the highest density of casual magic I’ve experienced this summer. This is a post about those moments of magic at the mall and what makes a computing experience compelling. I’ve always been jokingly disdainful about Apple fans, but in those few hours in the store, I started to see what they’re so enthusiastic about.</p>
<h2 id="dont-quick-start-now">Don&rsquo;t (Quick) Start Now</h2>
<p>Picking up my phone was a very straightforward 5-minute errand. Unfortunately, when I got home and was admiring my edge-to-edge screen, I noticed the phone had a scratch. I wasn&rsquo;t about to spend that much money and take a defective phone, so I had to go back and exchange it<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. I didn&rsquo;t want to risk taking another trip again if there was another visual defect or some software problem, so I decided to do my setup in-store (and also because the only AVP demos were at the end of the day, so I had some time.)</p>
<p>I don&rsquo;t think I had to worry much about the migration though, since Quick Start was seamless. <a href="https://support.apple.com/en-ca/102659">Quick Start</a> is a way to wirelessly transfer all your apps, app data, messages, and preferences over to a new device. You can do Quick Start via an iCloud backup, or you can run one directly from the device. One really nice touch was that I didn&rsquo;t even have to log into or search for the Apple store WiFi, and even over the public WiFi, which I didn&rsquo;t expect to be very fast, the setup was done in less than five minutes. I don&rsquo;t have a lot of photos or data backed up on my phone, but nonetheless I was impressed. This speed underscores something about magic: it works fast. Magic doesn&rsquo;t need endless loading bars and doesn&rsquo;t get stuck downloading something.</p>
<p>There are plenty of small touches that transferred over: my texts and contacts were just as I&rsquo;d left them, I was already logged into most apps, my wallpaper and lock screen was identical, the years of settings I&rsquo;d carefully curated were in place, my pirated textbook PDFs were set up perfectly with Apple Books. My muscle memory for everything still worked, without any of the tedious setup and comparing things between either phone. It was like having an exact, scaled-up replica of my old phone.</p>
<p>I don&rsquo;t think I&rsquo;ve ever been so pleasantly surprised with a migration process. When I bought my phone, I was dreading the hours I&rsquo;d expected of downloading everything and setting up logins and preferences again. Taking all that away with such attention to detail was a very good investment on Apple’s part. I used to be distrustful of cloud syncs and signing into browser/device services<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>, but they’re so helpful for applications where they’re (somewhat) strictly necessary. I think background processes like syncs and preprocessing do a lot of the heavy lifting for magic. I have no insight into how Apple’s photos systems works, for example, but I’d bet they’re creating their recap albums with an off-device queueing service and they’re running their people detection online when a new photo’s uploaded to iCloud Photos. This ties into my previous point about magic working to match your speed — because the processing happens before you’re looking to access something, it seems more magical when you go to look something up and it’s already there.</p>
<p>This background processing is also a lot easier when your endpoints are all centralized, because this provides consistency in how data is stored/read. This is how you can make your service feel omniscient and predictive, since you have all the data to figure out all the patterns. You can certainly engineer a way to connect more disparate data sources: I keep coming back to reference <a href="https://www.inkandswitch.com/cambria/">Cambria lenses</a>, and I was introduced to the <a href="https://en.wikipedia.org/wiki/Resource_Description_Framework">Resource Description Framework</a> model in one of the first few chapters of <em>Designing Data-Intensive Applications</em>. I just finished the book, and one of the primary focuses of the last chapter is on data integration via derived state, batch processing, and federated/unbundled databases, which I expect would be the key components for bringing something like this to generic devices. To this end, Samsung’s Smart Switch seems to be able to bring iOS data to Android devices as well as from other Android devices to a Samsung, and I’d be interested in learning more about how it all works.</p>
<p>I think Quick Start’s adaptability and coverage were key to making it feel like magic. I wouldn’t have expected Apple Books, for example, to get ported over so seamlessly, though now that I think about it, it probably does back up to iCloud. Even apps outside of the Apple ecosystem, like Discord and FitBit had me already authenticated and all my app-specific settings were translated over. Magic works over everything – it’s not meant to let on that it’s forgotten something and can’t be even slightly inconsistent with the ecosystem’s ‘magic system’, and Quick Start does so expertly. Maybe this is all because I haven’t done iOS development before and this is all thanks to some slick data access API requirements, but I’m so awed by how flexible the setup feels.</p>
<p>I think two things that could be improved, perhaps, are the Apple Wallet transfers for credit cards (although I guess there&rsquo;s good regulatory and privacy reasons for this) as well as SIM-card / phone-number based apps like Whatsapp or Signal. But even off the top of my head, I can think of some technical limitations with each, so there&rsquo;s probably a reason they&rsquo;ve not been implemented.</p>
<h2 id="interlude-i-miss-my-home-button">Interlude: I Miss My Home Button</h2>
<p>With my new phone itself, there are a few key things I’ve noticed. The first is fairly obvious: the camera quality is certainly a step up (and in tandem, the quality of the screen to view the photos I take has definitely improved). I was recently at a work dinner with my fellow interns, and I took out my phone to take a quick photo for my parents. The second I opened the camera app, I blurted out that the camera quality was so much better than my old phone. Doubling my model number also seemed to double the warmth, depth of colour, and sharpness of even casual pics. When I was on a trip to the UK earlier this month, I took a few nighttime photos of the very classic architecture – lots of fine details and masonry. Despite the dim lighting, the photos were still able to capture things quite well with minimal grain. I’m very impressed with this camera, and I can’t imagine the further upgrades that the newest models are supposed to deliver.</p>
<p>Another thing I’ve appreciated is finally having good NFC support! I wrote another blog post about <a href="https://kewbi.sh/blog/posts/240811/">building a webring that interacts with a physical NFC ring</a>, something that wasn’t possible to test with my old phone. I’ve been a little obsessed with NFC tags and having little physical checkpoints that interact with my digital world (for example, <a href="https://x.com/spencerc99/status/1818721711858368890">this do-not-disturb phone pillow</a>, or its more consumer counterpart <a href="https://getbrick.app/">Brick</a>). I&rsquo;ve played around a little with NFC and the Shortcuts app, and I’m also happy with how much Shortcuts has levelled up since iOS 15.</p>
<p>Finally, I’ve realized how nice it is that apps and features are able to pick up on patterns of usage. One example is the wallet app – I was travelling abroad and was using a credit card that I don’t normally use. Within a few days, Apple Wallet knew to bring that card up as my default when I double-pressed the power button. This is a nice tidbit of magic — Apple Wallet was smart enough to pick up on my intents without explicit configuration, but it’s easy to override and pick a different card if I needed to.</p>
<p>There’s much more, like being able to customise my home screen more with different icons and widgets and fonts, but I’d like to move on to another major magic experience.</p>
<h2 id="14-pounds">1.4 Pounds</h2>
<p>I impulsively booked an AVP demo since I’d be there at the store anyways, having not really seriously thought about the device or read up about its features beyond the ‘first look’ demo that was all over my feed. When I got there, the first thing the Specialist said was that the AVP was not a VR headset: it was a spatial computer. I still don’t really buy the rebranding — it feels a tad pedantic — but I will say it’s unlike anything I’d ever tried before.</p>
<p>My headset experience is limited to a ten-minute stint playing Fruit Ninja on an Oculus Quest during a summer camp, so maybe that&rsquo;s why I was so intrigued. First off, I was not expecting to have my face scanned and a custom-fit headset delivered to the demo station. I think I have a fairly normal set of face shape/head size/vision requirements, so maybe it was all a bit of theatre to make the demo feel more personalised.</p>
<p>Tailoring the demo is the major thing I felt like was lacking. The experience starts with learning how to browse photos, view live photos immersively, and navigate around apps, culminating in a very well-shot immersive video. This really highlighted the gestures and new interactions that were possible with the AVP and certainly provided the most wow-factor. I wish I&rsquo;d have gotten more walkthroughs through more productivity and everyday work demos, though. The AVP was constantly touted as a portable way to make the world your workspace for anything, but we never got to actually see what doing work was like. Going through a spreadsheets program, editing a video, or doing some debugging might have made the demo more compelling for people looking for a more serious, professional use-case for the AVP. It would have been annoying to set up and pair a MacBook for every demo, but surely there&rsquo;s a way to streamline this (make more magic, y&rsquo;all!) It would have been extra amazing if the demo could pull from iCloud data – there must be something they can copy-paste from Quick Start. I would have loved to learn how to read my EPUBs from Apple Books or how to use Shortcuts with the AVP, especially any cross-device capabilities. If anything, the current demo sells the AVP as a (heavier), more immersive version of the VR headsets already available, focusing on entertainment and casual usage: exactly what Apple was trying to avoid.</p>
<p>Controlling the AVP was a bit like how I imagine Harry Potter et al. felt at Hogwarts — harnessing magic is tricky. The calibration helped serve as a tutorial to pick up the mouse mechanics, and I liked the slight gamification. Once you got used to having to look exactly where you wanted the cursor and do the pinch-clicking, the tutorial went by fast. Pointing with my eyes didn&rsquo;t feel very accurate at first, though, unless I really focused on a point, or if I shifted my focus, then looked back. Doing gestures without a button also felt a little unusual due to the lack of tactile feedback, but by the end of the demo I was well-adjusted. I was still relying on instructions for when to use the crown and for what, but I&rsquo;m sure with more time it&rsquo;d have gotten ingrained into muscle memory.</p>
<p>One of my favourite parts of the demo was that cinematic video I mentioned before — it tied together the best of the audio/video capabilities. There are a bunch of safari and immersive walk-with-the-animals-type clips, and I&rsquo;d swear that the elephants were right by me. There was a scene with a tightrope walker, and I felt my heart drop when they also fell off. There was also an NBA scene where the player throws the ball right at your face — I visibly flinched, and the Specialist said she uses it as a marker of how far people are into the video when they recoil.</p>
<p>I loved the depth of field of the AVP, and I think it really helped with the suspension of disbelief and the resulting magic. Had the videos felt flatter or the layers more compressed, it wouldn&rsquo;t have gotten the feeling just right. Because most of the demo was so flawlessly executed, I think it made it obvious when and highlighted when things didn&rsquo;t go quite so well (e.g. the visual pointing). That&rsquo;s another aspect of magic: if you&rsquo;re going to build an immersive and comprehensive experience, it&rsquo;s crucial you take care of the smallest details too.</p>
<h2 id="conclusion">Conclusion</h2>
<p>A few days ago, before I started writing this post, I was trying to check my phone when it froze. I’d opened the Clock app to set an alarm, but all I saw were a row of icons at the bottom with a grey screen. I locked my phone and swiped up to open it — it faded out the clock font but wouldn’t open the app. It was much too late to go out to the Apple Store, and I was dreading making the trek the next day. The illusion of magic? Gone<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.</p>
<p>But I’ll forgive Apple on this one, since the whole experience of getting the phone itself was enough magic for a good bit. Between Quick Start, old new features, and the Apple Vision Pro, I’ve both been able to be enchanted and think a little more about what made each so charming. All of the encounters captured some of the core aspects of magic: Quick Start flipped the script on tedious phone migrations and did so in an incredibly intuitive, embedded, and comprehensive way. My new phone made me appreciate the power of good predictions, and how those little touches can seem so obvious in hindsight. On the other hand, the Apple Vision Pro underscored the responsibility that comes with this power — its immersion felt magical, which especially underscored the handful of papercuts along the way.</p>
<p>Overall, I think what really makes or breaks magic is how well it fits into an existing worldview and how intuitive it is. I was comparing magic in software to classic fantasy book series like Harry Potter, which have very composable, expansive, and consistent magic systems. Tech is how we’ll make hard things easy and make the impossible possible, but we’ll have to do so carefully to really capture the magic. Radically different offerings, like the AVP, bring a lot of opportunities for moments for casual magic, but I can appreciate the challenges that must have come up in order to make it feel so spellbinding.</p>
<p>I thought I’d have hated getting a new phone, but I both appreciate the phone itself and the fascinating few hours I had at the Apple store because of it. I plan on holding on to this phone for a long time again. I went from an iPhone 7 to an iPhone 14, so the logical next step is to wait for the next multiple of seven. When I drop by to pick up my iPhone 21, I’ll see what demos, features, and crazy, magical moments are possible then.</p>
<hr>
<p>P.S. If you&rsquo;re looking for posters or art, you should check out the <a href="https://www.etsy.com/ca/listing/1762613124/6-minimalist-computer-patent-prints">Toronto Island Patent Press</a> on Etsy! A friend and I made a set of posters based on retro schematics and patents for classic computing companies, like IBM, Nortel, and DeskMaster. They&rsquo;re available as digital downloads in a wide variety of very aesthetic colourways — perfect for the sort of people who geek out over the first monitors and blueprint drawings.</p>
<figure><img src="/img/240825/promo-pic.png"
         alt="Figure 1. Example posters and colourways."/><figcaption>
            <p><em>Figure 1. Example posters and colourways.</em></p>
        </figcaption>
</figure>

<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>This is a lie – my parents had gotten me my own phone for my birthday the year before IIRC, but I told them to return it since I didn’t really need it for anything. I think you can draw many conclusions about my personality from this.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>This was also an ordeal in and of itself — if you ever get a friend to buy you a phone with a friends-and-family discount, try to avoid having to exchange it since you&rsquo;ll have to get them to refund, then re-buy the phone with the discount. A bit of a hassle if your friend isn&rsquo;t local and doesn’t come with you.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>When I got my first computer, I not only refused to use Chrome logged in, but I used Chrome only in an incognito window. No history, no way to keep my tabs between sessions. I’d only put my laptop into Sleep instead of ever shutting it down, and I’d try to put off Chrome updates as long as possible. Whenever I was forced to re-open Chrome, I made a trigger list of sites to have to log back into again, and I’d need to go through and login to each and every one of them. Like footnote 1, I think you can see how this fits into my personal lore.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>If you run into the same issue, press and release volume up, then volume down, then hold the power button til after the Apple logo comes up.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>Webring²</title>
      <link>https://kewbi.sh/blog/posts/240811/</link>
      <pubDate>11 Aug 2024</pubDate>
      <author>Emilie Ma (Kewbish)</author>
      <description>On ring fingers and finger prints.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Earlier in April, I was preparing for <a href="https://kewbi.sh/blog/posts/240602/">my first conference talk</a>. Not quite knowing what to expect, I thought there&rsquo;d be plenty of networking and job opportunities and swag. I was right on only one of these counts (spoiler: it was the merch)<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<p>But when I&rsquo;d just been accepted, I thought I&rsquo;d be hitting up recruiters left and right on the showcase floor. I printed ten paper copies of my resume, and if that wasn&rsquo;t enough, I also ordered a NFC ring to flash my personal website onto. This way, when people wanted to learn more, they&rsquo;d be able to tap their phone against the ring to bring up my site, and I wouldn&rsquo;t have to fumble around typing my LinkedIn URL or pulling crumpled paper resumes out. I thought it&rsquo;d be a memorable way to leave a first impression: look, this candidate not only knows about web and distributed systems, but is making their resume available to me in a way I&rsquo;ve never seen before! I chose a slick double-sided titanium ring off <a href="https://store.nfcring.com/products/signature?variant=602072773">nfcring.com</a> and giddily wrote my site URL onto both chips.</p>
<p>Suffice it to say that no one scanned my ring during the conference. However, I still had the ring, and I wasn&rsquo;t about to wear it every day in the deranged hope I&rsquo;d be able to organically sneak it into conversation one day. I wanted to make a little project out of the ring, and after some very Emilie-core wordplay brainstorming, I happened upon the idea of a webring ring. You&rsquo;d be able to scan it, just like when I flashed my site URL onto it, but it&rsquo;d lead to a <a href="https://en.wikipedia.org/wiki/Webring">webring</a> of me and my friends&rsquo; blogs, and you&rsquo;d be able to navigate between sites on the ring by scanning it multiple times.</p>
<p>It didn&rsquo;t take long at all to build — a weekend and a Cloudflare Pages deep dive later, my webring ring was ready. I&rsquo;m calling it Webring², because it&rsquo;s a <code>web(ring)(ring) = web(ring)²</code>. Here&rsquo;s a demo video:</p>


<div style="display: flex; justify-content: center; margin-bottom: 1em">
	<video src="https://github.com/user-attachments/assets/0d54fa1d-c8c1-447f-80c4-9fa6e20035cd" autoplay loop>
</div>


<p>The source is available <a href="https://github.com/kewbish/webringsquared">here</a>. In this blog post, I&rsquo;ll go through the three main components of the project: the webring site itself, the mechanism to open different pages each time you scanned it, and the physical NFC ring. Along the way, I&rsquo;ll cover the history of early web search engines, some niche behaviour of WebRTC, and odd body modifications.</p>
<h2 id="if-you-searched-it-then-you-shouldve-put-a-ring-on-it">If you searched it, then you should&rsquo;ve put a ring on it</h2>
<p><a href="https://en.wikipedia.org/wiki/Webring">Webrings</a> are a collection of sites that link to each other in a cycle. Each site has &rsquo;next&rsquo; and &lsquo;previous&rsquo; links, which are wired together so by the time you&rsquo;ve hit the last site, its &rsquo;next&rsquo; link takes you back to the first site, and vice versa. The links were usually included in a commonly styled footer that everyone in the ring included on their site, and sometimes there&rsquo;d be a central directory listing to make sure that if someone&rsquo;s website went down, the ring didn&rsquo;t turn into an abbreviated line instead. There was typically a &ldquo;ringmaster&rdquo; who&rsquo;d manage the ring structure, moderating new applications to join or detangling awry linking loops.</p>
<p>Before search engines like Google or DDG, the early web subsisted on web directories. You couldn&rsquo;t just search across the whole web: you needed to have an explicit entrypoint to some site. (Before DNS, people even had to share IP addresses, not domain names!) Web directories bridged this gap by providing a central catalog of sites, neatly categorized for your perusal. <a href="https://en.wikipedia.org/wiki/DMOZ">DMOZ</a> was one such web directory, owned by AOL but managed by volunteers. Yahoo! also started as a <a href="https://en.wikipedia.org/wiki/Yahoo!_Directory">hand-curated directory</a> before pivoting to a more modern crawler approach.</p>
<p>Webrings were sort of like a branching-off point and alternative to directories, like a human-curated &lsquo;Recommended videos&rsquo; at the bottom of each site. Once you&rsquo;d found a page via a directory, it&rsquo;d be very convenient to be able to find other similar sites without going through the directory again. It&rsquo;s natural, then, that webrings were also used to boost SEO rankings, especially for early search engines in the era of PageRank, since they provide a guaranteed few links to/from relevant content. <a href="https://en.wikipedia.org/wiki/Backlink">Backlinks</a> were crucial then, because more backlinks from quality sites would boost your search result rankings. In some ways, webrings were a less-commercialized version of the linkfarms and SEO drivel that&rsquo;s pervasive on the front page of the web today.</p>
<p>Nowadays, it seems that webrings are seen as a cute nod to the web of the past. The major webring providers have shut down, and search engines have replaced much of the discovery functionality of webrings. The primary webring site, WebRing.com, was summarily end-of-lifed by Yahoo! fairly early on, in 2001, but other providers stuck around til through the 2010s. There are still modern webrings running, like <a href="https://webring.xxiivv.com/">this one by Devine Lu Linvega</a>, and every so often, <a href="https://news.ycombinator.com/item?id=38268706">a post</a> <a href="https://news.ycombinator.com/item?id=38177128">comes up</a> on Hacker News lamenting their relative demise. While poking through Hacker News, I also found this site <a href="https://webri.ng/">webri.ng</a> that lets you manage webrings by generating you the footer HTML to insert (bonus for a cool domain!). I&rsquo;m sure there are plenty of webrings still running, particularly personal ones like the one I started, hosted by a group of friends or colleagues. In my head, they&rsquo;re in the same semantic space as the cozy web, as little relics of delight that you might stumble upon from a quirky site.</p>
<p>Speaking of my webring, let&rsquo;s look into its HTML structure. The NFC ring has a URL record written on it, so when you scan it, all it does is open a website. I&rsquo;ll discuss why we have to use a single website in the next section, but for now, let&rsquo;s take a look at the site itself. The ring links to <a href="https://webring.kewbi.sh">webring.kewbi.sh</a>, which is a static Cloudflare pages site. It displays a single <code>&lt;iframe&gt;</code> with the current webring page, and the footer links navigate to the previous and next sites in the ring. The source is <a href="https://github.com/kewbish/webringsquared/blob/master/src/index.html">here</a> — the markup is very simple, simple enough to have been mostly generated by ChatGPT.</p>
<p>One of the more unique bits about this webring is its architecture. Webring² is a centralized display, linking to sites that are separately controlled by each of my friends. Using an <code>&lt;iframe&gt;</code> to dynamically link to their content and keeping a central navigation header retains the core experience of exploring a webring, but guarantees no broken links or downtime. Even if one of my friends&rsquo; pages goes down, their site won&rsquo;t load, but the navigation will still allow you to go to the next and previous sites. This lessens maintenance burden, since I won&rsquo;t have to run around asking the owners of the pages that link to the broken site to change their links. This also makes it easier to join and leave webrings: all the links are managed centrally on my link service, so I can localize changes just to that configuration file instead of requiring multiple people to update their links. Because this site is just a static HTML file, it&rsquo;s easy to rehost on another provider, providing additional future-proofing.</p>
<p>I claim no originality, since I was neither alive nor on the Internet for the webring heyday, but I don&rsquo;t think I&rsquo;ve seen this structure in any of the other (modern) webrings I&rsquo;ve found. I don&rsquo;t see any reason why this architecture couldn&rsquo;t have been supported by the early Web — Netscape added support for <code>&lt;frame&gt;</code>s, a predecessor to <code>&lt;iframe&gt;</code>s, <a href="https://en.wikipedia.org/wiki/Frame_(World_Wide_Web)#History">in 1996</a>, and sites were being hosted by servers already anyways. To be honest, I haven&rsquo;t dug into the history of WebRing.com or any of the other webring providers, so maybe that&rsquo;s indeed how they worked.</p>
<h2 id="web-whorls">Web Whorls</h2>
<p>The static Webring² website has links to navigate between sites in the ring, but I also wanted to build out an interaction where scanning the ring multiple times would automatically advance the user&rsquo;s &lsquo;position&rsquo; in the webring. The obvious way to do this is to somehow change the URL stored on the ring each time it was scanned, but there&rsquo;s no way to make a self-modifying record as far as I know. You could try making some standalone app to force the user to both read/write to the ring, but I wanted my site to work on the web, as was originally intended! This is also why I had to use the one-centralized-site architecture, since otherwise I wouldn&rsquo;t be able to change the view to the selected website.</p>
<p>If you go through <a href="https://github.com/kewbish/webringsquared/blob/master/src/index.html">the HTML</a> and <a href="https://github.com/kewbish/webringsquared/blob/master/functions/progress/%5Bipkey%5D.js">the Pages Function</a>, you&rsquo;ll see that the app is quite simple.</p>
<ul>
<li>In the HTML, we fetch from the <code>api.ipify.org</code> API to get the user&rsquo;s public IP, then pass this as an identifier <code>ipkey</code> to the Pages Function.</li>
<li>The Pages Function maps the IP address to the current position in the webring and returns the current URL alongside the current index and the list of URLs.</li>
<li>The frontend then displays the current URL in the <code>&lt;iframe&gt;</code>.</li>
<li>When the user clicks the &lsquo;previous&rsquo; or &rsquo;next&rsquo; links, the frontend makes a request to the Pages Function to set the current user&rsquo;s index.</li>
<li>This way, the next time the user taps the ring, the index will increment again in the Pages Function, and they&rsquo;ll be sent to the next site.</li>
</ul>
<p>This is well and good if you have a dedicated IP address, but nowadays your public IP will be shared by multiple devices on your network, or even at a higher level, by other households on your ISP. At this point, the site only maps an IP to a position, so if I scanned my ring and clicked &rsquo;next&rsquo; a few times, my parents on the same network would see my state instead of starting from the beginning. This isn&rsquo;t the end of the world — it&rsquo;s the whole reason that webrings are a cycle in the first place. However, I wanted to drill down more, so ideally each device would be able to have its own position in the webring. I came up with three potential approaches and prototyped one of them out. The other two were very interesting reading, but I decided they were a bit too invasive to implement, especially for a page of this scale (read: no real users).</p>
<p>The first idea I had was to find a way to figure out the user&rsquo;s local IP. This is the address that&rsquo;s used intra-network, and would uniquely identify the device among the others sharing the public IP. The WebRTC API seemed to promise a solution and path forward. The WebRTC API is used for real-time communication: video and voice in particular. It enables screensharing, streaming, and sending messages between peers. WebRTC needs to know the local IPs of devices in order to negotiate connection information between them. You can see this in action on <a href="https://net.ipcalf.com">net.ipcalf.com</a>, which will display a <code>.local</code> address, or by running JS similar to this Gist:</p>


<script src="https://gist.github.com/antyakushev/a5d153654e02036d81cb9aec21125bdf.js"></script>


<p>The address we get back is a <code>.local</code> address, not in typical IP octet format. That in itself is fine, since I only need some unique identifier for the device, and I don&rsquo;t care about what that ID looks like. You&rsquo;ll notice that if you refresh your tab, though, the <code>.local</code> address will change. This <code>.local</code> address is a <a href="https://en.wikipedia.org/wiki/Multicast_DNS">mDNS</a> protocol address — mDNS is like DNS but for small networks where you don&rsquo;t need a hierarchy of nameservers and can just address peers directly. It&rsquo;s a bit like <a href="https://en.wikipedia.org/wiki/Address_Resolution_Protocol">ARP</a>:</p>
<ul>
<li>The requesting device will multicast a request for the local IP address linked to the <code>.local</code> address.</li>
<li>The device matching the <code>.local</code> address will multicast back its local IP address as its response.</li>
<li>All devices except the one that matches the address don&rsquo;t respond to the request, but can read the response and cache it for future reference.</li>
</ul>
<p>At this point, you might realize that being able to access local IP addresses from anywhere on the Internet, just via a simple API call, feels a little iffy privacy-wise. That&rsquo;s why the WebRTC API uses mDNS instead. The browser will dynamically generate a <code>.local</code> address for you each time you create a connection and resolve it for you behind the scenes. This way, your local IP is never leaked into the Internet, but unfortunately that means your local mDNS address can never be used for identifying your device for webring purposes either. Notwithstanding the fact that the <code>.local</code> address kept changing on refresh, I think the API also threw errors on mobile Safari, though I can&rsquo;t be sure since I can&rsquo;t see the console logs. This WebRTC connection code is still <a href="https://github.com/kewbish/webringsquared/blob/96a5b2440f4fae69433ea84b2f8a342f0c1e49b3/src/index.html#L178">left in the HTML source</a>, but it&rsquo;s commented out.</p>
<p>At this point, I realized I should probably stop fighting the privacy protections that people smarter than me had come up with, but I couldn&rsquo;t help myself from looking into a couple of other ways to identify users. After trying out the WebRTC API, I realized I could quite trivially generate a UUID and store it into the browser&rsquo;s <code>localStorage</code> or as a cookie, and reference that as an identifier. (I&rsquo;ll get around to adding this one day, but it&rsquo;s not up on the webring site now quite yet.) While looking into cookies, I came across the concept of <a href="https://en.wikipedia.org/wiki/Evercookie">Evercookies</a>, which were self-reconstructing cookies that couldn&rsquo;t be deleted. Instead of just storing data into a cookie, <code>localStorage</code>, or the IndexedDB, Evercookies hide themselves into weird, niche storage mechanisms like <a href="https://en.wikipedia.org/wiki/Evercookie#Description">reading cookies from the RGB values of force-cached images</a>. Usually, the typical &lsquo;clear cookies&rsquo; option on major browsers just wipes the cookies themselves. If any other pieces of the cookies are left, however, like a piece left in a Flash Shared Object, the JS is smart enough to reconstruct and restore all the cookies back where they came from. <a href="https://github.com/samyk/evercookie">Here&rsquo;s the source of the Evercookie API</a> — it&rsquo;s really something, and the list of places where you can sneakily store data is worth a read through. Though it doesn&rsquo;t have anything to do with this project per se, <a href="https://samy.pl/csshack/csshack.js">CSS history knocking</a> stood out to me in particular as an interesting way to exfiltrate visited status. It&rsquo;s a little scary being aware of all the ways that you can get around the basic browser controls to remove your history or web footprint, and that it seems so clean and simple to do.</p>
<p>Perhaps even scarier are the possibilities laid out by <a href="https://en.wikipedia.org/wiki/Device_fingerprint">browser fingerprinting</a>. JS libraries can combine your user agent, screen resolution, timezone information, plugins, fonts, and more to create a mostly unique identifier for you. <a href="https://github.com/fingerprintjs/fingerprintjs">FingerprintJS</a> claims 40-60% accuracy, which is already impressive, but their <a href="https://fingerprint.com/">closed-source version</a> claims 99.5%. <a href="https://codepen.io/vsbeats/pen/RjMQex">See a demo here</a> for the open-source version of FingerprintJS to get a sense of just how much identifying information you&rsquo;re transmitting with every request. Or, check out the closed-source version&rsquo;s site — I opened the site in a regular Chrome window, then in an incognito window, then after restarting Chrome, and all three times it got my identifier right.</p>
<p>I was super surprised that even with the user agent, screen resolution, and other parameters that the more limited open-source version tracks, it&rsquo;s able to pinpoint users so precisely. I&rsquo;d have expected that there&rsquo;d be billions of users on the internet, millions within my country and using my same ISP, and probably in the thousands using the same browser at the same standard laptop resolution. Even taking browsing time into account, I&rsquo;d have expected maybe at least ten other folks matching my browser version and parameters to be on the Web at any given time, but it&rsquo;s been so far quite impressive. Again, sorting out proper cookies or browser fingerprinting felt like too big of a lift for my weekend project, so I didn&rsquo;t really consider adding the library. Certainly fun (and spooky) to play around with, though.</p>
<h2 id="the-right-hand-rule">The Right-Hand Rule</h2>
<p>We&rsquo;ve gone through the webring site itself and how I navigate between pages on the webring — all the software components that make the site fully usable in a browser. This brings us, finally, to the physical ring itself. My ring is the cheapest model off <a href="https://store.nfcring.com/products/signature?variant=602072773">nfcring.com</a><sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. I was worried the company wasn&rsquo;t even in business, since their forum was inactive and their stock fairly low, but a quick email assuaged my concerns. I wasn&rsquo;t too fussy about the <a href="https://cdn.shopify.com/s/files/1/0259/9785/t/17/assets/NFC_Ring_Datasheet_Signature.pdf">datasheet</a>, since I didn&rsquo;t have particular requirements on the NFC chip, as long as it could hold a tiny URL.</p>
<p>NFC tags are pretty easy to read and write — just download a writer/reader app on your phone and you&rsquo;re set. To write Webring²&rsquo;s URL to the ring, I used the <a href="https://apps.apple.com/us/app/nfc-tools/id1252962749">NFC Tools</a> app. For most record types, you don&rsquo;t even have to download a reader if you have a recent smartphone, so you can just wave your phone near a tag to automatically open the contact or whatever data&rsquo;s stored on it. You can see in the demo video above that I can just tap my phone on the ring to scan it without opening any particular app. I was at a cybersecurity competition recently, hosted in an official government building, and several participants had a lot of fun surreptitiously writing their own data to the NFC-enabled visitor lanyards we were required to wear (This is your warning to not do this.) Some folks also brought their <a href="https://flipperzero.one/">Flipper Zeros</a>, which is a standalone device that can, among many other things, scan and emulate NFC cards, so there might have been some shenanigans with cloning hotel room cards. (Again, don&rsquo;t do this.)</p>
<p>To understand how NFC tags work, you&rsquo;ll need to take a throwback to high school physics, and the electromagnetism unit in particular. The high-level overview goes like this:</p>
<ul>
<li>NFC tags themselves, like the one in my ring, doesn&rsquo;t have a power source of its own. These are called passive NFC devices.</li>
<li>Your active NFC device, like your reader, has some power source. This power source can generate an electromagnetic field when you pass the current (electricity) through a coil.</li>
<li>Remember the right-hand rule from physics class? It turns out that if you put a coil into a magnetic field, you also get current back out. If you look closely at the NFC ring, you&rsquo;ll see a little coil of wire, which now can get some current running through it.</li>
<li>A capacitor is like a mini-battery: it charges up and can release all the power in a short burst.</li>
<li>This provides enough power for the NFC tag&rsquo;s microprocessor to create its own magnetic fields, which the phone can then read. It&rsquo;s like the NFC tag is wirelessly tapping power from your phone in order to transmit its own information. The official term for this is &lsquo;magnetic induction&rsquo;. Transferring power this way isn&rsquo;t very efficient, however, so it has to be done at very close intervals.</li>
<li>Data is stored on NFC tags in raw bytes, which can be formed into one of several record types <a href="https://gototags.com/nfc/ndef/record-types#nfc-forum">defined by the NFC forum</a>. These include records like URLs, contact information, WiFi passwords, or application-specific NDEF records.</li>
</ul>
<p>NFC tags typically differ in terms of their capacity and speed, and are categorized into one of <a href="https://nfc-forum.org/build/specifications">five types</a>. Type 1 tags are older and slower, and the tags generally go up in performance as their type number increases. The tag on my ring is the <code>NTAG203</code> type, which <a href="https://ubitap.com/ntag203">falls under Type 2</a>. It has 144 bytes of space, which is limited but more than enough for my single URL. I don&rsquo;t notice any speed issues, but I&rsquo;ll note that it can sometimes be difficult to scan the ring. I think the problems are due to the curved surface of the ring, but in the spirit of PEBKAC, are just as likely to be because I don&rsquo;t quite know where the NFC antenna is on my phone. I&rsquo;ve tried scanning the ring with a thinner plastic case, which works, but it wouldn&rsquo;t scan through some of my friends&rsquo; thicker cases (e.g. Otter) or wallet cases. Again, not sure if the problem is on me or the ring.</p>
<p>I was surprised by how long ago NFC tags existed, even on smartphones. By 2010, Nokia released the <a href="https://en.wikipedia.org/wiki/Near-field_communication#History">first NFC-enabled smartphone</a>, and it&rsquo;d already been used in public transportation networks in 2009. Today, I think the most prevalent use-case for NFC that most people have interacted with is wireless payments and digital wallets like Apple Pay, as well as transit cards and other tap-based ecosystems. NFC stickers are fairly cheap now too: I found some to bulk order on AliExpress from $0.03 a piece.</p>
<p>Here&rsquo;s an interesting aside: NFC chip hand implants have been around since at least 2015. It&rsquo;s the same underlying technology as a ring, but you don&rsquo;t have to remember to bring it with you anywhere anymore. <a href="https://www.jhsgo.org/article/S2589-5141(24)00057-4/fulltext">This paper</a> estimates that 50K to 100K people have some sort of chip in their hand, so if you maybe cut that number in half for NFC implants in particular, that&rsquo;s still a sizeable 25K. <a href="https://www.npr.org/2018/10/22/658808705/thousands-of-swedes-are-inserting-microchips-under-their-skin">A 2018 NPR post</a> says it&rsquo;ll run you about $180, but if you&rsquo;re a DIY hacker (which I&rsquo;d expect most people who&rsquo;d think about injecting a chip into their hand would be), you can <a href="https://dangerousthings.com/product/xnt/">buy a kit to insert it yourself</a> for as little as $69. When I was younger, I briefly considered getting an implant, just because it was <em>weird</em> and it&rsquo;d have been a hell of a fun fact, but I also don&rsquo;t have much of a use for it. Giving someone a high five to pass on your LinkedIn is cool, but a little too extreme for a one-day Linux conference.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Every time I build a small side project over a weekend, I find myself learning much more than I&rsquo;d have imagined. I try to prototype out unique ideas that I haven&rsquo;t seen done before, or at least ones that combine fields that I know of, but not fully understand. Building Webring² and <a href="https://kewbi.sh/blog/posts/240721/">my HTML Day 2024</a> have inspired me to humour myself more and keep occasionally hacking on ideas that catch my fancy. The self-contained, timeboxed nature of these projects makes them more approachable than returning to my long-running projects, so maybe this is all just productive procrastination.</p>
<p>This has been the story of how I built one Webring², but I&rsquo;ve been toying around with the idea of buying up a bunch of NFC rings for my friends so we can have a Webring² ring — a Webring³, if you will. We could use a system slightly different than the current one, where each ring statically links to its own &lsquo;starting position&rsquo; (the URL of the friend that has the ring) but can navigate to any of the other sites. On the other hand, we could use the current implementation, where we dynamically fetch the starting URL on scan and have all the rings use the same position in the webring, but I think this&rsquo;d be a bit less personalized for each person with a ring.</p>
<p>There&rsquo;s probably more untapped potential in doing fun (or cursed) things with the NFC ring — I haven&rsquo;t looked too far into the technical details, but I&rsquo;d reckon there&rsquo;s some hacky stuff I could do. Even though the ring is a passive tag, <a href="https://www.reddit.com/r/homeassistant/comments/113acfr/what_do_you_use_nfc_tags_for/?rdt=33530">there are some very unique ideas for automations</a> I could rip off. If you&rsquo;ve been thinking of getting into home automation, or just playing around with NFC in particular, go <a href="https://github.com/kewbish/webringsquared">take a look at the repo</a> and set up your own. I&rsquo;m looking forward to seeing more webrings (or Webring²s) in the future!</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>To be fair, the other two were certainly available, but with my limited time there I didn&rsquo;t have much luck. Everyone simply redirected me to their online job board and I was busy during the preplanned mixers.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>It was one of the only manufacturers at this price point that had ring sizes small enough for my finger. I found several cheaper alternatives on Etsy and Amazon, but their ring sizes started at 10. For reference, that&rsquo;d probably be several millimeters too loose, even on my thumb. Small detail, but made me think about the target demographic and demand served a bit.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>Compile Your Life</title>
      <link>https://kewbi.sh/blog/posts/240728/</link>
      <pubDate>28 Jul 2024</pubDate>
      <author>Emilie Ma (Kewbish)</author>
      <description>On front-loading forethought.</description>
      <content:encoded><![CDATA[<p>When I was <a href="https://blog.cloudflare.com/introducing-the-2023-intern-ets">interning at Cloudflare</a>, my manager taught me about the concept of a &ldquo;WIP cloud&rdquo; and drilled it into the team. A WIP cloud is all the dangling PRs, unfinished work, loose threads that you have in flight at once, and it&rsquo;s dangerous if you let it grow. A WIP cloud starts out fairly easy to manage: you&rsquo;re wrapping up a PR or have just put something up for review, and now find yourself blocked on others with some spare time on your hands. Being an eager engineer, you look for something else to work on, or perhaps start iterating on the next PR <a href="https://kewbi.sh/blog/posts/230611/">in your stack</a>. Then, you wrap that up — maybe your original PR is still waiting for review — and poke around for some other paper cut to solve. Over time, you amass more and more stuff to just keep on your radar for later.</p>
<p>And then all your PRs get returned at the same time, or all your JIRAs switch from &lsquo;Waiting for response&rsquo; to &lsquo;Awaiting team response&rsquo;. The WIP cloud grows too heavy and starts to pour. You&rsquo;re suddenly swamped. Now all of those little odds and ends that were easy to keep track of require attention all at once, and while already under pressure, you need to figure out how to prioritize and action on them.</p>
<p>I find myself feeling overwhelmed when I have a WIP cloud brewing, particularly during busy weeks at school or when I have soft deadlines approaching at work. The commonly-cited <a href="https://lawsofux.com/millers-law/">Miller&rsquo;s Law</a> states that we can only keep seven items in working memory. After that, we need to rely on context clues to recall things — it&rsquo;s like paging from disk when the values can&rsquo;t be found in the cache. When I hit those WIP cloud limits and memory cache misses, I can tell I don&rsquo;t do very well figuring out what to tackle on the fly.</p>
<p>I&rsquo;m the type of person that says yes to too much and expects a lot of herself, so in some respects I&rsquo;ve come to anticipate the WIP cloud before the storm. I try to keep WIP in mind when I go about my work, but avoiding it isn&rsquo;t the most important. It&rsquo;s what I do to process the WIP, plan it out, and reduce mental load of having to swap things in and out of my working bubble that matters to me.</p>
<p>I&rsquo;ve found something that works well for me: a cycle of collection, curation, and execution that I do ahead of time so I can focus in the moment when the WIP storm&rsquo;s passing by. The metaphor I use for it is <em>compilation</em> — just like compilers, put in some work before the action so you can optimize and take away some computational load while running.</p>
<p>For example, something many &ldquo;how to become a morning person&rdquo; articles mention is laying your clothes out the night before, so you don&rsquo;t have to think about it in the morning, when you&rsquo;re groggier and vulnerable to taking any excuse to stay in bed. This lets you take advantage of your current, more aware, state, and gives you more time to spot imperfections with your outfit (it&rsquo;s a very me thing to do, but maybe it&rsquo;s not a good idea to wear dress pants for a picnic after all).</p>
<p>Compiling your life introduces a split between a more deliberate preparation phase and an execution phase that&rsquo;s ideally as frictionless as possible. This is adjacent to the divide between <a href="https://thedecisionlab.com/reference-guide/philosophy/system-1-and-system-2-thinking">System 1 and 2 thinking</a> popularized by Daniel Kahneman&rsquo;s <em>Thinking, Fast and Slow</em>. System 1 is a faster, snap-judgement mode of thinking, whereas System 2 is slower and requires more mental calculation. We don&rsquo;t want the execution of our work to be done in System 1, per se — it&rsquo;s still important work and we need to be deeply focused for it too. But we want to remove all barriers to starting that work, so we don&rsquo;t want to be forced to plan what work we&rsquo;ll do as we&rsquo;re doing it to mitigate procrastination and bikeshedding. We want to be able to access what we&rsquo;ll do in System 1 mode, so we can spend our System 2 energy on building, creating, and achieving.</p>
<p>If you&rsquo;re like me, I think you should compile your life too; at least, the times of high pressure and stakes. Make use of more self aware states to make sure even your future unmotivated, frazzled self knows what to turn to. Taking time to do all the logistics, prioritization, and prework first makes the actual work much easier. I&rsquo;ve used these strategies to make it through six finals seasons and three internships so far. While I think it&rsquo;s a bad idea to minutely preplan downtime like this, I&rsquo;ve found it easier to decompress and step away from my responsibilities if I know I can rely on having precompiled what I need to do when I do return.</p>
<p>This post will walk you through how I compile parts of my life, spin off into some loosely adjacent metaphors on compilation, and give you a better sense of how I&rsquo;ve applied my strategies from hell weeks to hijack my habits.</p>
<h2 id="get-in-loser-were-going-planning">Get in Loser, We&rsquo;re Going Planning</h2>
<p>The first pass of compilation is planning — collecting and curating whatever you&rsquo;ll be working on. I first started doing my planning in batches in advance when it came to my first finals season. It&rsquo;s too easy to realize it&rsquo;s the last week of class and your first finals are in a few days. When you&rsquo;re in that high-stress mindset and feeling like you have too many chapters to review and practice problems to work through, the last thing you want to do is spend more valuable time figuring out what to study when. But that&rsquo;s exactly what I think led to my success in my first few years: I made sure to carve out just an hour or so to figure out a game plan, try my best to cover everything optimally, then forget about editing it and start to focus.</p>
<ul>
<li>I make a Google Tasks list each time I have a lot of work to organize. Google Tasks is rather slow and frustrating, but it&rsquo;s easy to drag around tasks on the calendar, which makes reorganizing work quickly easier.</li>
<li>I go through and dump out all the chapters, exercises, review sessions, flashcards, or other things I need to do. This is the collection part of planning. See below for how I do this for each class.</li>
<li>I go in order of the closest exam or whatever I&rsquo;m most worried about, and space out each task to load balance across the time I have left.</li>
<li>I go to the next group of tasks, and repeat the same spreading out of work.</li>
<li>I go over everything and make sure no one day is too heavy. I also edit in some buffer time at this point if I have free days, and a few smaller time blocks to reassess how things are going and adjust my plan if need be.</li>
<li>I now have a plan of everything I need to do on that day. I&rsquo;ll sometimes copy over tasks into <a href="https://calcurse.org/">Calcurse</a> so I can time block and plan on the per-day level.</li>
<li>Then, when I have a study session or wake up for the day, I just reference my list and get to work. I don&rsquo;t doom about having so much to do, and I trust in my past self to have allocated time properly.</li>
<li>Sometimes, things come up, and I need to reorder work or add more. I block out more explicit planning time, drag things around, then go back into execution mode where I don&rsquo;t think about the what, just the how.</li>
</ul>
<p>The way I dump out my tasks is also somewhat meta-compiled. I have this template for each course of what I need to reference for each chapter or unit. It might be something like the exercises in the workbook, the review questions at the end of each chapter, and listening and speaking exercises from Canvas, for my French class, and the slides, the PrairieLearn questions, the textbook exercises, the clickers, and my flashcards, for my operating systems class. Then, for each unit, I might have specific areas I want to focus on, so I&rsquo;ll allocate more time and exercises from those sections. This is a little like <a href="https://rustc-dev-guide.rust-lang.org/backend/monomorph.html">monomorphization</a>, a Rust compiler construct for creating explicit instances of generic functions for each type that they&rsquo;re called with. Here, I&rsquo;m creating explicit tasks from the coursework templates for each of the units I have to work through<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<p>I also take time to prioritize during this organization phase: the curation part of planning. Part of making it through the WIP cloud is knowing that to cut or delay (delegation isn&rsquo;t usually an option for students). When things are laid out across my week in Google Calendar, I can start to see when certain days are too heavy, and if I can&rsquo;t move things around, I&rsquo;ll need to decide what&rsquo;s most important to focus on. A key part of this strategy, though, is making these decisions once while planning, and perhaps scheduling in explicit touchpoints throughout the few weeks that I&rsquo;m planning ahead. I try not to touch the prioritization outside of this time so I have one less thing that I can do to procrastinate, but more on this later.</p>
<p>During finals week, I can barely find the motivation to drag myself from bed most mornings (especially during the Fall term when I have to wake up in the gloom and dark), so taking the time to plan in advance is helpful so I can start my day right away and know I can make progress.</p>
<h2 id="from-clothes-to-chrome-tabs">From Clothes to Chrome Tabs</h2>
<p>The next pass of compilation is what I call the precomputation phase — substituting constants and doing basic computation ahead of time. There are certain algorithm problems where it&rsquo;s more efficient to batch compute all the results at initialization, cache it somewhere, then access those results at runtime. That&rsquo;s exactly what this preparation stage is useful for in real life as well.</p>
<p>We&rsquo;ve all heard trite advice somewhere to lay your clothes out the night before to become a morning person. When you&rsquo;re tired and grouchy, having already made your decision to lay out your workout clothes and your work fit makes it easier to just get up in the morning and go. That&rsquo;s the same principle precomputation exploits.</p>
<p>You might have heard of the concept of a &ldquo;trigger list&rdquo;, an idea first developed in <a href="https://gettingthingsdone.com/wp-content/uploads/2022/06/GTD_Incompletion_Trigger_List.pdf">&ldquo;Getting Things Done&rdquo;</a>, or a list of all the things you might need to remember or think about on a recurring basis. For example, it might look something like:</p>
<pre tabindex="0"><code>- meals:
	- planning
	- restaurants for eating out
- outfits
- chores:
	- laundry
- cleaning:
	- bathroom
</code></pre><p>This precomputation phase takes this idea one step further. Run through the list of triggers you have based on the plan you have, and set up your environment so that you&rsquo;ve done as much of The Thing as possible without actually doing it. For example, after deciding on your workout fit, fill your water bottle, roll out your yoga mat, and set out your equipment. Pull up the video you&rsquo;re planning to follow, pair your headphones, and queue your favourite playlist. By putting in all this prep, you&rsquo;re priming yourself to get up the next morning and just do the thing. Besides, it&rsquo;ll be easier than putting everything back away again.</p>
<p>One of my first Python projects was a script that&rsquo;d open up all the tabs I&rsquo;d want to check through each morning for notifications, including my email, social media, and messages. I haven&rsquo;t applied quite the same level of janky automation to my studying, but I follow a similar routine. I wrap up each day by going through my todo list for the next session and opening up the textbooks, practice problems, exams, and study tools that I&rsquo;ll need. When I open my laptop the next morning, I&rsquo;m greeted not by a blinking screen inviting me to tab over and scroll through Hacker News for an hour, but by what I need to get done.</p>
<p>For work and personal projects, I do the same — at 5PM, or whenever I&rsquo;m done for the day with my side project, I take a few minutes to figure out what I&rsquo;ll get done the next day, then open or bookmark the tabs I&rsquo;ll need to get started. For work, this tends to be opening up my PRs on GitHub so I can refresh and check their statuses the next morning, or the service deploy page so I can remember to hit the button and kick off some tests. For personal projects, I open up my design files for reference or docs pages for the libraries I&rsquo;ll be using for my next steps.</p>
<p>This principle is inspired by the <a href="https://en.wikipedia.org/wiki/Mise_en_place"><em>mise en place</em></a> cooking technique, so it&rsquo;s fitting that I also apply it to my meal prep. I don&rsquo;t follow recipes usually, so I write up my own list of prep steps and from there, set out all my ingredients and cookware and utensils. The weirdest example of this is probably what I do to set up for breakfast: the night before, I&rsquo;ll get all my cookware (pan, bowl, etc.) in place so the next morning, I can just turn on the stove and start cooking<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. This <em>mise</em> step is a textbook example of precomputation: doing the little things to get your station and environment set up so the actual cooking is much easier, and you&rsquo;re not scrambling around in your cupboards looking for a whisk while your sauce is scalding.</p>
<p>Collecting as much of the paraphernalia related to your work as possible ahead of time helps you avoid distractions. This is most obvious when it&rsquo;s applied to studying and work, but even in the kitchen or when I&rsquo;m doing chores, I find that not only knowing what I have to do, but also that I have everything in the right places to get it done, helps with removing all my mental blocks to getting in the flow.</p>
<h2 id="jit-compiling-and-spontaneity">JIT Compiling and Spontaneity</h2>
<p>You might think that this all sounds very rigorous and rigid, and it&rsquo;s supposed to be. Compiling your life is most effective for heads-down periods of life. There are times (e.g. finals week) where I don&rsquo;t want to think too hard about what I need to do and just execute. I do build in some escape hatches though: above, I&rsquo;ve mentioned allowing for checkpoints to rejig my schedule, say, in the middle of the week, if need be.</p>
<p>I also give myself the option to &ldquo;panic&rdquo; out and restructure at any time. However, I try to ensure that this decision is intentionally done — like an explicit context switch from my &ldquo;doing&rdquo; phase to &ldquo;planning&rdquo;. Forcing myself to only do so deliberately avoids situations where I don&rsquo;t want to actually work, so I make myself feel productive by reorganizing my time blocks, or revenge plan after feeling bad for not getting work done. It&rsquo;s also worth just recognizing what mode you&rsquo;re operating in at any time to be more cognizant if you find yourself flipping between modes often in order to better plan around that. <a href="https://linear.app/blog/planning-for-unplanned-work">This Linear blog post</a>, which inspired this post, also has some advice on planning for unplanned work. It&rsquo;s focused more on product work, but I think its ideas translate fairly well to areas of personal life.</p>
<p>There are plenty of times when I&rsquo;m less under pressure and I don&rsquo;t need so much of a drill-sergeant approach — in fact, I&rsquo;d say that most of the time putting so much effort into planning isn&rsquo;t fruitful for me. Planning and precomputation are useful when you have very clear goals for a shorter time horizon, when it&rsquo;s worth sticking to your metaphorical initially-thought-out guns. On the other hand, they&rsquo;re also premature optimization for lots of other situations. Your mileage may vary, and you&rsquo;ll have to define for yourself what sorts of times you need these tools and how far you&rsquo;ll take them.</p>
<p>For more daily-level plans, I try to use JIT compiling: I figure out what I&rsquo;ll do for the day the day-of, or at the start of a study session. I set aside some time to go through a less-structured version of what I have to do, including high level things I need to think about, and decide then. I timebox these times though, since I feel like it&rsquo;s super easy for me to get off track trying to figure out the best way to do something, when I just have to go do it.</p>
<p>This summer, I don&rsquo;t have as many strict responsibilities, so I&rsquo;m letting myself be looser with all this. I precompute mostly for menial, repetitive tasks, like cleaning and meal prep, and I&rsquo;ve been blocking in more time to let things come up.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Program, precompute, perform. This is, so far, my recipe for compiling my life, and this is how I&rsquo;ve gotten through the last few years of my university and industry career.</p>
<p>I&rsquo;ve been told this sounds robotic, but I&rsquo;ve realized it&rsquo;s just like being your own <a href="https://en.wikipedia.org/wiki/Secretary#Executive_assistant">executive assistant</a>. Executives tell their ABPs the high-level initiatives they want to focus on, the presentations and tasks they have to focus on, and the events they need to attend: all the execution work they need to do. Then the ABPs and EAs go out and do all the legwork, the organization, the prepwork, to make it happen. EAs are a separate person from the executive, which lines up well with my ideas of splitting up planning and prework into a distinct phase before execution. My notion of precomputation isn&rsquo;t even that novel: sure, maybe the EAs don&rsquo;t go to the extent of logging onto their exec&rsquo;s laptop to open up all their tabs for them, but they prepare summaries and readings and generally set up their mental environment. If the high net-worth individuals and successful businesspeople of the world have decided to spend money on outsourcing their planning and precomputing so they can focus on the executing, I think there&rsquo;s some value in doing the same for ourselves too.</p>
<p>The next time you start feeling overwhelmed, consider compiling parts of your work. Do as much of the pre-work and the hard thinking ahead of time, when you&rsquo;re in a planning mindset, as possible — as much of it as you can without actually doing it. Make the execution the easy part.</p>
<p>I still remember my manager intoning &ldquo;Don&rsquo;t let that WIP cloud grow!&rdquo; at most team strategy meetings. At my current internship I try to remind myself of that whenever I have a free moment and reflexively think of picking something new up. Sometimes I listen and I don&rsquo;t go for the shiny new thing, and sometimes I do. But when all the consequences of my pending, in-progress, &ldquo;waiting on action&rdquo; work hit at the same time, compiling my life helps me make sense of the chaos. Whenever my WIP storm touches down on my mental ground zero, I trust that I&rsquo;ve done the work ahead of time to make it through.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>I will admit this metaphor is a little stretched, but I really wanted to mention monomorphization. A throwback for those who know.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>I keep the food itself in my fridge though. I don&rsquo;t think <em>mise en place</em> is a good idea if you&rsquo;re not immediately preparing the food.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>HTML Day 2024</title>
      <link>https://kewbi.sh/blog/posts/240721/</link>
      <pubDate>16 Jul 2024</pubDate>
      <author>Emilie Ma (Kewbish)</author>
      <description>On the vestiges of HTML.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>July 13th last week was <a href="https://html.energy">HTML Day 2024</a><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. HTML Day is a community-organized event where folks gather around the world to freewrite HTML. The meetups seem to take place generally in parks or beaches (so no Internet access) with fun prompts like &ldquo;make a website inspired by the sounds you hear&rdquo; or &ldquo;make a website inspired by your favourite, lesser-known element&rdquo;. Some folks even handwrite their HTML — note that the event descriptions state to bring a &ldquo;HTML-writing device&rdquo; or &ldquo;something to write HTML on&rdquo; and not &ldquo;a laptop&rdquo;. The intent is less of a &ldquo;Google all the things and make a technically challenging website&rdquo; and more of a &ldquo;go back to the roots of raw HTML and have fun&rdquo; vibe.</p>
<p>I was planning to go to my local HTML Day as well, but it was a bit of a transit trek. By the time I&rsquo;d gotten to one of my transfers, I decided just to head to a library and have my own little HTML day instead. Armed with a slice of surprisingly high-quality carrot cake from a cafe nearby, I sat down for an afternoon to hack away.</p>
<p>I&rsquo;d originally come up with some slightly cursed ideas with raw CSS: building a Shamir Secret Sharing tool with server-generated CSS files that could be recombined to regenerate the secret was my initial plan. I&rsquo;d also thought about making some kind of HTML emoji canvas with <a href="https://github.com/knobiknows/all-the-bufo">bufo emojis</a> as a little puppet show. Going back to my <a href="https://kewbi.sh/blog/posts/211031/">spaced-repetition enjoyer roots</a>, making a CSS-based Leitner system was on the list as well. I wondered if I could somehow rig up a hash function in CSS to get to a sort of blockchain, and I considered using CSS as an API as <a href="https://css-tricks.com/custom-properties-as-state/">mentioned in this CSS Tricks post</a>. However, these ideas all hinged around <a href="https://kewbi.sh/blog/posts/240303/">abusing</a> CSS instead of HTML. It&rsquo;s the one thing about HTML Day: HTML is supposed to be the star of the show.</p>
<p>I was feeling pretty uninspired at some point, so I looked to the <a href="https://html.energy/">html.energy</a> page for past event prompts. I found a <a href="https://d2w9rnfcy7mm78.cloudfront.net/22595115/original_569f17680cabc94a7555a7d764e94232.jpg?1688960301?bc=0">pic in their event gallery</a> that mentioned finding a lesser used HTML element and making a page around that, and I started thinking of older, retro (to me) tags like <code>&lt;marquee&gt;</code> and <code>&lt;blink&gt;</code>. They&rsquo;re what I think about when I imagine the Internet of the 90s/00s (please keep in mind I was not around for said Internet era): the &ldquo;the more I know&rdquo;-esque, Neocities-core, loud and gaudy and interactive pages. I don&rsquo;t know if these tags were ubiquitous then, but they&rsquo;re pretty well-used now by tongue-in-cheek people looking to recreate that aesthetic.</p>
<p>Unfortunately, they&rsquo;re also deprecated. In particular, <code>&lt;blink&gt;</code> isn&rsquo;t even implemented in modern browsers anymore — it has to be recreated via a CSS animation. I started thinking of other tags that had seen better days and were no longer implemented, and I set out on a quest to find the weirdest tags that I&rsquo;d never heard of and bring them back to life via a little extra CSS or JS. I started envisioning a circus with a freak show of resurrected tags brought back from the dead, but not quite behaving the same way: think PT Barnum crossed with Frankenstein.</p>
<p>The result was <code>1pt bar&lt;number&gt;'s travelling circus: html element freak show'</code> — an ASCII-art heavy little page showing off a few freaks of Internet nature. In this post, I&rsquo;ll expand a little on some things I learned while making the page, both about the tags themselves and the Web in general. This post is full of links to obscure pages and jumping-off points for further reading, and I hope it&rsquo;ll spark some rabbitholes of your own!</p>
<p>Here&rsquo;s an embed of <a href="/files/240721/html-day-2024.html">the site</a> if you&rsquo;d like to check it out:</p>


<div style="display: flex; justify-content: center; margin-bottom: 1em; background-color: white; overflow: none">
	<iframe src="/files/240721/html-day-2024.html" style="width: 100%" height="800" frameborder="0" ></iframe>
</div>


<h2 id="blink-twice-and-youll-miss-it"><code>&lt;blink&gt;</code> Twice and You&rsquo;ll Miss It</h2>
<p>The <code>&lt;blink&gt;</code> tag used to, as may be obvious, blink text. According to <a href="https://www.fastcompany.com/3015408/saying-goodbye-to-the-html-tag">this article by Fast Company</a>, it was the result of a joke turned challenge: engineers working on <a href="https://lynx.invisible-island.net/">Lynx</a> were laughing at some of their wilder ideas for the browser, seeing as it couldn&rsquo;t even blink text. One of the engineers took this as a dare, and returned the next morning with a working blinking tag. It was never standardized, and few browsers supported it, but up until 2013, Firefox supported it, and Google&rsquo;s WebKit fork was even named after it. There was also a <code>text-decoration: blink</code> CSS declaration available, but it&rsquo;s also deprecated and usually not supported.</p>
<p>One reason for its removal seems to have been accessibility concerns: the blinking makes it hard to read text and could potentially trigger seizures for those with epilepsy. But the other reason cited is its lack of standardization.</p>
<p>Part of the beauty of the web is standardization: for the most part, you can trust that things will render the same across browsers. However, there&rsquo;ll always be little differences, even in well-specified standards — how else would we be able to do browser fingerprinting even without user agent headers? It makes sense that browsers would want to reduce extra surface area for divergence and focus on the specification, especially since you can recreate its functionality with the rest of the standard.</p>
<p>To implement the blinking eye on the page, I used a looping CSS animation that would toggle <code>visibility: hidden</code> on and off. <code>visibility: hidden</code> differs from <code>display: none</code> in that it keeps the element&rsquo;s block present, so it doesn&rsquo;t shift around on the page, but stops rendering the element itself.</p>
<h2 id="keygen-and-early-web-auth"><code>&lt;keygen&gt;</code> and Early-Web Auth</h2>
<p>The <code>&lt;keygen&gt;</code> element was used to generate a public/private keypair to be used in forms to validate the server&rsquo;s response (yes, back when PHP was all the rage!) <a href="https://www.youtube.com/watch?v=KhGVPriid58">Here&rsquo;s a video I found of the tag</a>: the &lsquo;High Grade&rsquo; refers to the strength of encryption via the length of the generated key. <a href="https://www.wufoo.com/html5/keygen-element/">This page</a> lists 2048 as the &lsquo;High Grade&rsquo; encryption, which isn&rsquo;t nearly as low as I thought considering the feature was implemented well before 2016.</p>
<p>When a form containing a <code>&lt;keygen&gt;</code> had its submit handler called, a public/private keypair would be generated locally. The private key would be kept local, and the public key sent to the server and signed by the server&rsquo;s own certificate to create the client&rsquo;s certificate. This was later used to authenticate the user. Most (then-) modern browsers implemented <code>&lt;keygen&gt;</code>, including Netscape, where it originated, Opera, Firefox, and Safari, and the feature was used in quite a few places, including in early online banking. However, IE notably decided not to support the feature, which might have led to its eventual decline, deprecation, and <a href="https://github.com/w3c/html/issues/43">removal from the HTML standard</a>. <a href="https://stackoverflow.com/questions/59289545/why-has-firefox-dropped-support-for-the-html5-keygen-tag#comment104798273_59289545">This SE comment</a> seems to imply that client certificates were all the rage before being supplanted by passwords — it makes sense, since early Internet users were likely fairly technical in order to onboard in the first place, but this demographic and thus this need for easier-to-understand auth shifted as adoption among the general public increased. There&rsquo;s a <a href="https://web.archive.org/web/20160409081411/https://lists.whatwg.org/pipermail/whatwg-whatwg.org/attachments/20080714/07ea5534/attachment.txt">very in-depth doc here</a> that explains a bit more of the background context.</p>
<p>If you want to really fake a <code>&lt;keygen&gt;</code> element, <a href="https://www.chiark.greenend.org.uk/~sgtatham/keygen-fake/">this blog post</a> shows you how to do it. Surprisingly, as of a few years ago when this article was published, some CAs still required the use of a <code>&lt;keygen&gt;</code> element, and their official guidance was to install and use IE11 to work around the tag&rsquo;s removal in modern browsers<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<p>But if you&rsquo;re happy just making a little <code>&lt;keygen&gt;</code> that displays an ASCII key when clicked, as I&rsquo;ve done on the page, it&rsquo;s nowhere near as complicated. I felt like using JS was a bit too easy and besides the point of HTML Day, so I made a CSS-only click handler. The TL;DR is that we can wrap the <code>&lt;keygen&gt;</code> element in a <code>&lt;a&gt;</code> with some <code>href=&quot;#target&quot;</code>, and when the <code>&lt;keygen&gt;</code> (really, the <code>&lt;a&gt;</code>) is clicked, <code>#target</code> is appended to the end of the URL. Interestingly, there&rsquo;s a pseudo-element <code>:target</code> that allows you to style tags that are targetted, so I simply added a <code>&lt;span&gt;</code> with a key symbol and set that to <code>display: block</code> only when the element is targetted. The downside of this is that it&rsquo;ll jump to said target, causing a bit of a jarring visual effect, and if <code>#target</code> is removed from the URL, the key symbol no longer displays. I figured it was enough of a proof-of-concept even with these rough edges, though. This DigitalOcean post has more details <a href="https://www.digitalocean.com/community/tutorials/css-css-only-click-handler">here</a>.</p>
<h2 id="dyk-theres-a-v-tag">DYK: There&rsquo;s a <code>&lt;V&gt;</code> Tag?</h2>
<p>There a few special-purpose text formatting tags like <code>&lt;big&gt;</code>, <code>&lt;center&gt;</code>, <code>&lt;s&gt;</code> (strikethrough), and <code>&lt;u&gt;</code> (underline) that were also deprecated. Scrolling down <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element">this list</a> should give you a sense of the deprecated presentational tags — look for the red trash-can icon beside each element&rsquo;s name.</p>
<p>I also wanted to call out a counterexample of these text formatting tags getting deprecated: <code>&lt;small&gt;</code>. <code>&lt;small&gt;</code> was supposed to be the counterpoint to <code>&lt;big&gt;</code>, but <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/small">isn&rsquo;t deprecated</a>. This is because of its reclassification as a semantic element: the Mozilla docs state that it&rsquo;s intended to represent copyright and legal small print, and importantly, it represents these things <em>independently of its styled presentation</em>.</p>
<p>This is exactly the reason these tags were deprecated in the first place. While these presentational tags were part of previous HTML standards, they&rsquo;ve been deprecated because they focus on the the styling of some text instead of its semantics. Note that <code>&lt;b&gt;</code>, which wraps bold text, is by default rendered the same way as <code>&lt;strong&gt;</code>. However, again, <code>&lt;b&gt;</code> marks the content just as bold, whereas <code>&lt;strong&gt;</code> denotes some sort of emphasis. This gives the user the freedom to represent the semantics however they want: perhaps a user likes to see <code>&lt;strong&gt;</code> text italicized in a different font instead<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>.</p>
<p>If you&rsquo;re getting tired of all this deprecation and thinking &ldquo;to hell with the standard, I just want to have have presentational elements!&rdquo;, go right ahead. The proper way to create custom HTML elements is with JavaScript and the <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_components/Using_custom_elements">Web components API</a>. You can use <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_components/Using_templates_and_slots">templates and slot in your own content</a> and even <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_components/Using_custom_elements#registering_a_custom_element">register it for use in HTML docs</a>. Custom elements are very powerful because they allow you to hook into different lifecycle events, like attributes changing.</p>
<p>One glaring problem with custom elements, though, is that they <a href="https://html.spec.whatwg.org/multipage/custom-elements.html#valid-custom-element-name">must follow the standardized naming convention</a>. This forbids you from using a custom name like <code>&lt;leet&gt;</code> without a hyphen, making it obvious that you&rsquo;re using a custom element. This is fine for most (all) use cases, but for my HTML day page, I really wanted the markup to be as clean as possible and avoid relying on JS.</p>
<p>So imagine my surprise when I tried just writing <code>&lt;blah&gt;content&lt;/blah&gt;</code> and adding a simple <code>blah { color: red }</code> style and seeing the text brighten up on my screen. I didn&rsquo;t realize that browsers would parse this as valid HTML (they&rsquo;re <code>instanceof HTMLUnknownElement</code>). You can style these fake tags just as you would normal HTML tags, as long as they&rsquo;re accepted as a tag name: underscores and hyphens in the tag name work, but periods don&rsquo;t. I didn&rsquo;t look up the exact rules here, but I&rsquo;d expect you can probably find more restrictions.</p>
<p>On my HTML day page, I&rsquo;d already mentioned <code>&lt;s&gt;</code> and <code>&lt;u&gt;</code> as deprecated tags (they aren&rsquo;t, see footnote)<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>, and I just applied a little <code>text-decoration: none</code> to them to remove their strikethrough and underline respectively. I also thought it&rsquo;d be funny to round out the alphabet here, with the fictitious <code>&lt;r&gt;</code>, <code>&lt;t&gt;</code>, and <code>&lt;v&gt;</code> tags. Of these, I think the <code>&lt;v&gt;</code> element is most interesting: it represents a little graffiti tag (pun not intended) that has blue Comic Sans (or whatever your browser default <code>cursive</code> font is) angled on top of the existing text. I did this via a <code>:before</code> pseudo-element: I added <code>v { position: relative }</code> and <code>v:before { position: absolute; left: 0 }</code> in order to get the pseudo-element to display directly on top of the <code>&lt;v&gt;</code>&rsquo;s content. Then, by setting the <code>:before</code>&rsquo;s <code>content</code> property, I was able to add the extra graffiti text. Finally, I added a little extra styling to rotate the text as if it&rsquo;d really been tagged onto the underlying content: I didn&rsquo;t expect transforms to work on pseudo-elements properly, but <code>transform: rotate(25deg)</code> did the trick. In retrospect, it might have been nice to add some drop shadows to make it seem like the &lsquo;paint&rsquo; was dripping, but an effect to explore for later.</p>
<h2 id="conclusion">Conclusion</h2>
<p>I came into HTML Day intending to hack together a funny page and move on, but I&rsquo;ve been able to learn more about how Web standards have evolved and a bit more about the history of HTML. As time went on, I can see the decisions to shift responsibility from more static technologies (HTML) into more dynamic ones (CSS, JS). I&rsquo;ve always thought of these three main Web languages as all wrapped up, listed as one big slash-delimited acronym on my resume. It&rsquo;s been interesting to reflect on the patterns of deprecation and how they&rsquo;ve very intentionally decreased possibilities for doing the wrong thing while separating each language&rsquo;s responsibility more clearly.</p>
<p>At first, coming into this whole deep dive, I&rsquo;d thought that deprecating tags was more restrictive, since it was encouraging people not to use more varied and diverse elements. Now, I think culling out-of-scope tags and focusing on these very delineated responsibilities is more freeing. The example that cemented this for me was that having clearer semantics, not mixing in presentational tags, and not implementing non-standard behaviour can allow users to specify how they want to view their semantics: for example, bolding an <code>&lt;em&gt;</code> tag or listing all <code>&lt;big&gt;</code> text in a different font style. By promoting clean semantics and removing possibilities for self-contradiction within the spec, users can trust the standard, and by extension their browsers, more.</p>
<p>Besides really enjoying the history, I learned that what I&rsquo;ve learned about HTML from modern web development via React et al. hasn&rsquo;t even scratched the surface of what (raw) HTML has to offer. I&rsquo;d previously considered myself fairly well-versed in HTML and CSS, but I&rsquo;ve realized that I&rsquo;m severely lacking in more of its history and its weirder corners. It&rsquo;s nice going down a little rabbithole, since the standard is so sprawling and there&rsquo;s so much to specify: there&rsquo;s bound to be interesting edge cases.</p>
<p>This exploration also puts into scale how complex web browsers are, yet how basic an initial set of tags that covers most functionality can be. In everyday dev, I probably only use 10-15 tags, whereas there are <a href="https://devdevout.com/html/how-many-html-tags-are-there">142 defined in the HTML5.2 standard</a> and probably many more non-standard ones, like <code>&lt;blink&gt;</code>. There are books covering <a href="https://browser.engineering/">how to build your own fully-fledged browser</a>, and I&rsquo;ve been thinking of skimming it to see if I&rsquo;d find web browser internals interesting. Once I&rsquo;d learned that <code>&lt;blink&gt;</code> wasn&rsquo;t implemented in modern browsers, I was considering having my HTML Day project being creating a custom fork of Chromium that automatically injected a CSS blink animation, but I figured with the time I had (and the battery, since I forgot my charger) it was something to leave for another day. It might be easier to figure out how to implement <code>&lt;blink&gt;</code> with a smaller browser like the one I&rsquo;d build with browser.engineering, so I might take a look.</p>
<p>Writing this little summary and building a small site was very fun. HTML Day was a well-timed excuse to build something for the sake of building it, with some loose constraints that were a great forcing function for my creativity. I&rsquo;m excited for the next events in August for HTML&rsquo;s birthday and what I&rsquo;ll learn there. Hopefully I&rsquo;ll be able to make it in person this time — I saw some Exquisite Corpse-inspired and handwritten HTML shenanigans that&rsquo;re really making me look forward to it.</p>
<p>If you didn&rsquo;t take a stab at anything for HTML Day, I&rsquo;d encourage you to find one of the deprecated tags in this post (or one of the supported ones in <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element">this list</a>) and try to figure out the weirdest edge cases and quirkiest uses you can think of for it! What you learn might just <code>&lt;em&gt;</code>aze you.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>As far as I can tell, July 13th isn&rsquo;t related to the history of HTML in any way, so I&rsquo;m assuming it&rsquo;s some arbitrary date picked by the organizers for people around the world to freewrite HTML. They also have an August series of events for HTML&rsquo;s birthday, but it&rsquo;s hard to pinpoint an exact date of HTML&rsquo;s launch, at least according to a cursory Google.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Arguably a sign that you might want to use a different CA.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>I&rsquo;ll also note that <code>&lt;b&gt;</code> is not deprecated in HTML5 anymore, and has specific use cases as the <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/b#usage_notes">&ldquo;Bring Attention To&rdquo; element</a>.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>While writing this post, I&rsquo;ve discovered that <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/s">neither are</a> <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/u">actually deprecated</a>, which is a little embarrassing.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>The Tireless Pupil&#39;s Talk Prep Primer</title>
      <link>https://kewbi.sh/blog/posts/240609/</link>
      <pubDate>09 Jun 2024</pubDate>
      <author>Emilie Ma (Kewbish)</author>
      <description>On delivering my first conference talk, in the midst of exams.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>A few weeks ago, I posted about my first talk: a <a href="https://ossna2024.sched.com/event/1aBNC/the-github-graph-characterizing-open-source-collaboration-patterns-emilie-ma-university-of-british-columbia">session about understanding open-source collaboration patterns</a> at <a href="https://events.linuxfoundation.org/open-source-summit-north-america/">Open Source Summit North America 2024</a>. I also posted a separate journal entry about <a href="https://kewbi.sh/blog/posts/240602/">my day-of experience</a>, which includes some more reflections on being a first-time attendee and speaker.</p>
<p>If you&rsquo;d prefer to watch my talk rather than read <a href="https://kewbi.sh/blog/posts/240526/">the previous post</a> for context, here it is!</p>


<div style="display: flex; justify-content: center; margin-bottom: 1em">
	<iframe width="560" height="315" src="https://www.youtube.com/embed/iUZaW_cjwYU" title="The GitHub Graph: Characterizing Open-Source Collaboration Patterns - Emilie Ma" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div>


<p>In this post, I wanted to list a few quick points about writing a talk that I&rsquo;d wish I&rsquo;d known before embarking on this rather harrowed adventure (to see why it was so stressful, see the <a href="https://kewbi.sh/blog/posts/240602/">previous post</a>.) It&rsquo;s by no means an exhaustive list, but I hope it&rsquo;ll be helpful to anyone in a similar boat of being incredibly busy while also prepping for a talk.</p>
<h2 id="giving-a-talk">Giving a Talk</h2>
<p>The research I presented in my talk was part of a project I&rsquo;d worked on for a Directed Studies course at university a few terms ago. As part of that course, I was required to give a final presentation summarizing my work (and in fact, I presented twice to two different labs!), so I&rsquo;d already gotten a set of slides done and a preliminary set of notes. However, that talk was primarily targetted towards my contributions to the project, the quantitative statistics and the tooling. It had a heavy focus on rather convoluted statistics and my own report as opposed to the more qualitative findings presented in the final paper. For OSSNA2024, I needed to reframe the talk for more of a general audience and highlight more of the big picture takeaways from our project.</p>
<p>In some ways, starting with an already fleshed out set of slides and a script was both a blessing and a curse. It was great to not have to start from scratch, which would have taken up valuable time, but it also lulled me into a false sense of security and made it much easier to procrastinate on making these more fundamental changes to talk pacing or structure. I applied for OSSNA2024 near the start of January, and heard back with an acceptance in February, so I had two full months to slowly chip away at these updates. Because I felt like I was so ahead of the preparation, however, I ended up only really focusing on my talk in mid-March, which didn&rsquo;t leave me with enough time to comfortably revamp the talk. I was balancing studying for finals season as well as a steady courseload, and with more urgent assignments and due dates always popping up, I kept deprioritizing talk prep and pushing it to later and later. I ended up having to pull some long, full-throttle days between when classes ended (the 11th) and my talk (the 15th), which was not ideal. I think of myself as a rather organized person who can estimate workloads and plan my time fairly well, but even I really dropped the ball on this process. If you take one thing away from this post with regards to starting public speaking for the first time, make it this: start earlier than you think, preferably right after you hear back that your talk has been accepted.</p>
<p>I worked on my script and slides in tandem: I think I speak better when I have a set of rehearsed sentences. I don&rsquo;t do well with just bullet points: I end up either reciting them, stammering while trying to find the right verb, or repeating the same adjectives and verbs over and over. The talk writing process was a cycle of adding a few sentences or a paragraph to my script, adding them to my slides, coming up with a visual, setting up animations, and repeating. I memorized my script (not as well as I&rsquo;d like) by repeating it over and over to myself until I knew the cadence and connections between slides. I&rsquo;d set aside half an hour each day to practice with just the script. Some days, I&rsquo;d take my after-dinner break or lunch break to rehearse as well: my friends once saw me pacing in circles in the hallway outside our club room muttering points to myself for an hour and were genuinely concerned if I was alright. In hindsight, I should have started practicing the script earlier with the slides, since it took some time to get the cues for animations and points right. The first few times I tried speaking with the slides, it felt very unnatural trying to juggle recalling my lines while trying to predict slide transitions.</p>
<p>Something I severely underestimated was how important the structure and the pacing of a talk was. I knew the gist of the raw information I wanted to impart, but it was only after several rounds of feedback from my professors that the talk started feeling more natural instead of like it was jumping all over the place. For example, I&rsquo;d previously put methodology at the end, since I thought people were more interested in what we were presenting than how we got that information. Later, I realized that sharing the &ldquo;how&rdquo; created valuable context for understanding the &ldquo;what&rdquo; and as per one of my professors&rsquo; suggestions, I moved it near the start of the talk. I also spent a lot of slides at the beginning reiterating various features of GitHub, which sluggishly dragged out the start, especially since the attendees were all open-source enthusiasts and would most probably already use GitHub. As well, my talk was covering collaboration patterns on GitHub, so we&rsquo;d created a list of these prototypical workflow types. Since I wanted to share as much information about our project as possible, I initially included summaries of all nine. After reading my talk out loud a few times and after feedback from my supervisors, I realized that it&rsquo;d be painful, but I&rsquo;d have to cut some of them out. Repetitive, overly detailed deep-dives into a topic aren&rsquo;t amenable to a shorter talk: listeners start tuning out after the first few points that appear similar at first glance but differ in super minute ways. Don&rsquo;t linger on any one section too long — try to divide your talk into roughly equally sized chunks for each subtopic. I think it&rsquo;s easier to keep people engaged when you cover a wider variety of material at a medium level of detail, particularly if some of that information is implications or real-world examples of the initial theory.</p>
<p>I did all my slide design in Figma. I wrote a little post about my experiences using Figma for presentations <a href="https://kewbi.sh/blog/posts/231231/">here</a> if you&rsquo;re interested in the general workflow. On top of that post, I&rsquo;d like to add that while Figma makes it very easy to design and share visually-appealing slides, the presentation workflow for serious talks is very much lacking. If you want speaker notes that are synced to the slides, you&rsquo;ll have to use comments, read from the tiny sidebar, open up two tabs, and use the follow cursor option: not a great experience. I ended up just memorizing my speaker notes and running from memory and the slides, but it would have been a nice safety net for my first ever talk.</p>
<p>On the topic of slide design, remember to visually highlight the key points you&rsquo;re making orally. For example, I tended to click through a series of bullet points talking about a graph without pointing out what part of the figure the point was referring to (e.g. &rsquo;the isolated nodes&rsquo; → should change the colour of or otherwise point out the bar chart for the &lsquo;size 1&rsquo; nodes) I would also sometimes make summarizing statements or talk about implications without writing them down on the slide, as I felt they were natural conclusions to draw from the rest of the points on the screen. It&rsquo;s helpful to make these TL;DR lines explicit, though, and I got a lot of head-nodding and sneaky-phone-camera-picture-taking when I clicked into one of these points.</p>
<p>One of the most helpful steps during this talk writing process was running through my talk and getting feedback on those runthroughs. If you have supervisors, mentors, or even just willing friends, it&rsquo;s very helpful to sit them down and rehearse to get a sense of how the talk <em>feels</em>, in terms of pacing, content, and depth. I recorded an early draft runthrough and shared it with my professors, who gave me plenty of valuable advice for where things felt too slow or where things needed more highlighting. This is why starting early is also so critical: more time means more runthroughs and more actionable improvements between runthroughs. I really wanted to present this talk to my friends to see what they&rsquo;d think of it and if they could follow it, since they come from an earlier stage in their career and have less exposure to open source, but I ran out of time.</p>
<h2 id="conclusion">Conclusion</h2>
<p>I think there&rsquo;s still a lot I need to learn with regards to architecting a talk, defining its purpose and takeaways, finding my voice, and learning to speak with more confidence. I don&rsquo;t have a lot of experience with public speaking — I quit my school&rsquo;s debate club after one meeting — but it&rsquo;s something that I want to work on. When I envision my future career, it involves a lot of mentorship and knowledge sharing, and clear communication, particularly verbal, is a skill that&rsquo;ll aid me do that more successfully.</p>
<p>If you want more concrete advice on writing a talk, I&rsquo;d highly recommend <a href="https://mercedesbernard.com/blog/start-conf-speaking-idea/">Mercedes Bernard&rsquo;s series on conference speaking</a>. I also found <a href="https://ines.io/blog/beginners-guide-beautiful-slides-talks/">Ines Montani&rsquo;s page on making beautiful slides</a> good inspiration, though their style might be a bit over the top in some fields.</p>
<p>This will be my last OSSNA 2024 summary post, but I wanted to extend one last thank you to the organizing team and the Linux Foundation for letting me break out of my shell and give my first talk. I couldn&rsquo;t have asked for more inclusive and comfortable conference to start my public speaking career.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>OSSNA 2024</title>
      <link>https://kewbi.sh/blog/posts/240602/</link>
      <pubDate>02 Jun 2024</pubDate>
      <author>Emilie Ma (Kewbish)</author>
      <description>On rescheduling a final for my first conference talk.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Last week, I posted a summary of my first talk: a <a href="https://ossna2024.sched.com/event/1aBNC/the-github-graph-characterizing-open-source-collaboration-patterns-emilie-ma-university-of-british-columbia">session about understanding open-source collaboration patterns</a> at <a href="https://events.linuxfoundation.org/open-source-summit-north-america/">Open Source Summit North America 2024</a>. I was able to share my work with OSS folks, meet up in-person with some people I&rsquo;d only met online, and take in all the buzz in the brand new Seattle Convention Centre. If you&rsquo;d prefer to watch my talk rather than read <a href="https://kewbi.sh/blog/posts/240526/">the previous post</a> for context, here it is!</p>


<div style="display: flex; justify-content: center; margin-bottom: 1em">
	<iframe width="560" height="315" src="https://www.youtube.com/embed/iUZaW_cjwYU" title="The GitHub Graph: Characterizing Open-Source Collaboration Patterns - Emilie Ma" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div>

This post is a collection of little anecdotes from the day-of as well as my reflections on the overall experience. I'll briefly cover some tips on preparing for a talk in a separate post to come.


<h2 id="day-of">Day-Of</h2>
<p>Even though the conference was three days and I would&rsquo;ve had a ticket to all three, I unfortunately had a final at 8:30AM the day of my talk (the first day of the conference) and another at 7PM the day after. When I applied to speak, I was banking on the fact that my courses were upper level and smaller than the intro / 100-level courses that usually have their exams scheduled for the first few days of finals. But I was unlucky: the first day of the conference was also the first day of exam period, and I had two finals back-to-back. I was initially debating if I&rsquo;d even be able to make it to give my talk, but thankfully I was able to reschedule my talk for a later time so I&rsquo;d theoretically be able to book it from Vancouver, right after my first exam, to Seattle. And later, in another stroke of luck, I went to the professor for the first exam and successfully begged to be able to sit my exam during the final for another course they were teaching. I was incredibly surprised that there were no questions asked on his part, but I certainly wasn&rsquo;t complaining.</p>
<p>With my first exam cleared out of the way, I&rsquo;d be able to leave Vancouver in the morning of my talk and make it to Seattle by lunchtime. My parents were tagging along to sightsee in downtown Seattle, so we bundled into the car early on the 16th and headed out. On the drive there, I was trying to silently run through my talk script, but something about sitting shotgun and staring out at the highway passing by isn&rsquo;t very conducive for focus. I was aiming to run through the talk at least a few more times there. The drive was ~3h and my talk was ~30min, so I could have reasonably got through it 5-6 times, but I got through it about twice, in fits and starts. At this point, I was starting to feel uneasy and underprepared, but we were soon reaching downtown Seattle and I had to go back to navigating off the highway.</p>
<p>When I got dropped off at the venue, I was in awe. We were in the brand-new, Summit building of the Seattle Convention Centre, so there were plenty of tall ceilings, natural light, and cute seating areas. Everything was sparkling clean and impeccably professional, which perhaps I haven&rsquo;t come to expect given my typical university surroundings. It was nice feeling starry-eyed with the sunlight streaming in through the floor-to-ceiling windows, running around gawking at all the amenities, decor, and spaces. One of my favourite places in the center was this garden patio decorated with little fairy lights, outdoor wooden seating, little tree-lined pathways, and a view out into the center of downtown. It looked like something out of an IKEA late-summer-nights catalog, and I spent some time rehearsing my talk there as well as enjoying the free breakfast my second day.</p>
<p><figure><img src="/img/240602/patio1.jpg"/>
</figure>

<figure><img src="/img/240602/patio2.jpg"
         alt="Figure 1. The garden patio."/><figcaption>
            <p><em>Figure 1. The garden patio.</em></p>
        </figcaption>
</figure>
</p>
<p>I also really liked this seating area by the stairs: there&rsquo;s something about the impossibly high ceilings, comfy atmosphere, and just the sun that made it feel very cozy. If I sound like I&rsquo;m appreciating the sun a lot, it&rsquo;s because we 1) weren&rsquo;t getting this type of sun in Vancouver and 2) in UBC there are few spaces that capture the light so well. The warm tones of the wood and the sun made even the shadowier sections look inviting.</p>
<figure><img src="/img/240602/stairs.jpg"
         alt="Figure 2. The stairs seating area."/><figcaption>
            <p><em>Figure 2. The stairs seating area.</em></p>
        </figcaption>
</figure>

<p>When I arrived, I spent some time walking around the sponsor showcase. I&rsquo;d never been to a conference before and was surprised at the level of detail and theming that went into some of these flagship booths. AWS&rsquo;s booth was themed like a 90s diner, complete with the bar stools and a retro menu remixing AWS services and tools into burgers. There was even a little window where someone was manning a cookie giveaway (only after you&rsquo;d listened to their spiel, of course). The Gitbook stand had a barista, and the Microsoft booth had one of the largest flatscreens I&rsquo;d ever seen. There were old arcade game cabinets and carnival-style games: I saw a massive Jenga set and gingerly took a piece out for the fun of it. I was accosted by someone shoving an iPad in my face and promising me a Starbucks gift card for filling out a survey on how I perceived their company&rsquo;s open source strategy (which I indeed did, if not more out of confusion than anything else).</p>
<p>Initially, I was seeking internship leads, but I quickly realized most of the folks were there to sell the product and would just redirect me to the careers page. Alas, I&rsquo;d wasted ten printed copies of my resume for nothing. In preparation for the event, I&rsquo;d also bought a <a href="https://nfcring.com/">NFC Ring</a> so I&rsquo;d be able to share my website and contact information in a unique and hopefully memorable way. No one asked me for my information at the event, so I didn&rsquo;t have the chance to show off my very sleek titanium business card ring<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<p>After killing time for a bit, I met up with the program manager for one of the experiences that&rsquo;s changed my life. (I&rsquo;m being vague in case they don&rsquo;t want to be identified) I&rsquo;d only ever met them online, and they introduced me to some of their colleagues at the event. I also met the former Director of Open Source at the company that hosted said impactful event, which was pretty fun. I didn&rsquo;t do too much networking besides this, though, which is something I perhaps regret not taking advantage of.</p>
<p>One thing the PgM mentioned to me was that conference food is always not great — they&rsquo;d gone out for dimsum for a work lunch instead of taking the catering. I found this to be unfortunately kind of true. For lunch, we had sandwiches + pasta salad + fruit + a cookie + a drink, but everything was pre-boxed. To be fair, it was very filling and wasn&rsquo;t <em>bad</em>, but I&rsquo;m automatically biased against untoasted sandwiches, especially cold ones. I think given the choice, I&rsquo;d have preferred dimsum too<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<p>After lunch, I spent a few hours walking around in circles around the garden patio rehearsing my talk (and fidgeting with the light-up spinners I&rsquo;d grabbed from the sponsor showcase). My talk went alright. I covered the content in <a href="https://kewbi.sh/blog/posts/240526/">a previous post</a>, but it was great seeing it resonate with people and I do genuinely feel that they got something from it. There weren&rsquo;t a lot of people, since I was the last talk session and people were either tapped out, heading to external events, or grabbing cocktails upstairs at happy hour. I got good questions, though, and sparked some follow-up discussion points for future research. Talking to a mostly empty room is just as terrifying as a full one! I think I fumbled a lot in the beginning but found my stride halfway through. I forgot to repeat the questions into the mic, I missed some of my points, and I didn&rsquo;t get to share with as many people as I&rsquo;d hoped, but I really enjoyed the experience and found it very fulfilling.</p>
<p>I walked out post-talk feeling energized, relieved, and ready to nab as much swag as possible from the happy hour upstairs. With my heavy backpacks (besides my personal one, the PgM gave me another one with some swag), I triumphantly made my way up the escalators while Dua Lipa was blaring on the speakers. On my ride up, the mix of catchy pop – I think it was Don&rsquo;t Start Now – and much less stress, not to mention the beautiful afternoon sun, made me feel like I was on top of the world. The boost of confidence made me briefly consider trying to sneak a beer from the drink cart (purely for the comedy, I don&rsquo;t drink), but System 2 clicked in and I got a mock margarita instead.</p>
<p>Newly hydrated, I hit the sponsor showcase in search of swag. I&rsquo;d already gotten this custom OSSNA 2024 picnic blanket at checkin, so I&rsquo;d been lugging that as well as both my personal bag and a gifted backpack around the whole day. But after the happy hour, my backpack, both my pockets, and even the bag I was using to carry the gifted backpack were full with random knickknacks. I got a red RedHat hat. I got multiple fidget spinners (some of which lit up and flashed). I got a yoyo for my dad. I got a tote bag. I got a totebag to hold the totebag. I put one of my backpacks into the totebag that was holding the other totebag. I stopped for some appetizers and was eating outside the main hall, when I noticed an abandoned white cardboard cube. Out of curiosity, I snuck a look at it, and it turned out to be a GitHub mug. I returned it to the booth and asked if I could keep it, and learned that they&rsquo;d been giving away as a sweepstakes prize, which unfortunately I didn&rsquo;t win any of. A shame — those Lego flower sets would have been cool. At the end of the day, my arms were killing me. I saw someone with one of those grocery shopping wheelie carts full to the brim with tshirts and merch, and I was kicking myself that I hadn&rsquo;t thought of that. I went to Cheesecake Factory for dinner with my parents (a childhood favourite) and was showing off my swag. They were already impressed with what I&rsquo;d had in my bag before I&rsquo;d even started emptying my pockets, which were full of stickers, small toys, and a stuffed chameleon (thanks SUSE!). I was lucky we were driving back and had plenty of room in the car for the extra backpack — I can&rsquo;t imagine having to fly back with limited luggage.</p>
<p>The next day, I got up bright and early to make the free breakfast, which was better than the lunch but was still lacking toasted bagels. After breakfast, I stuck around for Linus Torvald&rsquo;s keynote talk — I didn&rsquo;t really take away much from it because I was a little busy fangirling. To this day, I still couldn&rsquo;t tell if the conversation was scripted or not, but it was really cool seeing one of the key figures in open source in real life and not just as some revered email handle. I did some French flashcards after the keynote, since my exam was in less than twelve hours, while I was waiting for the showcase to open up again. Didn&rsquo;t you have an exam to get to, you ask? This was very true, but my dad had seen someone with a <a href="https://en.wikipedia.org/wiki/Zephyr_(operating_system)">Zephyr</a> kite the day before and he was dead set on coming home with one. Thus, I was sent into the showcase right when it opened to listen to the spiel in exchange for a kite. When I got back into the car and we set out, my dad asked, very melancholically, why I didn&rsquo;t get two.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Unfortunately, I wasn&rsquo;t able to move my second exam, so right after his keynote, we headed back to Vancouver. We made it with a few hours to spare before my second final, but I hadn&rsquo;t really slept well so wasn&rsquo;t able to cram much. I made myself a shot of matcha, spedran a little more Anki, and prayed that the studying I&rsquo;d put in throughout the rest of the term would come in handy then.</p>
<p>Here are some brief lessons learned:</p>
<ul>
<li>Leave more time for attending other talks, instead of stressing about your own. Plan out the talks you&rsquo;ll attend in advance and look for topics far outside of your comfort zone, since conferences are a good way to find yourself in close proximity to experts in diverse fields.</li>
<li>Your talk will be okay, even if you feel that you underprepared. This might be a uniquely me thing — I&rsquo;ve been realizing I can be more cursory (for my definition of cursory) than I expect and still surprise myself.</li>
<li>Obviously, prepare earlier. Ask for feedback earlier, so you can go through multiple rounds of it and don&rsquo;t feel so overwhelmed by pages of notes in the days leading up to the conference. This also gives you the opportunity to be more comfortable during the first time you rehearse for feedback, so you can focus on content comments instead of criticism about missing your cues or whatnot.</li>
<li>Look for scholarships or reimbursements from the conference, your school, or your/external companies to cover costs. I was able to drive down to Seattle, avoiding flight costs, but the Linux Foundation also has grants available that covered my hotel costs. I was able to also get a scholarship from my university to cover the gas and miscellaneous fees.</li>
<li>Ask professors for help when it&rsquo;s needed. You&rsquo;re not alone, and even if it feels embarrassing to be frantically trying to prep days ahead of the talk, they can offer a more mature, experienced outlook.</li>
<li>Strategize how you&rsquo;ll get swag. Keep your backpack as empty as possible. Try to bring a tote bag and figure out how to binpack your swag based on your weight and swag size limits.</li>
</ul>
<p>Giving a talk is something I&rsquo;d certainly do again! I&rsquo;ve already applied for a few other CFPs with this same talk or other interesting things I&rsquo;d like to speak about. It&rsquo;s given me an opportunity to learn how to present myself and my work and start establishing myself as some sort of respectable resource while also meeting others and exploring a new venue/city.</p>
<p>Thank you again to the Linux Foundation for making this incredible trip and talk possible. Many thanks also to my parents for taking time off to drive me and support me day-of. And of course, thanks to my math prof for letting me move my exam and make it to give the talk that&rsquo;s left me feeling so energized, bold, and excited.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>I did forsee this, though, and to be honest I kind of just wanted the ring. I have plans for it in the future!&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Or Din Tai Fung, which I hear a lot about online and was just a few blocks away. My parents had been wanting to go, but they got lost — they&rsquo;d found the block but didn&rsquo;t look up to see the sign, and it was raining and they were hungry, so they went someplace else. They really should have used Google Maps and just asked me for an umbrella, since they were giving so many free ones away at the convention center.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>The GitHub Graph</title>
      <link>https://kewbi.sh/blog/posts/240526/</link>
      <pubDate>26 May 2024</pubDate>
      <author>Emilie Ma (Kewbish)</author>
      <description>On GitHub Issues, PRs, and all the links in between.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>In mid-April, I had the opportunity to speak at <a href="https://ossna2024.sched.com/">Open Source Summit North America 2024</a>, held in Seattle. I spoke about research that I&rsquo;ve been working with Professors <a href="https://dwyoon.com/">Dongwook Yoon</a>, <a href="https://www.cs.ubc.ca/~bestchai/">Ivan Beschastnikh</a>, and <a href="https://scholar.google.ca/citations?user=5kncGscAAAAJ&amp;hl=en">Cleidson de Souza</a> on: characterizing open-source collaboration patterns from the GitHub Issues and PRs that they comprise. I was nervous attending my first real conference, let alone speaking, but it was an amazing first experience. Even though I was only there for a day<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, it was an incredibly fulfilling one, and I&rsquo;m grateful to the organizing committee for making the conference a welcoming, engaging success.</p>
<p>I&rsquo;ll touch on more of my conference experience and some backstory (because oh boy, is there a backstory) in another post, but I wanted to first share my talk in post format. This article will be a transcript-style rehash of my talk based on my speaker notes, but if you&rsquo;d prefer to watch my talk rather than read, I&rsquo;ve also listed it here. The raw slides are available <a href="https;//emilie.ma/ossna2024">here</a>.</p>


<div style="display: flex; justify-content: center; margin-bottom: 1em">
	<iframe width="560" height="315" src="https://www.youtube.com/embed/iUZaW_cjwYU" title="The GitHub Graph: Characterizing Open-Source Collaboration Patterns - Emilie Ma" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div>


<h2 id="talk-intro">Talk Intro</h2>
<figure><img src="/img/240526/1.png"/>
</figure>

<p>Hi friends! I&rsquo;m Emilie, an undergrad at the University of British Columbia and software developer. I&rsquo;ve recently had the privilege of working with Professors Cleidson de Souza, Dongwook Yoon, and Ivan Beschastnikh at UBC on the topic of understanding open-source development practices empirically, in the real world. We found that there&rsquo;s a lot of hidden context and unspoken patterns that GitHub and other development platforms don&rsquo;t highlight, and today, I&rsquo;m going to show you how we revealed them. We analyzed over fifty large open-source projects hosted on GitHub and developed a novel graph-based perspective that we&rsquo;re calling the PR-Issue Graph.</p>
<figure><img src="/img/240526/2.png"/>
</figure>

<p>Today, I&rsquo;ll cover our methodology, this PR-Issue graph&rsquo;s attributes, and our workflow type definitions. You&rsquo;ll walk away with a new outlook on managing open-source collaboration, able to recognize common workflow patterns and what they might mean for your community. I hope this talk will spark some discussion and brainstorming over how we can be mindful of these workflow types as we lead open source development efforts.</p>
<h2 id="about-github">About GitHub</h2>
<figure><img src="/img/240526/3.png"/>
</figure>

<p>How many of you are familiar with GitHub? It&rsquo;s a developer platform built on top of the version control software Git. You&rsquo;ve probably already heard of it in some of the previous talks. GitHub is a popular choice among open source projects as a central platform for collaboration.</p>
<figure><img src="/img/240526/4.png"/>
</figure>

<p>On GitHub, community bug reports, feature requests, or general discussion about projects are done on issues. On the slide are a couple examples of projects we&rsquo;ve studied. I&rsquo;d like to highlight that issues have a designated status. You can see that on the left, the issue is open, which usually means it hasn&rsquo;t been addressed; and on the right, it&rsquo;s closed, meaning someone has fixed the issue or the discussion has been deemed off-topic or closed.</p>
<figure><img src="/img/240526/5.png"/>
</figure>

<p>New code, perhaps addressing these Issues, is merged into projects via Pull Requests, or PRs. They also have statuses: open, if it&rsquo;s undergoing or will undergo review; closed, if the work was unsatisfactory or extraneous; and merged, if it&rsquo;s been accepted and integrated into the project. People discuss and review code in PRs. As well, folks can link to Issues in a PR: for example, if a PR fixes an issue that&rsquo;s already been reported, contributors will typically link to the issue within their PR to create an explicit connection between the two.</p>
<figure><img src="/img/240526/6.png"/>
</figure>

<p>Here, that link has an explicit &lsquo;fixes&rsquo; type. GitHub will automatically create two types of links for you: &lsquo;fixes&rsquo; or &lsquo;duplicate&rsquo;. If you mention a keyword associated with &lsquo;fixes&rsquo; or &lsquo;duplicate&rsquo; with an issue number, GitHub will create a link with the appropriate type. Here, the link was created with the keyword &lsquo;closes&rsquo;, which is associated with the &lsquo;fixes type&rsquo;.</p>
<figure><img src="/img/240526/7.png"/>
</figure>

<p>If we frame this as a collaboration graph for a second, we start to see something basic but interesting emerge. Let&rsquo;s consider the PRs and issues as nodes, and the links between them as edges. We can model the different node types and statuses: so here, purple means merged, green means open, and red means closed, just like in the GitHub UI. This is a graph-based visualization of the example I&rsquo;ve been showing: we have a problem (Issue 8150, the red square), and we have a solution (PR 8156, the purple circle), the PR fixes the Issue, and there&rsquo;s nothing else connected to this work.</p>
<figure><img src="/img/240526/8.png"/>
</figure>

<p>Here&rsquo;s another example of this graph based perspective, this time with an explicit &lsquo;duplicate&rsquo; link taken from a project called discord.py. Here&rsquo;s a visualization of what that looks like. The problem reported in this issue on the left was marked as a duplicate of this other issue on the right. The issue on the right was created earlier and covers the same problem of a missing field.</p>
<figure><img src="/img/240526/9.png"/>
</figure>

<p>Prior work in software engineering research has focused on these links extensively. While the examples I&rsquo;ve shown have explicit link types that are detectable and supported by GitHub, about 85% of links on GitHub don&rsquo;t have a &lsquo;fixes&rsquo; or &lsquo;duplicate&rsquo; relationship, according to <a href="https://www.cs.ubc.ca/~bestchai/papers/cscw21-pr-references.pdf">Chopra et al</a>. Researchers have been primarily trying to classify these blank links into richer categories than just &lsquo;fixes&rsquo; or &lsquo;duplicates&rsquo;, and many papers have built several distinct taxonomies for doing so, including link types like &lsquo;dependent&rsquo;, or &rsquo;enhanced&rsquo;. Some work has also been done to do this automatically, with NLP classifiers.</p>
<figure><img src="/img/240526/10.png"/>
</figure>

<p>However, let&rsquo;s take this idea one step further. We were interested in &lsquo;zooming out&rsquo; of the prior, very focused research on individual links. Our research is the first to look at multi-node clusters of issues and PRs as a graph. This graph perspective grants us more context, as we simply have more metadata and relationships to examine, which in turn gives us a richer understanding of the work practices associated with each type of graph. For example, with more context, we can understand larger implications about the development work practices in a project, like if developers break their work down into small chunks for review, or if there&rsquo;s a lot of competition in certain areas of the project. This graph perspective is what we&rsquo;ve nicknamed GitHub&rsquo;s PR-Issue Graph, and for the rest of this talk, I&rsquo;ll show you what lurking insights about open-source collaboration we&rsquo;ve found hidden in it.</p>
<h2 id="methodology">Methodology</h2>
<figure><img src="/img/240526/11.png"/>
</figure>

<p>You might be wondering how we did all this analysis. We chose to analyze GitHub as our platform of choice because of its popularity, its data availability, and the diversity of projects hosted on it. We initially scraped 56 popular projects on GitHub, based on a prior sample by <a href="https://www.cs.ubc.ca/~bestchai/papers/cscw21-pr-references.pdf">Chopra et al</a>. These cover a variety of topics, like machine learning with projects like mlflow and foundational technologies like gRPC. We downloaded the contents, metadata, and links of more than ninety thousand nodes. We used a diversity sampling technique to generate a sample of sixty clusters, or subgraphs, in which each link was manually coded into a list of extended link types. From there, we identified some repeated structural patterns that we thought could represent work practices in software development.</p>
<figure><img src="/img/240526/12.png"/>
</figure>

<p>Next, we imported the issues and PRs and their metadata into <a href="https://neo4j.com/">Neo4j</a>, a graph database software, and created reusable queries to search for all occurrences of these collaboration patterns. Onscreen, you can see an image of the built-in query browser visualizing some results.</p>
<figure><img src="/img/240526/13.png"/>
</figure>

<p>We also developed an image visualization module and interactive explorer tool that we used to streamline manual coding and validate our findings during interviews with open source developers. Here&rsquo;s what this looked like.</p>
<h2 id="project-characterization">Project Characterization</h2>
<figure><img src="/img/240526/14.png"/>
</figure>

<p>Let&rsquo;s start with some characterization of the projects we studied. We found all the connected components, or clusters of connected issues or PRs, and measured their sizes and frequencies, which is shown in this bar chart. The x-axis here shows component size, and the y-axis show the number of clusters of that size on a log scale.</p>
<p>Cluster sizes are important, as they give us an immediate intuitive idea of how collaboration is normally structured: we can understand if people tend to build on previous work, making larger clusters of connected nodes, or if they create one-off PRs or Issues.</p>
<p>We found that cluster sizes followed a power law distribution, with many isolated nodes not connected to anything else and a few very large components with lots of nodes and interlinking. We found that there are ten times as many isolated nodes than 2-node components, which is unusual. You might think with how GitHub is structured around collaboration and documentation, there&rsquo;d be a lot of PR to Issue links, or problem-solution pairs. Instead, we observed a lot of distinct problems or solutions that don&rsquo;t apply to or aren&rsquo;t linked to each other.</p>
<p>This power law distribution can help maintainers predict growth and appropriately allocate resources: power law distributions often indicate the presence of a few highly connected and influential hubs of nodes, so when we observe larger clusters starting to form, we can know, oh, okay, it&rsquo;s likely that work and collaboration will continue to grow.</p>
<figure><img src="/img/240526/15.png"/>
</figure>

<p>We also hypothesized that connected components of different sizes represented different types of work. We found that small clusters of nodes had more issues (shown in blue), which represents that they have more problems, fewer closed issues or merged PRs, so the work is left more unfinished, and a shorter duration between first and last update, indicating small clusters of nodes tend to represent isolated problems.</p>
<p>On the other hand, large connected components had fewer issues with more merged PRs and a longer duration. We might think of large connected components as more mature and active initiatives in a project, whereas isolated PRs or issues might represent one-off contributions.</p>
<p>Again, this can help with prioritization: larger clusters represent these critical areas where lots of high-impact collaboration is happening, so it&rsquo;s worth taking a look to see how those issues are being managed. This is also useful for encouraging diverse contributions: developers new to open-source might just want to tackle a one-off issue, so smaller clusters or isolated issues might be a good place to start. On the other hand, developers looking to tackle a challenging core problem might want to start looking at the open issues in one of these large clusters or use them as reference documentation for past design decisions.</p>
<h2 id="workflow-type-definitions">Workflow Type Definitions</h2>
<figure><img src="/img/240526/16.png"/>
</figure>

<p>Next, I&rsquo;ll cover the cornerstone of our work: our workflow type definitions. Let&rsquo;s start by taking a look at one of these clusters of issues and PRs. Here&rsquo;s an example from the <a href="https://github.com/Rapptz/discord.py/">discord.py</a> project, a library used to build chatbots and other automations for the app Discord. Here, someone is requesting a feature — something to do with adding <code>invoke_parent</code>. Then, two folks have come along to create their own implementations of this feature. sudosnok on the left proposed an approach, but it was vetoed by the maintainer of the project. A few days later, SebbyLaw submits their own implementation, which is eventually merged. Note that both PRs reference the original issue with those &ldquo;fixes&rdquo; keywords I mentioned before: sudosnok used &ldquo;closes&rdquo;, and SebbyLaw uses &ldquo;resolves&rdquo;. Let&rsquo;s visualize this as a graph. This is actually an example of one of our workflow types: the competing PRs workflow type. The competing PRs workflow type abstracts this example a bit by allowing for more competition, and more of these closed PRs.</p>
<figure><img src="/img/240526/17.png"/>
</figure>

<p>These workflow types are associated with work practices, like &lsquo;breaking down complex work into small chunks&rsquo;. Each workflow type also has a prototypical graph structure, as shown here. These graph structures were combinations of four types of metadata constraints: node type, status, authorship, and creation timestamps. We&rsquo;ve defined nine prototypical examples of workflow types: I&rsquo;ll go through a couple today.</p>
<figure><img src="/img/240526/18.png"/>
</figure>

<figure><img src="/img/240526/19.png"/>
</figure>

<p>Sometimes, contributors are eager to offer their own implementation of a task, and begin working on one while not communicating, which we observed in our competing PRs workflow type. The discord.py example I just showed earlier was one of these. It has a graph structure of a closed issue, this red square, connected to multiple PRs, with only one PR being merged, this purple circle. These PRs are created by different people, represented by the little people icons labelled A and B.</p>
<p>As in the discord example, you might have a feature request that multiple contributors start working on in parallel without &lsquo;claiming&rsquo; the issue or otherwise communicating, and now you have multiple candidate implementations to review. Its associated work practice, or in this case, malpractice, has an implication that there was some wasted work due to poor communication, although competing PRs also allowed projects to be picky about what they merged. That discord.py example showed how the maintainer called out a performance implication in the first PR that they didn&rsquo;t like.</p>
<figure><img src="/img/240526/20.png"/>
</figure>

<p>Another painfully familiar issue in open source is duplicate issues, where contributors report similar problems that are duplicates of previous discussions. Redirecting them to other, established discussions, takes valuable maintainer time. This is captured in our duplicate issue hub workflow type. Its structure consists of a issue, this leftmost red square, connected via &lsquo;duplicate&rsquo; links to many other issues, the red squares on the right. The <code>t=0</code> notation means that this hub issue, Issue 1, was created before the other duplicate issues on the right.</p>
<p>This happens if a bug goes out in a release, for example, many folks might report it at the same time without first searching through the other issues posted to GitHub, causing additional maintenance burden. Though this was one of the workflow types we first thought of and observed in our query refinement process, it turns out that duplicate hubs are actually rather infrequent. We observed only 15 instances of them, over all 90K nodes.</p>
<p>However, I&rsquo;ll note that when issues are highly noticeable, as with breaking changes, these duplicate issue hubs can grow quite large. Here&rsquo;s another example from the discord.py project: issue 5867 caused by a breaking change on the underlying app&rsquo;s end was connected to 11 duplicate issues. When maintainers notice high-growth or large duplicate issue hubs, it&rsquo;s worth considering how the change can be better communicated and made noticeable for users.</p>
<figure><img src="/img/240526/21.png"/>
</figure>

<p>To perhaps combat some of the miscommunication that cause duplicate issue hubs, projects also tend to publish updates about recent work. Here&rsquo;s an example from the <a href="https://github.com/apache/dubbo/">apache/dubbo</a> project, which employs a bot to create weekly Issues as a status report. It lists all the merged PRs from the last week. We observed this in our Integrating PR/Issue hub workflow type: its structure is a central PR or issue, as shown on the left, linked to many merged PRs, so those purple circles you see on the right.</p>
<p>The dubbo project used issues to collect reports, but some projects also use a PR: for example, listing all the PRs merged into the latest release candidate before merging that release candidate PR into the mainline branch.</p>
<p>This workflow type has a temporal constraint: the central PR tends to be created <em>after</em> the merged PRs. In practice, this typically models a release or documentation workflow, where maintainers aim to surface work after the fact so the community is more aware of what initiatives are occurring.</p>
<figure><img src="/img/240526/22.png"/>
</figure>

<p><a href="https://kewbi.sh/blog/posts/230611/">Stacked PRs</a> and <a href="https://graphite.dev/">Graphite.dev</a> are becoming more popular recently: if you&rsquo;re not familiar with Stacked PRs, they&rsquo;re a workflow for creating small, incremental PRs that build on each other and that facilitate code review and dev velocity. For example, I might make a small PR to update the backend, then a small PR to update the frontend, then a small PR to update the docs.</p>
<p>Our Dependent PRs workflow type models this type of workflow. Its graph structure is a series of PRs referencing one another in a chain, with a limited set of authors, usually just one. This workflow type is regarded as a good practice as it makes changes easier to understand and ship without blocking implementation work on a review.</p>
<figure><img src="/img/240526/23.png"/>
</figure>

<p>Finally, we found that developers tend to sometimes overdeliver in their PRs, addressing multiple issues at once. Here&rsquo;s an example of a PR taken from the <a href="https://github.com/grpc/grpc-web">grpc-web</a> project. A single PR fixes many issues that are, at first glance, unrelated. PRs like these will be more difficult to review and give feedback for, since the fixes are all mixed up in each other. This workflow is captured in our Divergent PR workflow type. It has the structure of a merged PR, this purple circle, that fixes several linked issues at once, these red squares.</p>
<p>This can be both positive, as when the linked issues are related or have the same underlying fix, or negative, as in the grpc-web example when the issues are unrelated. Again, negative divergent PRs violate the general principle of small, easily-understood PRs addressing a single issue.</p>
<h2 id="workflow-type-comparison">Workflow Type Comparison</h2>
<figure><img src="/img/240526/24.png"/>
</figure>

<p>Let&rsquo;s now talk about some ways these patterns compared to one another. We wanted to understand how frequently each workflow type arose in projects as it would explain the most common ways engineers work. We ran each of our queries across all 90K nodes, and counted up the number of matches from each workflow type. We found over a thousand matches of our workflow type definitions.</p>
<p>What&rsquo;s more was that we found that these patterns weren&rsquo;t evenly represented across projects: some workflow types were more frequently used, and we hypothesize that they&rsquo;re more natural or embedded in software development culture. For example, the Consequent Issue-PR workflow type we identified was very popular : that&rsquo;s an issue solved by a PR, which creates another issue. It represents the typical pull-based development model of Issue-PR well, so it wasn&rsquo;t surprising that that was the most frequent workflow type. Another example: we didn&rsquo;t see a lot of competing PRs, which implies that there&rsquo;s usually limited wasted work.</p>
<p>This might be useful to keep in the back of your head as a benchmark: if your project has many more duplicate issue hubs or competing PRs than what we observed, that might be a sign to re-evaluate your issue reporting or code review workflows.</p>
<figure><img src="/img/240526/25.png"/>
</figure>

<p>Finally, we observed that workflow types are fairly representative of the projects. 52% of all nodes that could have been in a workflow type – so, in a cluster with some other nodes and some links, were in a workflow type. Most projects contained at least one match, with larger projects tending to have more matches. This is a graph of the number of nodes in projects to the number of workflow types we observed in said project, with each dot representing a project, and you can see this general upwards correlation. You can see the largest projects, like <a href="https://github.com/App-vNext/Polly">App-vNext/Polly</a> and <a href="https://github.com/apache/dubbo/">Apache Dubbo</a>, had the most workflow type matches, with about 150 matches each.</p>
<p>To us, this indicated that there&rsquo;s some connection between project maturity leading to more organized collaboration and higher usage of these structured workflow types. This was consistent with the fact that the projects that used no workflow types, like <a href="https://github.com/roboguice/roboguice">roboguice</a> or <a href="https://github.com/go-chi/chi">go-chi</a>, were all relatively small, with only a couple hundred contributions. If your project has a very high number of workflow types, this represents that your project tends to have highly structured collaboration, which can be good.</p>
<figure><img src="/img/240526/26.png"/>
</figure>

<p>These were our takeaways from examining workflow type matches. But we were also curious what open source developers — you all — thought. We validated our definition and visualization tools with open-source developers in a series of six interviews. They focused on introducing developers to the interactive explorer tool preloaded with their project.</p>
<figure><img src="/img/240526/27.png"/>
</figure>

<p>All developers agreed workflow types would help make better decisions on aspects of the development process. Some examples taken from our interviews: someone noted duplicate issue hubs might indicate a need for change in the documentation or bug reporting process, another person came up with the unique idea that workflow types can help prioritize and direct maintainership. If you see a competing PRs cluster, for example, but you see one of the PRs is part of a divergent PR cluster that resolves multiple issues, you might want to review that solution first. Our interviewees all noted that the visualization tools were useful for understanding inter-dependencies between features, providing valuable context that GitHub doesn&rsquo;t.</p>
<p>Finally, some interviewees rightfully highlighted that workflow types are limited because they don&rsquo;t examine the content of the issue, but we hope the visualization tool making even the initial surfacing of collaboration patterns easier is a first step.</p>
<h2 id="implications">Implications</h2>
<figure><img src="/img/240526/28.png"/>
</figure>

<p>Our work has wide-ranging implications. First, we&rsquo;ve seen how examining the sizes of clusters of linked issues and PRs can demonstrate the type of work it contains. Because clusters tend to be either very small or grow to be very large, this can predict growth in certain areas of the project and in turn inform resource allocation and prioritization.</p>
<p>As well, we&rsquo;ve seen that the visualizations of the graph perspectives can help navigate between initiatives and surface issues or PRs where additional effort is high impact, something our interviewed developers noted. The WorkflowsExplorer tool can be used to figure out which PR among a competing PRs workflow type is best to review, as I brought up before, and to visualize dependencies between features in development.</p>
<p>The tool can also help visually find problem areas in a project, where there are outsize numbers of competing PRs, duplicate issue hubs, or other forms of wasted work.</p>
<p><a href="https://dl.acm.org/doi/10.1145/3338906.3338949">Hirao et al.</a> argued that code review and duplicate issue identification tools can be improved by closer inspection of the links between nodes, among several other factors. Our approach does just that, and even extends it by analyzing multiple links within a cluster of nodes. Another implication of our graph perspective is that it&rsquo;ll allow us to further improve these tools: we can use our insights on cluster sizes to improve good-first-issue detection.</p>
<p>But the biggest next step for our work is talking more with developers like you. I&rsquo;ve already spoken about some of the developer interviews we&rsquo;ve conducted, but if you&rsquo;d like to work with us on understanding the collaboration patterns in your project, please get in touch or find me after this talk!</p>
<h2 id="talk-conclusion">Talk Conclusion</h2>
<figure><img src="/img/240526/29.png"/>
</figure>

<p>Today, I&rsquo;ve covered our novel graph-based perspective and our ideas for the workflow types definitions. The core plus of our approach is that it can be easily automated, making it easier for maintainers to identify and curb unwanted types of collaboration. As well, I&rsquo;ve highlighted some insights that this graph perspective has revealed and how we can make use of them to steer your communities in a better direction. Think of this PR-Issue graph as a Grafana dashboard that lets you monitor your project&rsquo;s collaboration health: it acts as a global point of reference for understanding your project as a whole and can give you early alerts when things might be going south. Again, reach out if you&rsquo;d like to work together with us!</p>
<hr>
<p>Thank you so much to the Linux Foundation for organizing the conference and making my first speaking experience such a memorable one. I thoroughly enjoyed the conference and am looking forward to speaking at others in the future — it&rsquo;s one of my goals for 2024!</p>
<p>Keep an eye out for the next post in this series where I&rsquo;ll cover more about the day-of experience, my hectic scheduling that I somehow pulled off, and lessons learned both from speaking and attending.</p>


<style>
	img {
		margin-bottom: 1rem;
	}
</style>


<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>The conference was smack in the middle of my first few finals — I even had to beg a professor to reschedule my first one — and I needed to rush back to Vancouver after my talk to make my second exam, so I was only able to stick around for one day.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
