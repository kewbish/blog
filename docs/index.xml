<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Yours, Kewbish - a collection of articles on tech and thought.</title>
    <link>https://kewbi.sh/blog/</link>
    <description>Latest Yours, Kewbish posts</description>
    <managingEditor></managingEditor>
    
	<atom:link href="https://kewbi.sh/blog/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>An Advent CAPlendar</title>
      <link>https://kewbi.sh/blog/posts/251201/</link>
      <pubDate>01 Dec 2025</pubDate>
      
      <description>On my Christmas-y systems-y countdown.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>For a couple years in high school, I dedicated my Decembers to Wastl&rsquo;s <a href="https://adventofcode.com/">Advent of Code</a>, a daily series of Christmas-themed programming puzzles. The regular cadence of problems kept the completionist in me hooked, and the problems were satisfyingly challenging beyond my ability. I lacked the background to make it through most days after the first week or two on my own, so I&rsquo;d sheepishly poke around the subreddit the next morning for hints. Through AOC, I learned a lot of Python builtins like counters, zips, and list comprehensions, and it was always a fun activity to look forward to during December.</p>
<p>Last year, I found out about a similar project called <a href="https://aods.cryingpotato.com/">Advent of Distributed Systems</a>. AODS breaks up the <a href="https://fly.io/dist-sys/">Fly.io Distributed Systems</a> challenges into weekly miniprojects, aiming to get you through to the final Raft KV store by the end of the month.</p>
<p>This fall, I realized I had a growing paper backlog and a bucket list of startups and technologies I&rsquo;d like to get around to reading someday, but never made the time for. Inspired by the subject matter of AODS and the cadence of AOC, I&rsquo;m putting together my own holiday countdown.</p>
<p>This post is my 2025 Advent CAPlendar, exploring topics in distributed systems (hence the <a href="https://en.wikipedia.org/wiki/CAP_theorem">title pun</a>), databases, formal verification, and security. I&rsquo;m planning to read some foundational papers like DynamoDB and Zookeeper as well as some newer works out of <a href="https://pdos.csail.mit.edu/">MIT PDOS</a> and <a href="https://sky.cs.berkeley.edu/">Berkeley SkyLab</a>. There are also some neat systems startups and protocols that I keep hearing about, so I&rsquo;m also planning to glean what I can about how they work from their documentation. You can see a calendar of topics below<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>


<p style="font-size: 30px;font-family:'Playfair Display';text-align:center;font-style:italic">2025 Advent CAPlendar</p>
<div class="flex" style="border-radius: 8px;background-color: var(--sub-colour);margin-bottom:1rem">
<div id="calendar-grid">
<div class="date-blocker"></div>
</div>
</div>

<style>
#calendar-grid {
display: grid;
grid-template-columns: repeat(7, minmax(0, 1fr));
grid-template-rows: repeat(4, minmax(0, 1fr));
gap: 8px;
padding: 12px;
}
.date-blocker {
opacity: 0;
}
.date {
grid-column: auto !important;
font-size: 18px;
padding: 0.5rem;
position: relative;
cursor: pointer;
 flex: 1;
  min-height: 0;
  overflow: auto;
  align-self: stretch;
}
@media (max-width: 1500px) {
.date {
font-size: 16px;
}
}
@media (max-width: 1370px) {
.date-blocker {
display: none;
}
#calendar-grid {
grid-template-columns: 1fr;
grid-template-rows: repeat(auto-fit, minmax(0, 1fr));
}
.date {
font-size: 18px;
}
}
.date p {
line-height: normal !important;
margin-bottom: auto !important;
text-indent: 0 !important;
}
.date > .date-number{
position: absolute;
top: .5rem;
right: .5rem;
font-size: 14px;
}
.date-mystery {
background: hsl(from var(--dark-accent-colour) h s l / 0.5);
color: var(--muted-accent-colour);
cursor: not-allowed !important;
}
a.date {
display: block;
}
.date-wrapper {
  display: flex;
  flex-direction: column;
  min-height: 0;
  a {
	display: flex;
  flex-direction: column;
  flex: 1;
  }
}
form {
display: flex;
gap: .5em;
width: 100%;
}
#email:disabled, #email-submit:disabled {
cursor: not-allowed;
}
</style>


<p>I&rsquo;m also trying something new and starting a <a href="https://craigmod.com/essays/popup_newsletters/">popup newsletter</a> to accompany this series. Signing up will get you a short weekend roundup list of the posts that week. To assuage the privacy-conscious: I&rsquo;ll only use this for Advent CAPlendar-related posts, I&rsquo;ll be running this by hand (no external data processors, etc.), and I promise I&rsquo;ll be too busy studying for finals to think about spamming you otherwise!</p>


<form id="email-form"
>
  <input type="email" name="email" required id="email" />
  <button type="submit" id="email-submit">Sign Up</button>
</form>

<h2 id="day-1-CAP-and-CAL-Theorems">1. The CAP and CAL Theorems</h2>
<p>I figured it&rsquo;d be most fitting to start this calendar by referencing the titular pun itself. The <a href="https://en.wikipedia.org/wiki/CAP_theorem">CAP theorem</a> is idea that a distributed system can only satisfy two of the following three guarantees:</p>
<ul>
<li><strong>C</strong>onsistency: reads return the most recently written value. This behaviour is also known as the <a href="https://en.wikipedia.org/wiki/Linearizability">linearizability</a> consistency level.</li>
<li><strong>A</strong>vailability: every request by a non-failing node receives a response. Note that this doesn&rsquo;t mean every request receives a response, which I think is the typical definition of uptime via high availability; it&rsquo;s predicated here on being received by a non-failing node.</li>
<li><strong>P</strong>artition tolerance: the system keeps behaving as-is even as arbitrary nodes fail.</li>
</ul>
<p>The idea is that if you have a network partition, you can decide to either eschew consistency and just return responses, even if they&rsquo;re not the latest values, or stick to your guns and not respond to requests, sacrificing availability.</p>
<p>The CAP theorem got <a href="https://arxiv.org/abs/1509.05393">picked on</a> <a href="https://blog.dtornow.com/the-cap-theorem.-the-bad-the-bad-the-ugly/">a bunch</a>. The most obvious argument is that the pithy &ldquo;pick 2 out of 3&rdquo; framing conveniently sidesteps that you can&rsquo;t ignore partition tolerance, or that you might end up with less than two properties. As well, if your system doesn&rsquo;t fit the definitions of consistency (e.g. isn&rsquo;t linearizable), the CAP theorem shrugs its shoulders and gives up — it won&rsquo;t really tell you anything. <a href="https://arxiv.org/abs/1509.05393">Martin Kleppmann&rsquo;s paper</a> also highlights some vagueness in definitions in the original CAP proof. This is summarized in <a href="https://blog.dtornow.com/the-cap-theorem.-the-bad-the-bad-the-ugly/">Dominik Tornow&rsquo;s blog post</a>, which illustrates how the conjecture and proof are a bit like passing ships in the night given their definitions are a bit off.</p>
<p>The CAP theorem could be a good rule of thumb, but I think its variants that focus on latency instead are far more interesting. <a href="https://en.wikipedia.org/wiki/PACELC_design_principle">PACELC</a> is an extension of CAP, stating that under <strong>P</strong>artitioning, you have to choose between <strong>A</strong>vailability and <strong>C</strong>onsistency, <strong>E</strong>lse you&rsquo;ll have to make the tradeoff between <strong>L</strong>atency and <strong>C</strong>onsistency. This makes sense, since in the absence of partitions stronger consistency models will still impose more acknowledgements and latency.</p>
<p>Kleppmann&rsquo;s paper goes further, modelling availability in terms of operation latency to match more industry-standard intuitions of uptime SLAs. &ldquo;If a service can sustain low operation latency, even as network delay increases dramatically, it is more tolerant of network problems than a service whose latency increases.&rdquo; sounds easier to compare to real systems. He also defines some operation latency lower bounds in term of network latency at different consistency levels which clearly match intuition.</p>
<p>Almost a decade later, Edward Lee takes this even further, defining the <a href="https://arxiv.org/pdf/2109.07771">CAL theorem</a>. This trades off against <strong>C</strong>onsistency, <strong>A</strong>vailability, and network <strong>L</strong>atency, but at a more gradual level than the all-or-nothing assumptions of the CAP theorem. The CAL theorem subsumes the CAP theorem, which is defined as a special case. It&rsquo;s a pity this isn&rsquo;t more widely known, otherwise I&rsquo;d have a cleaner title for this Advent series.</p>
<p>I think the neatest part of Lee&rsquo;s paper is the <a href="https://github.com/lf-lang/lingua-franca/">Lingua Franca</a> framework he introduces. It&rsquo;s a coordination language for distributed, realtime software for cyber-physical systems like autonomous vehicles. The key feature I picked up on was the fault handlers that trigger when you hit some apparent latency bounds, in which you can specify callbacks and explicitly define whether you&rsquo;d like to relax your consistency or availability properties. If you choose to sacrifice availability, it provides centralized coordination primitives; if not it provides decentralized coordination with error bounds on clock synchronization to relax consistency. Lee has a video tutorial for how to do so <a href="https://www.youtube.com/watch?v=3lHmiWOedHM">here</a>. It looks like a nice abstraction layer to keep the CAL theorem top of mind while developing a distributed system.</p>
<p>I think the most valuable part of the CAP/CAL theorems for me is the quick gut check mental model. The CAP theorem appeals to instinctive sense. I also had the opportunity to attend Lee&rsquo;s Distinguished Lecture at UBC, and it was just as clearly laid out and intuitive. Writing this up has highlighted some of the caveats of each model, though, which I&rsquo;ll need to remember to keep in mind.</p>
<h2 id="day-2-FLP-Impossibility">2. I&rsquo;m FLPping Out</h2>
<p>The CAP theorem is one of the classic impossibility results in distributed systems, and back at <a href="https://kewbi.sh/blog/posts/241006/">DARE</a> last year I learned about another: <a href="https://groups.csail.mit.edu/tds/papers/Lynch/jacm85.pdf">the FLP</a>. Unlike the CAP theorem, FLP&rsquo;s initialism doesn&rsquo;t serve as a neat mnemonic: it stands for <a href="https://en.wikipedia.org/wiki/Michael_J._Fischer">Fischer</a>, <a href="https://en.wikipedia.org/wiki/Nancy_Lynch">Lynch</a>, and <a href="https://en.wikipedia.org/wiki/Mike_Paterson">Paterson</a>, each very storied researchers that I&rsquo;d recommend checking out if you have some time.</p>
<p>The FLP states that every asynchronous consensus algorithm may not terminate if there&rsquo;s at least one silently faulty process. Asynchronous in this case means that we can&rsquo;t bound the amount of time it takes to pass messages, and the paper also assumes no access to synchronized clocks (this would make things easier with timeouts.) Consensus means that all non-faulty nodes decide on the same value, like 1 in the set {0, 1}, for instance. If you rotate the FLP on its head a bit, it shows that it&rsquo;s impossible to determine if the system will never terminate and has failed or if latency&rsquo;s just high.</p>
<p>They model consensus via one big message buffer system, shared by all processes and requiring explicit send/receive notifications. They define a configuration as the set of processes running, their internal states, and this message buffer. A <em>reachable</em> config is a config with some series of messages being applied on top. They outlaw protocols that always decide either 0 or 1, and define a concept of &lsquo;partial correctness&rsquo; that means all configurations have exactly one decision value, which could be null, and some reachable configuration will eventually decide 0 or 1. I&rsquo;ll hand-wave a bit below so look into the paper if you feel unconvinced.</p>
<p>The proof proceeds as follows:</p>
<ul>
<li>Lemma 1: Given two configurations that have disjoint sets of processes, you can replay the events of one onto the other and vice versa and you&rsquo;ll end up with the same final configuration because the sets of processes doing work are entirely distinct.</li>
<li>Lemma 2: Any such protocol has an initial configuration that can eventually lead to a configuration that&rsquo;s decided 0 and a separate configuration that&rsquo;s decided 1 — this is called an initial, <em>bivalent</em> config. Bivalent means we can&rsquo;t decide on either 0 or 1, which is bad news.</li>
<li>Lemma 3: From a bivalent config C, applying an event to any configs reachable from C will still result in at least one bivalent config being reachable. You can see where this is going now.</li>
<li>Lemma 4: If we have to start with a bivalent config, and can only get to states where we still have some bivalent configs, we&rsquo;ll never get into a config that only has univalent configs that have decided on other values. This means there can&rsquo;t be such a protocol that always terminates.</li>
</ul>
<p>The paper also includes an example protocol that does achieve asynchronous terminating consensus if we assume a fixed number of live processes and that processes can&rsquo;t die and become faulty during consensus. This consensus protocol creates a clique of known live nodes, where each node uses information received from only the other clique members to deterministically decide. This <em>does</em> terminate but is significantly more constrained.</p>
<p>I like that in the conclusion they mention &ldquo;These results do not show that such problems cannot be “solved” in practice; rather, they point up the need for more refined models of distributed computing […]&rdquo;. It&rsquo;s quite a poetic takeaway. Something similar arose with the extensions to the CAP theorem, and indeed I can find plenty of recent work still iterating on modelling the problem and defining the impossibility boundary: <a href="https://arxiv.org/abs/cs/0206012">here</a>, <a href="https://arxiv.org/html/2507.10413v1">here</a>, and <a href="https://arxiv.org/html/2502.09116v2">here</a>, just to name a few.</p>
<p>The <a href="https://groups.csail.mit.edu/tds/papers/Lynch/jacm85.pdf">FLP paper itself</a> is very short and approachable, so I&rsquo;d recommend giving it a direct read! The definitions aren&rsquo;t too onerous to wade through and the contradiction structure follows quite intuitively. The paper is quite theoretical and technical, but I need to get better at understanding and eventually producing work like this to do more convincing research.</p>
<h2 id="day-3-Consistency-Models">3. Consistency Canvases</h2>
<p>In my first week of my internship at the Sorbonne, I was talking about consistency models with my mentor, who promptly whipped out an annotated, thick paper with crazy-looking diagrams illustrating the different consistency anomalies given certain models. He offhandedly said he could send it to me so I could give it a read later, but just the brief glimpse of the diagrams scared me off enough to never press the topic while I was in Paris. However, I was thinking about consistency again while brainstorming ideas for this calendar, and this certainly feels like a paper I&rsquo;d put off, so I asked him if he still remembered which paper it was. I was uncertain how to feel when the PDF landed in my inbox, but I wanted to keep an open mind.</p>
<p>The terrifying behemoth in question is Cerone et al&rsquo;s <a href="https://drops.dagstuhl.de/storage/00lipics/lipics-vol042-concur2015/LIPIcs.CONCUR.2015.58/LIPIcs.CONCUR.2015.58.pdf">A Framework for Transactional Consistency Models with Atomic Visibility</a>, which I was happy to learn was only 14 pages (not a behemoth at all!) It formally defines consistency models (read atomic, snapshot isolation, etc.) for transactional systems. Prior work on formalizing models was one-off and scattered, &ldquo;often tied to database internals&rdquo;. Cerone et al. present a more unified, general approach that can be used for higher-level reasoning about applications using said transactional systems.</p>
<p>They define six axioms:</p>
<ul>
<li><strong>INT</strong> (internal consistency): Within a transaction, sequential semantics are respected, so reads observe the most recently written value, avoiding unrepeatable reads.</li>
<li><strong>EXT</strong> (external consistency): A transaction observes the maximum value determined by the visibility and arbitration relations, avoiding dirty reads.</li>
<li><strong>TRANSVIS</strong> (transitive visibility): If a transaction T1 is visible to another, T2, and T2 is itself visible to another, T3, T1 should also be visible to T3.</li>
<li><strong>NOCONFLICT</strong> (no concurrent updates): Two concurrent transactions may not both modify the same object.</li>
<li><strong>PREFIX</strong> (prefix consistency): All transactions become visible in the same order defined by arbitration order; if T1 is observed by T2, T2 also observes all of T1&rsquo;s arbitration relation predecessors.</li>
<li><strong>TOTALVIS</strong> (total visibility/serializability): The visibility relation is totally ordered.</li>
</ul>
<p>The six consistency models they define all satisfy some of these axioms. For example, read atomic, the weakest level, only satisfies INT (internal consistency) and EXT (external consistency); snapshot isolation, the strongest non-serializable model, satisfies PREFIX (arbitration order prefix) and NOCONFLICT (no concurrent modifications) as well. In order, the six models they define are read atomic, causal consistency, parallel snapshot isolation, prefix consistency, snapshot isolation, and serializability.</p>
<p>The diagrams I saw were the illustrations of anomalies that can occur if certain axioms don&rsquo;t hold. These are neatly formatted on page 6, which alongside page 10 lay out the key ideas of the paper. For instance, if TOTALVIS (total order/serializability) is not satisfied, then write skew can occur. This means that modifying two different objects (e.g. withdrawing from two separate accounts) while expecting a shared condition to hold (e.g. total balance over some threshold) can violate the condition unless there&rsquo;s some total order. These diagrams were much clearer than I&rsquo;d thought — they use concrete examples like bank accounts or posting comments to make the anomalies more concrete.</p>
<p>Interestingly, their work also outlines some more implementation-driven algorithms that satisfy their framework&rsquo;s models, &ldquo;closer to the intuition of practitioners&rdquo;. They prove how these operational specifications align with their axioms and properties. On a meta-level, they&rsquo;re structured a bit like the ones in an ICDT submission we made for specifying database backends — straightforward and ready to be directly implemented. They define a set of constraints separate from the model axioms earlier in the paper, and present a similar mapping of consistency model to constraints needed. Each of the original axioms follows from some combination of these constraints: for example, the transitive visibility axiom follows from causal delivery because replicas will only receive a transaction&rsquo;s logs after receiving its predecessors.</p>
<p>As I&rsquo;d mentioned yesterday, I want to improve my appetite for theory-heavy work, so reading this paper was a good chance to exercise that skill again. It certainly wasn&rsquo;t as intimidating as I thought it&rsquo;d be. The consistency diagrams look complex at first glance but are fairly straightforward even with the formal lens. I&rsquo;m itching to get back to some more applied research, though, which will be the focus of the next few days.</p>
<h2 id="day-4-Causal-Trees">4. CRDTs Don&rsquo;t Grow On Trees</h2>
<p>On the topic of consistency, I wanted to read about some <a href="https://crdt.tech/">CRDTs</a>: <a href="https://www.farley.ai/posts/causal">causal trees</a> and <a href="https://www.bartoszsypytkowski.com/operation-based-crdts-arrays-1/#rga">replicated growable arrays</a>. Both are very similar ideas for ensuring that concurrent edits to a doc (e.g. two friends working on a paper while both offline) are resolved in an orderly, meaningful way. Their basic variants operate on a per-character basis, but more <a href="https://www.bartoszsypytkowski.com/operation-based-crdts-arrays-2/">advanced variants</a> support editing ranges of text.</p>
<p>The main idea with causal trees is that each user is assigned an entity ID, and an idea similar to <a href="https://en.wikipedia.org/wiki/Lamport_timestamp">Lamport timestamps</a> is used to track time. The start of the text is the root of the tree, and when a user adds a character, an edge is created from the character&rsquo;s causal predecessor. If another user concurrently edits, the tree forks at the common causal predecessor and continues branching. A user&rsquo;s timestamp, which is attached to each character added, increments on each character added and is the maximum timestamp of all merging branches. Intuitively, this means timestamps represent how much content a user has seen before they insert something else. I&rsquo;d highly recommend checking out the visualizations on <a href="https://www.farley.ai/posts/causal">Farley&rsquo;s post</a> for an interactive look.</p>
<p>To merge branches, the head events are first sorted by decreasing timestamp (since this means the user saw more recent content as they were making their edit), then by decreasing entity ID (to deterministically create a <a href="https://en.wikipedia.org/wiki/Total_order">total order</a>). To recover the final text, we then take a depth-first pre-order traversal of the tree.</p>
<p>Deleting text uses a common CRDT strategy called tombstoning, where the inserted character is not actually removed from the tree, but just marked as a tombstone and skipped on final rendering. This is because if you hard-delete the node in the tree, no matter what design decision you take, you&rsquo;ll end up with tradeoffs. Surely you can&rsquo;t just delete the children of the node along with it, so this means you&rsquo;ll have to reattach them somewhere. Reattaching to the root of the tree is easy and deterministic but doesn&rsquo;t make semantic sense (characters would jump to the start of the text). Reattaching to the deleted node&rsquo;s parent sounds like a good idea, but if there are concurrent operations, things might end up out of order across clients if the immediate ancestor hasn&rsquo;t been received yet. Sort order would also change among the ancestor&rsquo;s siblings. All this means it&rsquo;s just easier to preserve the structure of the tree and skip the node later.</p>
<p>The original <a href="https://web.archive.org/web/20190505005829/http://www.st.ewi.tudelft.nl/victor/articles/ctre.pdf">causal trees paper</a> goes into more detail suggesting a data format based on Unicode to represent the timestamp numbers with special characters like ॐ representing the root of the tree. It gets pretty crazy: there&rsquo;s a diagram for how each data format converts to others and what methods combine them. Said methods are implemented not in a typical programming language, but via PCRE regexes. The author makes a fair point for this: they assert that higher-level languages like JS would have too much overhead creating individual objects for each character due to GC, hence the need for a text-only format and portable regex manipulation. I enjoy a cheeky <a href="https://regexcrossword.com/">regex puzzle</a> from time to time, so it was really neat seeing regexes be pushed here, but this was a bit of an arcane choice. There are also a lot of fiber-arts-related names for terms in the paper, which makes it a bit challenging to initially parse (in my head &ldquo;weave&rdquo; and &ldquo;weft&rdquo; occupy too similar of a semantic space).</p>
<p>There&rsquo;s another very similar CRDT called a <a href="https://web.archive.org/web/20190628155418/http://csl.skku.edu/papers/jpdc11.pdf">replicated growable array</a>. Instead of a tree, though, it&rsquo;s a flat list that&rsquo;s easy to read back. Each edit is still tagged with a timestamp and entity ID. For concurrent edits, the insertion pointers are offset for each concurrent client. In practice, it&rsquo;s faster due to simpler traversal and implementation, so RGAs end up being used in production libraries like Automerge and Yjs.</p>
<p>Thinking through why tombstones were necessary over raw deletions was a fun challenge, and I also liked coming to terms with the odd vocabulary of the causal types paper. Reading it first made understanding the RGA paper much easier, so I&rsquo;d recommend this order. Neither paper has overwhelming amounts of formalism either, which I appreciated. I got surprisingly interested in these couple papers now that I&rsquo;ve gone through the basic theory of CRDTs a few times, so I&rsquo;m looking forward to continuing this trend a bit in tomorrow&rsquo;s writeup.</p>
<h2 id="day-5-Fugue">5. A Fugue State</h2>
<p>Yesterday, we discussed causal trees and replicated growable arrays, two text editing CRDTs. Both CRDTs have a problem, though: while the concurrent edits neatly resolve for forward edits (e.g. typing as you normally would), what happens for backwards edits (e.g. typing a character, then moving your cursor to the start of the text, typing another character, and so on)? This is an issue called interleaving: both causal trees and RGAs are fine with forward interleaving but are susceptible to backward interleaving. If we think of typing &ldquo;bread&rdquo; backwards with a causal tree, we&rsquo;d end up with a shallow tree, with leaves for each character. If another, offline user concurrently types &ldquo;cakes&rdquo; backwards in the same document, we&rsquo;ll end up with the same shallow tree situation, but the text will be interleaved since the timestamps are the same. You&rsquo;ll end up with &ldquo;bcraekaeds&rdquo; — not quite ideal. Matthew Weidner and Martin Kleppmann&rsquo;s <a href="https://arxiv.org/pdf/2305.00583">Fugue paper</a> tackles this with a new CRDT that limits both forward and backward interleaving.</p>
<p>Causal trees and RGAs prevent forward interleaving because of the tree/list structure preserving &ldquo;inserted after&rdquo; information, but there&rsquo;s no structural relationship that preserves &ldquo;inserted before&rdquo;, leading to the backward interleaving. Fugue defines the concepts of &ldquo;left&rdquo; and &ldquo;right&rdquo; origins respectively: left origins track &ldquo;inserted after&rdquo; information and right origins &ldquo;inserted before&rdquo;. This comes together in the definition of maximal non-interleaving:</p>
<ul>
<li>if B is the first thing inserted after A (forward insertion), A and B should be consecutive in the final list.</li>
<li>if A is the last thing inserted before B (hence B is A&rsquo;s right origin, backward insertion), A and B should be consecutive in the final list, except where we might need to prioritize another concurrent forward-typing insertion.</li>
</ul>
<p>Fugue is also represented in a non-binary tree. However, its tree has the unique concepts of &ldquo;left/right children&rdquo;: each child of a node must be designated left or right, but there can be multiple left or right children, and left/right children are sorted amongst themselves by ID. On insertion, if the left origin&rsquo;s node has no right child, then the current insert is made a right child. This handles the forward-typing case. If the left origin&rsquo;s node already has a right child, the current insert is made a left child of the right origin, handling the backward-typing case. To recover the final text, perform an in-order traversal of the tree.</p>
<p>This tree will more or less stay binary, except in the case of concurrent inserts. In that case, there can be multiple left/right children, but the important thing is that the series of edits based on those left/right children stay together in a branch. Let&rsquo;s take the &ldquo;bread&rdquo; and &ldquo;cakes&rdquo; edit example I considered for the causal trees/RGA case. In Fugue, the updates for typing either word backwards would become a branch of nodes, each a left child. Now, merging these two users&rsquo; updates would cause there to be two left children branches on the root node, so we&rsquo;ll get &ldquo;breadcakes&rdquo; (or &ldquo;cakesbread&rdquo;, depending on the users&rsquo; entity IDs) as intended.</p>
<p>Non-interleaving thus holds intuitively, since the concurrent inserts get separated into separate branches. In-order traversal thus makes sure runs of text are consecutive instead of interleaved. Some interleaving is still inevitable, but this happens only in limited edge cases with multiple interacting concurrent updates. In practice, this should rarely be of concern.</p>
<p>The maximal non-interleaving property is so named because it defines a unique order over the operations. This is important, since this means no other non-trivial properties can be proven on top — this interleaving as as strong a statement as you&rsquo;ll ever make. Kleppmann et al. had previously proposed another interleaving property, which will never hold on any CRDT, and their prior CRDT proposal was proven to not converge in some cases.</p>
<p>As described earlier, Fugue will prevent backward interleaving, but still won&rsquo;t satisfy maximal non-interleaving in some of these edge cases. The paper thus defines FugueMax, a slightly more complicated variant that sorts multiple right children in reverse order of their right origins instead of by lexicographic node ID. FugueMax is proven to support maximal non-interleaving, but the paper argues the extra complexity isn&rsquo;t worth it.</p>
<p>In terms of performance, Fugue is fairly comparable to the SOTA Yjs in terms of network bandwidth, load times, throughput, etc. FugueMax fares a little worse but is still reasonable. One thing that struck me was that even Yjs has a lot of metadata overhead, on the order of tens of bytes per character insertion. This put into perspective just how performant modern systems are, if we can afford that overhead and remain perfectly usable.</p>
<p>I really liked the continuation between first looking at causal trees and RGAs, then realizing the edge cases and interleaving problems, then reading Fugue — it was like tracing through each logical step in the frontier of CRDT capability. The way each paper builds intuitively from the last makes the problem space seem very approachable. I think I read somewhere once that (paraphrased) &ldquo;good papers make <em>you</em> feel like you&rsquo;re making discoveries&rdquo; and that&rsquo;s certainly been true for this series.</p>
<h2 id="day-6-Braid">6. Braid and Abet</h2>
<p>I mentioned how the <a href="https://arxiv.org/pdf/2305.00583">causal trees</a> paper discussed two days ago made an almost-offputting number of weaving references. Today, we&rsquo;re picking up the trend again with Braid!</p>
<p>Braid is a project working towards interopable state synchronization, with the goal of making &ldquo;read[ing] &amp; writ[ing] distributed state as easily as a local variable&rdquo;. They&rsquo;re run as an <a href="https://invisible.college/">Invisible College</a> community and are actively developing the ecosystem, with an <a href="https://datatracker.ietf.org/doc/html/draft-toomim-httpbis-braid-http">IETF draft</a>, a Braid-based filesystem, <a href="https://braid.org/automerge">Automerge support</a> and example <a href="https://github.com/braid-org/braid-chrome">Chrome extension</a>, just to name a few initiatives.</p>
<p>There&rsquo;s four major aspects of Braid-HTTP:</p>
<ul>
<li>Subscriptions attached to GET requests, for real-time synchronization</li>
<li>Versioning via new <code>Version</code> and <code>Parent</code> headers, which extend <code>ETag</code> headers as a better indicator of time (as opposed to a content-based hash)</li>
<li>Custom concurrent edit handling via <code>Merge-Type</code> headers</li>
<li>More efficient patch updates, ability to <a href="https://datatracker.ietf.org/doc/html/draft-toomim-httpbis-braid-http#ref-RANGE-PATCH">patch a range of data</a></li>
</ul>
<p>The spec draft lists these advances in a different order, but I&rsquo;ve listed them in order of what&rsquo;s most interesting to me. The demo, with very tangible updates, makes the first couple feel very magical, especially knowing all this is done over an HTTP extension rather than custom code.</p>
<p>The <code>Merge-Type</code> is an interesting space for customizability. They have one implementation available with <a href="https://github.com/josephg/diamond-types">Diamond Types</a>, the <a href="https://josephg.com/blog/crdts-go-brrr/">fastest CRDTs available</a>, written by one of the Braid spec draft coauthors Joseph Gentle. Besides the performance boosts, from my understanding this is a pretty standard CRDT in theory. They have another implementation called a <a href="https://braid.org/meeting-76/simpleton">&lsquo;simpleton&rsquo;</a>, which is a bit like an OT in that there&rsquo;s some rebasing of peer edits. The idea is that CRDT-ifying everything is a waste if most data is read instead of written, and most writes are sequential instead of concurrent. Simpleton merging means that all conflicts resolve to the client&rsquo;s version, and the server does the work of rebasing edits and bringing clients up to speed. Clients ignore all received versions without its current version in the history and send edits as patches to the server. This gets you consistency and simplicity but comes at the cost of clients being half-duplex, unable to read other peers&rsquo; updates within a RTT of writing due to the server needing to be in the loop to rebase.</p>
<p>They&rsquo;ve recently been talking about an unification of OT and CRDTs called the <a href="https://braid.org/meeting-111">time machine</a>. It&rsquo;s a &ldquo;CRDT that implements OT, and is implemented via OT implemented via an internal CRDT&rdquo;. At first I thought this was vaguely reminiscent of the <a href="https://arxiv.org/html/2409.14252v1">Eg-walker</a> paper, but then I put two and two together and realized Gentle coauthored both the paper and this spec. In this time machine framework, OTs provide the mechanism for allowing events to &ldquo;time travel&rdquo; and be applied out of order to different CRDT versions, and CRDTs let us reason about the different views of clients given distributed time. History is stored in a neutral format, and different peers can use different merge strategies while interoperating since the OT-on-CRDT lets us transform remote histories for local usage.</p>
<p>I&rsquo;d recommend first reading the <a href="https://braid.org/">Braid landing page</a> and watching <a href="https://braid.org/post/response-to-gritzko#:~:text=See%20the%20demo%20video%20from%20Braid%2075%3A">a demo</a> video to contextualize what Braid can do, before reading <a href="https://replicated.wiki/blog/braid">this review of Braid by the causal trees author</a> and the <a href="https://braid.org/post/response-to-gritzko">Braid community&rsquo;s response</a> to go into more detail. The <a href="https://braid.org/meeting-111">time machine</a> meeting summary can stand alone in case you&rsquo;re less interested with the rest of the protocol. It&rsquo;s neat to see that I can recognize names in the space (e.g. Gentle) and have enough of a knowledge base that I can connect across familiar work now. Overall, I&rsquo;m looking forward to seeing where Braid goes, particularly the time machine CRDT piece!</p>
<h2 id="day-7-ElectricSQL">7. The ElectricSQL Slide</h2>
<p>I found out about <a href="http://electric-sql.com/">ElectricSQL</a> when I was in Paris, after looking up a sticker I saw in the lab. Surprise: my supervisor is also a technical advisor there. My first exposure to the company was its <a href="https://electric-sql.com/about/team">team page</a>, where I realized they&rsquo;d accumulated a good number of the CRDT Mafia<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> as advisors and had received angel investments from a couple Ink &amp; Switch-adjacent folks I knew. I figured if they&rsquo;d managed to convince my supervisor to advise them, they must be working on something interesting.</p>
<p>Electric is a sync engine from Postgres DBs to client apps. It enables live reactivity, like <a href="#day-6-Braid">Braid-HTTP</a>, and boasts a wide range of framework integrations, from old favourites like React to CRDT interfaces with Yjs. It syncs subsets of data, which are called &ldquo;shapes&rdquo; in their framework, a bit reminiscent of the goals of Gritzko&rsquo;s <a href="https://github.com/gritzko/go-rdx">RDX</a>. Shapes are defined with a table, with the option to specify a <code>WHERE</code> clause or specific columns — it&rsquo;s like a database view. The <code>WHERE</code> clauses can tank performance (the docs say perf is inversely proportional to the number of Shapes) unless they include <code>field = constant</code> clauses, which have a <a href="https://electric-sql.com/docs/guides/shapes#optimized-where-clauses">special index</a>.</p>
<p>Electric sits in front of Postgres, consuming the <a href="https://www.postgresql.org/docs/current/logical-replication.html">logical replication log</a> and massaging it into a so-called Shape Log. The Shape Log is conceptually similar to Postgres&rsquo;s log, but just filter for the relevant data. Clients will first request the entirety of this Shape Log, which might need to be split up over multiple requests. Electric remaps initial query results to insert operations so the shapes don&rsquo;t have to be predefined. Once the client has processed the Shape Log and gets an &lsquo;up-to-date&rsquo; header response, it switches into Live Mode, repeatedly issuing requests and holding each request open until new data is received or it times out. Clients are responsible for then taking the Shape Log and turning them into materialized data for the app, which can be done via Typescript/Elixir clients or <a href="https://electric-sql.com/docs/guides/client-development#materialise-the-shape-log">a hand-rolled implementation</a>.</p>
<p>Auth is left to the user to worry about, a nice concession that allows for flexibility. There are two main patterns suggested: proxy-based auth and gatekeeper auth. Proxy-based auth does as advertised: you can add an authorization header to the client&rsquo;s request, pass it to the proxy, validate it, and pass Electric&rsquo;s response back if authorized. The gatekeeper auth pattern is a bit more interesting: you&rsquo;ll need to build a gatekeeper endpoint in your API, which issues Shape-scoped auth tokens. Then there needs to be another authorizing proxy to check the claims of the token against the request parameters, which is less complicated because we&rsquo;re only checking for an exact match. There are additional implementation details and examples <a href="https://electric-sql.com/docs/guides/auth">here</a>.</p>
<p>I didn&rsquo;t poke too hard into the technical details of implementation, but I did scroll down their blog post list and found this neat gem about <a href="https://electric-sql.com/blog/2022/05/03/introducing-rich-crdts">Rich CRDTs</a>. These are CRDTs that additionally support composition, compensations (side effects), and reservations (escrows fine-grained locking). I couldn&rsquo;t find additional papers or implementation examples anywhere, but I thought the idea was compelling at least.</p>
<p>This was a good opportunity to dive into one of the live sync tools available — Electric itself lists <a href="https://electric-sql.com/docs/reference/alternatives">many alternatives</a> which seem to be clustered around a similar space. I&rsquo;m happy to see the CRDT/sync/local-first-adjacent community is flourishing with so many up-and-coming products. I hate to bring AI into the mix, but with increasing LLM/&ldquo;magic&rdquo; integrations into more and more apps, I expect we&rsquo;ll want to see more syncing of context (and <a href="https://electric-sql.com/blog/2025/04/09/building-ai-apps-on-sync">the Electric blog agrees</a>). The research legwork the community&rsquo;s already invested in will be such a strong foundation for this.</p>
<h2 id="day-8-LiveStore">8. LiveStore, Laugh, Love</h2>
<p>One of ElectricSQL&rsquo;s integrations is with a platform called <a href="https://livestore.dev/">LiveStore</a>, and one of the technical advising team members at Electric happens to be its founder, <a href="https://www.schickling.dev/">Johannes Schickling</a>. Electric also funds (funded?) development of LiveStore and there seems to be a close partnership between the two teams. Before reading up about either, I&rsquo;d heard of both in passing, but at a distance, both seemed vaguely similar: sync-y reactive database-shaped tools. I was curious to see how they differed.</p>
<p>LiveStore is similarly a sync and state management framework, but is geared more towards local-first apps. It also provides live reactivity on the client-side, but is based on SQLite and an event-sourcing model, separating reads and writes. Read events go through the SQLite layer. Separately, each write event that&rsquo;s created on a client is committed and pushed to the sync backend, which then accepts the push. It then actively pokes the other clients to pull the new event. LiveStore defines a flexible event model API and automatically manages the persistence and recomputating of state.</p>
<p>The <a href="https://docs.livestore.dev/reference/reactivity-system/">reactivity system</a> is based on three types of state:</p>
<ul>
<li>Reactive SQL queries, which additionally support callbacks and explicit dependencies to manage relationships with other queries and state</li>
<li>Reactive signals, state values that aren&rsquo;t in tables but should be persisted</li>
<li>Reactive computed values, which can perform additional processing on signals</li>
</ul>
<p>LiveStore was based on an earlier research project, <a href="https://riffle.systems/">Riffle</a>. The project was based at MIT CSAIL, and <a href="https://www.geoffreylitt.com/">Geoffrey Litt</a> at Ink &amp; Switch was part of the project. One of the interesting key ideas here was the separation of concerns between state management and syncing (loosely, the separation between LiveStore and the sync provider, like ElectricSQL). As <a href="https://riffle.systems/essays/prelude/#introduction">their Riffle blog post reads</a>, &ldquo;[i]f an app developer could rely on a sufficiently powerful local state management layer, then their UI code could just read and write local data, without worrying about synchronizing data […] or other chores […]&rdquo;. The local-first aspect allows for local queries and reactive state to update in the time it takes to render a single frame. It&rsquo;s also neat to see the progression of the work, from the <a href="https://riffle.systems/essays/prelude/#prototype-system-sqlite--react">demo music library app</a> in Riffle blog posts to <a href="https://overtone.pro/">Overtone</a> to the underlying tech being split out into LiveStore. There aren&rsquo;t many technical details on how the reactive query performance is optimized in either the blog posts or the UIST paper as I can find, which makes sense for the given audience, but I would have liked to dive deeper into that piece.</p>
<p>LiveStore is still under pre-1.0 active development, so I&rsquo;m excited to see where the project continues growing. One thing that stood out about even LiveStore&rsquo;s early, incomplete docs was the explicit <a href="https://docs.livestore.dev/evaluation/design-decisions/">Design Decisions</a> page. It&rsquo;s short but collects all the main considerations in one place, so I could get an overview of the project&rsquo;s motivations without trying to read between the lines elsewhere. Making decisions and learnings explicit makes them easier to take to heart, and it&rsquo;s also a big part of what makes books like <a href="https://aosabook.org/en/">AOSA</a> compelling. This is a strategy I&rsquo;ll try to adopt for some of my research projects moving forward — I keep a research log, but the decisions get blurred together with all the other WIP notes. Explicitly calling them out somewhere will help context-share as well as make paper writing clearer and more straightforward.</p>
<h2 id="day-9-ConvexSQL">9. Making a Convex-ion</h2>
<p>I&rsquo;m rounding out my miniseries of live, reactive sync systems with <a href="https://www.convex.dev/">ConvexSQL</a>, a framework I keep hearing about on Twitter.</p>
<p>Convex is an ACID compliant, reactive datastore that lets apps atomically live-update as queries change. It doesn&rsquo;t use SQL and doesn&rsquo;t require ORMs — like Electric it leans more heavily on the Typescript type system. Compared to the other systems, Convex is more of a platform, running queries as if they were serverless functions within their architecture. It&rsquo;s more opinionated and abstracts away any database optimization that developers might have to perform.</p>
<p>A Convex deployment includes the database itself alongside a sync worker and a function runner. Queries and updates must be expressed as functions, which are bundled and executed by the V8 runtime in the function runner. The database stores tables in a transaction log format that tracks all versions of documents, a bit like a write-ahead log with CRDTs. Operations are wrapped in transactions, which Convex ensures are run serializably using optimistic concurrency control and tracking read and write sets. The transactions implementation was quite cool to read about, since I recognized many of the ideas from my formally-specified database project during my research internship in Paris.</p>
<p>To sync, the database can use a similar algorithm as what they use to ensure serializability to detect updates in queries. After rerunning dependent queries, results are pushed to the relevant sync worker and transferred via Websocket to the client, which updates its local state.</p>
<p>ElectricSQL supports <a href="https://materializedview.io/p/electricsql-pglite-crdts-and-elixir">transactional causal consistency+</a>, which means that there are atomic (and high-availability) transactions that respect causal consistency and that this is implemented with CRDTs. LiveStore doesn&rsquo;t have transactions and is eventually consistent with a total order on events in its log. In contrast, Convex provides serializability: &ldquo;We believe that any isolation level less than serializable is just too hard a programming model for developers.&rdquo; This is both rather funny and rather correct, and I appreciate that it eliminates this whole class of issues in favour of mechanisms like transaction abort-retry that are more intuitive to implement.</p>
<p>I noted that Convex is leaning quite heavily into highlighting how optimized their platform is for AI — they have a whole <a href="https://www.convex.dev/ai">landing page</a> detailing the best prompts and rules as well as quickstart examples for building LLM integrations. Because they lean more into Typescript schemas, they claim LLMs are better equipped to generate Convex code (which I believe). Their quickstart page contains the typical React and other frameworks, but also has a plain textbox to directly generate an app. They&rsquo;ve built a whole platform-on-a-platform, <a href="https://chef.convex.dev/">Chef</a>, to oneshot entire realtime apps, which is very compelling to demo. Also, one application they show that meshes nicely with reactive frameworks is <a href="https://stack.convex.dev/build-streaming-chat-app-with-persistent-text-streaming-component">LLM streaming output</a> — it makes sense that we might want to store the output but also stream a call&rsquo;s progress in real time. Convex was started around the same time as the other projects (2020, in comparison to ElectricSQL and LiveStore, which both seem to have been started in 2021) but seems to have enjoyed more developer popularity, and I wonder if this recent LLM push had anything to do with adoption.</p>
<p>This writeup was mostly based on the very thorough <a href="https://stack.convex.dev/how-convex-works">How Convex Works</a> blog post — I appreciate the transparency and clear walkthrough. The <a href="https://stack.convex.dev/how-convex-works#putting-it-all-together-walking-through-a-request">life-of-a-request diagrams</a> were particularly helpful, and I&rsquo;d have liked to see something similar with the other platforms to get a better sense of the flow in one place.</p>
<p>Reading about three adjacent realtime frameworks in a row let me compare-and-contrast them more easily: learning about each system contextualized it among the others, and I got to &ldquo;map out&rdquo; the space. I think three is enough sync frameworks for now, but there are <a href="https://electric-sql.com/docs/reference/alternatives">many others</a> I&rsquo;d still like to read about sometime, like Turso, Jazz, Zero, and InstantDB. Check those out if you&rsquo;re interested in exploring further, but for now, I plan to turn back to some notable database systems papers in my next few writeups.</p>
<h2 id="day-10-MapReduce">10. MapReduce Your Expectations</h2>
<p>This marks the start of a series of more mainstream database papers. I have this sense I should have read these by now, if only to build more context for what folks keep referencing in design discussions or online. The first system I chose was MapReduce, which seemed most approachable.</p>
<p>Functional programmers will find MapReduce intuitive to work with. It does exactly what it says on the tin: map and reduce values. The input is a large series of key-value pairs, which are then transformed into intermediate key-value pairs with the user-defined <em>map</em> function, and collected by intermediate key into a series of final values with the user-defined <em>reduce</em> function.</p>
<p>The framework aims to parallelize computation, so it makes use of a special master worker orchestrating many other workers. Input data is partitioned into splits and distributed among map worker nodes, which write the results locally. Intermediate pair tasks are partitioned (via a hash, for example) and then split up amongst the reduce workers. These reduce workers use RPCs to read the buffered data from map workers&rsquo; disks, sorts it, then performs the reduce operation. The final result is sent back to the master worker, which is also responsible for waking up the user program to pass the final results.</p>
<p>MapReduce also takes care of fault-tolerance concerns for the user. If a task times out, the master worker will mark it as failed and rerun it. Completed reduce tasks are safe since its results are stored in global storage, but if a machine that ran a map task failed, the master worker will also rerun its map task since the local disk becomes inaccessible. Interestingly, MapReduce supports non-deterministic tasks, simply providing weaker semantics and guarantees. One neat point is that stragglers, or slow/delayed tasks, can impact performance for the entire job, so once a job is close to completion, the master worker also schedules backup executions of the remaining in-progress tasks to race against existing executions. This is an interesting technique especially given nondeterministic functions are allowed, but they reported this noticeably improved performance.</p>
<p>The magic of MapReduce is that mapping and reducing is all users have to worry about. They don&rsquo;t have to figure out how to optimally place data to reduce latency (the master worker will take locality into consideration when assigning tasks) or think about fault tolerance and durability. Because MapReduce takes care of these reliability and performance concerns in a common abstraction layer, teams don&rsquo;t have to keep reinventing the wheel, which increases maintainability and productivity. I assume the MapReduce team played the role of a dev infra startup within the company — it&rsquo;s cool that this type of framework and subteam forms in industry. Working on infra teams like this in the past was really fun and I enjoyed the impact and considerations we&rsquo;d have to make.</p>
<p>The two meta points that drew my interest most were how the paper demonstrates how widely this simple pattern has been applied, and how they decided to discuss some of the actual library evolution and document real usage examples along with concrete results. For example, in addition to their more contrived experiments, they&rsquo;d explain what size partitions they&rsquo;d tend to choose, why, then explain the CPU utilization they observed with the configuration after real usage. I rarely see this with more academic papers: being able to share these patterns is a real strength of industry papers.</p>
<p>This paper was actually assigned as one of my class readings this term — I was already planning to read it, so I was happy to kill two birds with one stone. I&rsquo;ll note that I based this writeup on the shorter Communications of the ACM version as opposed to the original OSDI paper. This version is a more concise version that glosses over the extensions to the basic model, like allowing users to define custom reduce partitioning logic, and is ironically more highly cited, which perhaps speaks to how easy to understand it was. These details weren&rsquo;t of the most interest at my level, though, so this version sufficed. One day I&rsquo;ll revisit the full version of this paper, but in the meantime I think reading shorter versions prepared by the same authors could be a good strategy for reading papers that seem intimidating, like the ones that follow.</p>
<h2 id="day-11-Zookeeper">11. Fantastic Beasts and How to Lock Them</h2>
<p>Zookeeper is one of those system names that sticks with you: what beasts<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> could they possibly be herding that warranted such a name? I&rsquo;d heard about Zookeeper in many a distributed systems reading list, so I figured it was time to give it a read. I was surprised as soon as I opened the PDF: I&rsquo;d always heard of Zookeeper in the AWS context, so I&rsquo;d thought Zookeeper was Amazon&rsquo;s brainchild, but it turns out it originated at Yahoo, who published the original paper at <a href="https://www.usenix.org/legacy/event/atc10/tech/full_papers/Hunt.pdf">ATC 2010</a>.</p>
<p>In it, Zookeeper is described as a coordination kernel with wait-free, hierarchical objects. It provides primitives that can be used to build locks, group management, and more, offering developers flexibility and modularity in how they&rsquo;re actually implemented. I found it helpful to visualize it as a filesystem with extra message ordering guarantees. Requests are pipelined and asynchronously processed, and Zookeeper&rsquo;s API provides notifications to watch for updates to objects.</p>
<p>Zookeeper&rsquo;s main primitive is the <em>znode</em> concept, which are sets of data nodes stored in a hierarchical order and accessible via JSON PATCH-like paths. These can be freely created and deleted by workers, and can be even made ephemeral, cleaned up when its parent terminates. Znodes use version tags for concurrency control, like CosmosDB etags.</p>
<p>These znodes can be used to build a variety of utilities, as described in the paper:</p>
<ul>
<li>Live config management can be implemented by having a &ldquo;ready&rdquo; sentinel object that a leader process can delete before making modifications to config and recreate when it&rsquo;s ready to be processed. Workers can subscribe to updates on the ready object to re-process the config.</li>
<li>Group management can be implemented with ephemeral nodes that are created as child znodes on some well-known znode. These nodes are created and die in lockstep with their parent processes, and group membership can be calculated by listing the children of the znode.</li>
<li>Locks can be implemented by creating a designated node with an ephemeral flag and letting all nodes race to create the object. To avoid a thundering-herd effect, nodes can be set up to only watch the previous node in the queue; read locks can be implemented by watching only the previous <em>write</em> lock node.</li>
<li>The paper then goes on to explain some real-world use cases that remix the above abstractions. In a few, they mention leader election being implemented via Zookeeper, but don&rsquo;t mention it in the rest of the paper: I would have really liked to see how it was put together from znodes.</li>
</ul>
<p>Zookeeper replicates all data in in-memory DBs that each contain the whole tree. It uses a special atomic broadcast protocol, <a href="https://ieeexplore.ieee.org/document/5958223">ZAB</a>, for requests that require coordination, like writes. For these, Zookeeper guarantees linearizable state writes, but it serves reads from each local replica. These means that reads can be stale, so they also describe their <code>sync</code> API that forces all writes issued before the read to finish first. Zookeeper also has a WAL and uses idempotent transactions in its request processor, making recovery a straightforward process of replaying events off a fuzzy snapshot.</p>
<p>Again, I&rsquo;ve realized I&rsquo;ve been over-intimidated by a paper: it was much easier to parse than I expected. Perhaps this speaks to the quality of the papers and why they&rsquo;ve been recommended so frequently. I found the structure of explaining the programming model of znodes first, followed by examples, then finally jumping into implementation made for a very clear storyline — this is a recipe I hope I&rsquo;ll remember to draw on in the future.</p>
<h2 id="day-12-Paxos">12. Grecian Politics</h2>
<p>Once upon a time I bodged together an <a href="https://github.com/kewbish/disparati">implementation of Raft</a>, one of the two most well-known consensus protocols. Masquerading as a distributed systems enthusiast, this felt like an apt thing to do. When people evoked log replication and leader heartbeats, I, too, could nod my head along. Unfortunately, I hadn&rsquo;t gotten around to learning about the other main consensus protocol, Paxos. Whenever folks would bring it up, all that came to mind was vague premonitions of it being harder to understand and implement than Raft. But I figure now&rsquo;s as good an excuse as any to finally read through it, so I added it to my reading list.</p>
<p>It turns out my suspicions of Paxos being a bit convoluted were well-founded. Based on the success of his Byzantine generals parable, Lamport&rsquo;s original paper frames the protocol as a story of a parliament on a Greek island, complete with characters with Greek-esque gibberish names. While this has Lamport&rsquo;s trademark quirkiness written all over it, the other distributed systems pioneers who read it <a href="https://lamport.azurewebsites.net/pubs/pubs.html#lamport-paxos">didn&rsquo;t pick up on the algorithm</a>. His original submission was rejected, and it was only published later with &ldquo;more serious&rdquo; annotations. I based this writeup on the easier-to-parse <a href="https://lamport.azurewebsites.net/pubs/paxos-simple.pdf">&ldquo;Paxos made simple&rdquo;</a> writeup Lamport made, whose abstract merely states &ldquo;The Paxos algorithm, when presented in plain English, is very simple.&rdquo; Thanks, Lamport, I&rsquo;ll try my best.</p>
<p>There are three roles in Paxos, which can be played simultaneously by each node in a larger system: acceptors, proposers, and learners. The proposers propose terms of the form (proposal number, value), which the acceptors evaluate and potentially commit to. Then, learners will learn the final, consensus&rsquo;ed value. The hard part of Paxos is making sure that only a single proposed value is chosen and processes only learn it&rsquo;s been chosen when it&rsquo;s been committed to. The paper handwaves away the problem of electing a leader (presumably done via whoever wins the consensus race) but it serves as a distinguished proposer and learner to optimize some communication.</p>
<p>There are also three phases in Paxos.</p>
<ul>
<li>The first phase starts with proposers proposing a proposal number (yes, <a href="https://dontasktoask.com/">don&rsquo;t-ask-to-ask</a> seems not to apply here) to the acceptors and requesting 1) a confirmation that the acceptor agrees not to accept any lower-numbered proposals 2) the previous highest proposal.
Once it receives responses from a majority of acceptors, it can issue its own proposal with the agreed-upon proposal number. The value of the proposal is the value of the highest-valued proposal it received in its Phase 1 response, or if no such accepted proposals exist it can provide its own choice of value. Finally, acceptors can accept the proposal if they haven&rsquo;t accepted a proposal with a lower number.</li>
<li>Finally, the acceptors can contact the distinguished learners (or all learners in general) to then gossip the result to the other learners.</li>
</ul>
<p>Paxos prioritizes <a href="#day-1-CAP/CAL-Theorems">consistency over availability</a>, and only makes progress if the majority of nodes are running. Progress can also only be guaranteed with a single distinguished proposer, the role of the leader I mentioned previously.</p>
<p>One neat thing this paper covered was a section on implementing a distributed state machine with Paxos. Each proposed value would be a deterministic command, and machines would only execute commands once agreed upon. I never realized this was a focus or even feature of Paxos.</p>
<p>Having come from a Raft-centric background in consensus, the main differences from Raft I noticed with Paxos were the lack of log replication, lack of explicit leader election, and reports of it being tricky to implement, whereas Raft has a reputation for being more clear. There are a few variants and implementations of Paxos that address some of the other tradeoffs, including <a href="https://www.cs.cornell.edu/home/rvr/Paxos/paxos.pdf">Multi-Paxos</a>, which allows multiple values in series to be decided upon. It&rsquo;s implemented at Google in the Chubby filesystem, which was also mentioned in the <a href="#day-11-Zookeeper">Zookeeper paper</a>. There&rsquo;s also a <a href="https://fpaxos.github.io/">Flexi-Paxos</a>, which allows more flexible interpretations of what qualifies as a valid quorum between the phases of the protocol.</p>
<p>I&rsquo;m not sure if the paper lives up to its abstract (untangling the propositions is not what I&rsquo;d call &ldquo;very simple&rdquo;) but it&rsquo;s certainly demystified the protocol. I quite liked the structure of the paper as well: it&rsquo;s very TLA+-coded (and fitting) of Lamport to formulate the algorithm in terms of its safety invariants first. The declarative proofs leading to each new bit of the protocol was an interesting choice but made the protocol&rsquo;s construction more incremental. I&rsquo;m not sure if I&rsquo;ll ever have to implement Paxos in a system — everything consensus-related I&rsquo;ve worked with so far has been Raft — but if I ever do, I&rsquo;ll feel a little more prepared, and perhaps a little more scared, thanks to this paper.</p>
<h2 id="day-13-Spanner">13. We Call It A Wrench</h2>
<p>When I was interning this last summer, I learned through the grapevine that the person sitting behind me was the former Director of Spanner at Google. I understood the weight and aura of such a title: I&rsquo;d read about Spanner in DDIA before, and I&rsquo;d heard it referenced frequently enough to get the gist that it was a Big Important System. I&rsquo;d never taken more than a cursory look at the paper abstract, though, and given I&rsquo;d just read about Paxos, which backs Spanner, I thought it was an apt time to take a deeper look.</p>
<p>Spanner is a multi-version semi-relational database with automatic geographic redistribution and resharding. It uses sharded <a href="#day-12-Paxos">Paxos</a> state machines to provide externally-consistent (more about this later) distributed transactions. Spanner deployments are organized in universes (very grand), with the unit of physical isolation being a zone in a universe. Zones serve data to clients via spanservers, which keep track of a unique data structure called a tablet. These tablets are bags of KV pairs with timestamps. Each tablet gets its own Paxos state machine for replication. While it&rsquo;s not discussed in the paper, a placement driver at the universe level then automates the movement of these tablets. I found this part a bit overly detailed, especially since there are abstractions and components that aren&rsquo;t discussed later on.</p>
<p>Spanner has the interesting property of supporting globally-consistent reads at a timestamp. As well, it enables externally-consistent reads and writes: external consistency is more or less linearizability, if you squint, but for transactions with multiple operations instead of linearizability&rsquo;s definition based on only individual atomic ops.</p>
<p>This is all thanks to the TrueTime API that Spanner defines: instead of timestamps bearing a single value, which presents issues with clock skew between machines, timestamps build in this very uncertainty interval. There are time &ldquo;masters&rdquo; per datacenter and clients per machine, which sync and determine the largest possible error rate. Most operations require waiting until the uncertainty interval has fully expired before proceeding. The parts about choosing timestamps for transactions were interesting to me because I&rsquo;d had to deal with a similar problem for a prior research internship: read-only transactions choose the earliest time that would not violate visibility to avoid backing up other concurrent transactions, and schema change transactions and operations that depend on said changes set their timestamp well into the future. I think it was worth reading about Spanner for this unique timestamp mechanism and the guarantees it gives alone — section 3 and 4 cover this in detail. For a less academic take, Spanner also has <a href="https://cloud.google.com/spanner/docs/true-time-external-consistency">developer docs</a> available.</p>
<p>I&rsquo;d expected more math proofs and complicated causality diagrams of the sort I encountered in some earlier FLP/CRDT papers, but I was pleasantly surprised the paper went without. I found the &ldquo;industry voice&rdquo; quite clear given the emphasis on applied deployment details. I was also surprised how open Google was about Spanner&rsquo;s relationships in contrast to other internal storage products like BigTable — in some parts, the paper read almost like an internal onboarding doc for when to use certain systems. It makes sense, since back then they&rsquo;d already published papers on those other systems.</p>
<p>I was curious about Google&rsquo;s more recent <a href="https://research.google/pubs/?&amp;category=distributed-systems-and-parallel-computing">distributed systems publications</a>, so I went to go poke around. It seems like they continue to put out &ldquo;bigger&rdquo; papers on new systems that build on their existing services. I&rsquo;m happy that&rsquo;s the case: sometimes it seems like I&rsquo;ve missed the heyday of the early 2010s for all the systems I keep hearing about, but it&rsquo;s encouraging to see there&rsquo;s still plenty of work and research to be done.</p>
<h2 id="day-14-Dynamo">14. DynamoDB Contra Dynamo</h2>
<p>Amazon is dogmatically adherent to its leadership principles: interview there and you&rsquo;ll likely need to speak to a time you exhibited &ldquo;customer obsession&rdquo; or one of the fourteen other platitudes. It turns out this values-driven approach extends to its systems research as well. Folks suggested I read about Dynamo, an &ldquo;always-writeable&rdquo; KV store that optimizes heavily for availability over consistency, published by Amazon at <a href="https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf">SOSP 2007</a>. It was a fascinating read, both technically and culturally:</p>
<blockquote>
<p>In this paper there are many references to this 99.9th percentile of distributions, which reflects Amazon engineers’ relentless focus on performance from the perspective of the customers’ experience. […] Amazon’s engineering and optimization efforts are not focused on averages.</p>
</blockquote>
<p>If the engineers were focused on an above-average customer experience, I&rsquo;m sure the paper authors were too: I found it quite clear and thorough. The bulk of the paper describes how they applied well-known techniques (e.g. consistent hashing for partitioning) and it&rsquo;s neat that the academic novelty is intended to focus on evaluation and production readiness optimization.</p>
<p>For some extra context, Dynamo focuses on being highly available instead of consistent. It&rsquo;s a KV-only store that supports small-blob storage. There&rsquo;s no isolation and no multi-key operations. Dynamo stores multiple versions of each object, so if concurrent writes conflict, the application can define a custom conflict resolution pathway, or fall back to the system&rsquo;s last-writer-wins strategy. It&rsquo;s supposed to be a leaderless service, with each node having enough information to route and handle requests — the paper describes it like a &ldquo;zero-hop DHT&rdquo;.</p>
<p>Keys are partitioned across nodes via a &ldquo;Dynamo ring&rdquo;, which just uses consistent hashing. Each key is replicated at some clockwise successor nodes. Merkle trees are used to reduce transferred data during replica synchronization. The Dynamo ring is updated manually, with updates being gossipped around. Some nodes are well-known and serve as seeds to relay information about newly joined nodes.</p>
<p>In Dynamo, multiple version of data can (rarely) coexist. They&rsquo;re labelled with vector clocks, and the node serving the read is responsible for collapsing these branches. There&rsquo;s an interesting vector clock optimization where they&rsquo;re keeping essentially a LRU cache of the nodes that last updated a key in the clock and discarding other older node updates.</p>
<p>Dynamo allows users to tune knobs defining replication: each key is replicated to N nodes, each write request goes to W of them, and each read to R of them, with W + R &gt; N to ensure at least one node gets the latest value. The usual value for these parameters is W=2, R=2, N=3. Dynamo uses a sloppy quorum, which means it uses the first N healthy nodes, which might require walking more of the ring beyond the first N nodes. There&rsquo;s metadata with each request that includes hints on the intended replica, so a replica further along the ring can buffer updates and catch the intended replica up once it comes back online. As well, if a node serving the read gets any stale versions among its R read responses, it can update those nodes without falling back to the Merkle tree replication — this is called read repair.</p>
<p>The implementation has three components: a request coordinator, membership and failure detection protocol, and a local DB engine. I thought it was interesting the underlying DB architecture was pluggable, which enables different engines according to application workloads. It seems perhaps feasible to use more traditional relational DBMSes and replicate tables instead of just keys, and I&rsquo;m wondering if they ever tried this or if the overhead of running a DBMS was contrary to the high-availability and performance angle.</p>
<p>Anyways, I thought I was done with Paxos-based systems, but it turns out I wasn&rsquo;t. Amazon came out with another paper on Dynamo<strong>DB</strong> at <a href="https://www.usenix.org/system/files/atc22-elhemali.pdf">ATC 2022</a>, which describes DynamoDB as we know it today in its AWS form. The design draws on the principles of Dynamo, but all of the architecture today is different. It now supports indexes, full tables, and a single-leader multi-Paxos replication strategy instead of the leaderless ring-based/Merkle tree approach in Dynamo. There are now microservices in the microservices: DynamoDB is itself built in small components with a centralized administration service. This paper contains much less actual architecture discussion than the first — I can&rsquo;t help but wonder if this is due to the increasing corporatisation and legal clampdown I&rsquo;d expect at today&rsquo;s scale. Unfortunately, this second paper contained much fewer iconic company-culture statements (I found none in my brief skim), so I wasn&rsquo;t motivated to read it in its entirety.</p>
<p>It was a bit disappointing to read a paper then find out it&rsquo;d been completely dismantled in current deployments, but it&rsquo;s also quite exciting that there&rsquo;s been so much change and progress. I think there&rsquo;s still plenty of value in reading the original Dynamo whitepaper, particularly as it surveys so many design techniques. I&rsquo;ll try to keep the practical optimizations mentioned in mind: they&rsquo;re smaller adjustments that might be easier to slot into existing systems I&rsquo;ll work on developing but may still make a big impact.</p>
<h2 id="day-15-Redis">15. I Mean No (Redis)respect</h2>
<p>Inspired by the <a href="https://notes.ekzhang.com/events/nysrg#:~:text=Jul%207%2C%202024-,Code%20reading%3A%20Redis,-We%E2%80%99ll%20read%20the">NYSRG</a>, a good friend and I spent a couple hours reading through the Redis 1.3.6 codebase. It was much more tractable then than now: as part of paper writing for my formally-verified databases project in Paris, my supervisor asked me to compute the LoC of the Redis core, and the whole project now stands at ~100K LoC of C, with the core at ~25K LoC. However, back in 1.3.6, its core was a single 9K LoC file, <a href="https://github.com/redis/redis/blob/1.3.6/redis.c"><code>redis.c</code></a>. Two hours was enough to learn the <a href="https://architecturenotes.co/p/redis">basics of what Redis does</a>, skim through the code and pick out the key helper libraries, and explore some of its current enterprise extensions.</p>
<p>The big <code>redis.c</code> file is loosely structured as follows:</p>
<ul>
<li><a href="https://github.com/redis/redis/blob/1.3.6/redis.c#L1">L1</a>: Definitions, prototypes, globals</li>
<li><a href="https://github.com/redis/redis/blob/1.3.6/redis.c#L802">L802</a>: Utility functions
<ul>
<li><a href="https://github.com/redis/redis/blob/1.3.6/redis.c#L1118">L1118</a>: Server networking utilities</li>
<li><a href="https://github.com/redis/redis/blob/1.3.6/redis.c#L1427">L1427</a>: Shared object creation to avoid repeated allocations</li>
<li><a href="https://github.com/redis/redis/blob/1.3.6/redis.c#L1532">L1532</a>: Server initialization and config parsing</li>
<li><a href="https://github.com/redis/redis/blob/1.3.6/redis.c#L2072">L2072</a>: Command processing</li>
<li><a href="https://github.com/redis/redis/blob/1.3.6/redis.c#L2429">L2429</a>: Client creation</li>
</ul>
</li>
<li><a href="https://github.com/redis/redis/blob/1.3.6/redis.c#L2587">L2587</a>: Redis object creation, like strings and zsets (ordered sets)</li>
<li><a href="https://github.com/redis/redis/blob/1.3.6/redis.c#L2981">L2981</a>: RDB (snapshotting) saving and loading
<ul>
<li>This is a high-performance way to ensure durable backups.</li>
</ul>
</li>
<li><a href="https://github.com/redis/redis/blob/1.3.6/redis.c#L3703">L3703</a>: Command implementations
<ul>
<li>Redis supports a lot of data types so there are a lot of commands to walk through here! They&rsquo;re all quite intuitive.</li>
<li><a href="https://github.com/redis/redis/blob/1.3.6/redis.c#L6712">L6712</a>: Batch command implementations, a bit further down the file</li>
</ul>
</li>
<li><a href="https://github.com/redis/redis/blob/1.3.6/redis.c#L6608">L6608</a>: Cache expiry and removal
<ul>
<li>There&rsquo;s separate functions for <a href="https://github.com/redis/redis/blob/1.3.6/redis.c#L6638">expiring keys due to TTL</a> and <a href="https://github.com/redis/redis/blob/1.3.6/redis.c#L6655">due to memory pressure</a>.</li>
</ul>
</li>
<li><a href="https://github.com/redis/redis/blob/1.3.6/redis.c#L6981">L6981</a>: Replication helpers
<ul>
<li>Redis replication is eventually consistent and has both full and partial synchronization steps. There&rsquo;re some neat resynchronization helpers here.</li>
<li>Redis replicas are read-only, unless you use Enterprise and get into their Active-Active setup (more below)</li>
</ul>
</li>
<li><a href="https://github.com/redis/redis/blob/1.3.6/redis.c#L7412">L7412</a>: AOF (write-ahead logging) implementation
<ul>
<li>This can allow full reconstruction of data given a crash, so supports better durability and crash corruption avoidance, but has performance overheads</li>
</ul>
</li>
<li><a href="https://github.com/redis/redis/blob/1.3.6/redis.c#L7950">L7950</a>: Virtual memory, a now deprecated feature for <a href="https://redis.io/docs/latest/operate/oss_and_stack/reference/internals/internals-vm/">swapping objects to disk</a></li>
<li><a href="https://github.com/redis/redis/blob/1.3.6/redis.c#L9076">L9076</a>: Main function</li>
</ul>
<p>There are also two other main files: the custom <a href="https://github.com/redis/redis/blob/1.3.6/ae.c">event loop</a> and the backing <a href="https://github.com/redis/redis/blob/1.3.6/dict.c">hash table</a>. Neither is very tricky to read through but they&rsquo;re very clean implementation examples.</p>
<p>A neat thing I learned about the event loop was that it&rsquo;s single threaded. This still admits some <a href="https://redis.io/glossary/redis-race-condition/">race conditions from clients</a> but ensures atomicity for individual commands and makes Redis behaviour easier to reason about. There&rsquo;s some asynchronous I/O for background RDB/AOF file saving, but this is handled by <a href="https://github.com/redis/redis/blob/1.3.6/redis.c#L3355">forking a child process</a> and I assume this is excluded from the &ldquo;single-threaded&rdquo; considerations.</p>
<p>On a whim, I looked up &ldquo;Redis CRDTs&rdquo;, wondering if they might have some experimental datatype support, but I was surprised to find a <a href="https://redis.io/resources/under-the-hood/">whole whitepaper</a>. I didn&rsquo;t read it as it was behind an email paywall, but from <a href="https://redis.io/docs/latest/operate/rs/databases/active-active/">their docs</a>, it seems that they&rsquo;re used to support the geo-distributed replication features for the Enterprise (!) product, allowing clients to both read and write from anywhere in the cluster. The whitepaper page is a bit plastered over with marketing-speak (&ldquo;intelligently resolving conflicts&rdquo;, &ldquo;unprecedented advantages&rdquo;, &ldquo;other techniques such as LWW&rdquo;, which is also a CRDT…) but it&rsquo;d be interesting to learn more about what their implementation looks like sometime.</p>
<p>I didn&rsquo;t get too into the weeds into the details of Redis during my foray, but what struck me was how simple, elegant, and approachable everything was. The most interesting areas for me were the two durability features (RDB vs AOF files) as well as the replication system. I haven&rsquo;t yet had the chance to work with Redis in an internship (surprising given its ubiquity) but I think this initial look will be a good foundation for when the time comes.</p>
<h2 id="day-16-Incremental-Views">16. Incremental Gymnastics</h2>
<p>The <a href="https://en.wikipedia.org/wiki/Frequency_illusion">frequency illusion</a> struck particularly resoundingly recently, this time for the concept of <a href="https://materializedview.io/p/everything-to-know-incremental-view-maintenance">incremental view maintenance</a>. A supervisor mentioned it as a continuation of a research direction, someone else posted a <a href="https://sophiebits.com/2025/08/22/materialized-views-are-obviously-useful">blog post</a> that came up on Twitter, and I stumbled across another reference on <a href="https://www.scattered-thoughts.net/writing/why-isnt-differential-dataflow-more-popular/">someone else&rsquo;s blog</a>. The second time I came across the term, I resolved to look into it some other time; the third encounter bumped it to the top of this reading list.</p>
<p>Incremental view maintenance refers to the fact that materialized views in databases improve performance, but must be efficiently brought up to date themselves. If a view tracks a like count, for instance, it needs to update whenever a new post is made or liked. Materialized views are defined just like a query, so an immediate idea would just be to re-run the query every time any data it references changes. This turns out to be very expensive, however, so folks have been working on smarter ways of computing results based on just the deltas between snapshots or in more parallel ways.</p>
<p>The first search result that comes up for &ldquo;incremental view maintenance&rdquo; is a dense <a href="https://wiki.postgresql.org/wiki/Incremental_View_Maintenance">Postgres implementation proposal</a>, which was a helpful overview of an intuitive solution with change sets. I&rsquo;d recommend reading this to get an easy-to-understand overview of one way to do this. There&rsquo;ve been three other main advances though, rooted in academia though some have become the <a href="https://materialize.com/">basis for infra companies</a>:</p>
<ul>
<li>Timely dataflow, which orders changes linearly by timestamps.</li>
<li>Differential dataflow, which imposes a partial order.</li>
<li>DBSP circuits, which model computation on changes as circuits.</li>
</ul>
<p><a href="https://github.com/timelydataflow/timely-dataflow">Timely dataflow</a> is a dataflow layer that&rsquo;s designed for incremental and iterative work. It tracks records with a special timestamp that pinpoints where a particular task is to know what data is stable and safe to process versus still recomputing and in flux. This time-awareness allows the system to avoid repeatedly recomputing from scratch, and can incrementally update results when later-timestamped data arrives. Time is tracked outside of the dataplane, so tasks get a meta-idea of the global view of time to determine if they can progress. This separation also allows tasks to be computing different points in time, <em>at the same time</em>, enabling more concurrent computation.</p>
<p><a href="https://github.com/TimelyDataflow/differential-dataflow/">Differential dataflow</a> is built on top of timely dataflow, as a slightly higher-level abstraction. Instead of keeping track of a single version of data, it keeps multiple versions in an index, with timely dataflow timestamps used to build a partial order. This allows the system to reuse previously computed values. Differential dataflow is more of a framework (an example of how to use it <a href="https://github.com/frankmcsherry/blog/blob/master/posts/2015-09-29.md">here</a>), whereas timely dataflow is more of a low-level runtime scheduler. Differential dataflow has <a href="https://www.scattered-thoughts.net/writing/why-isnt-differential-dataflow-more-popular/">low adoption</a> given its relative immaturity, but <a href="https://materialize.com/">Materialize</a> is an example of a startup providing a live SQL layer that&rsquo;s based on differential dataflow.</p>
<p>I found the <a href="https://sigmodrecord.org/publications/sigmodRecord/2403/pdfs/20_dbsp-budiu.pdf">DBSP circuits paper</a> most interesting given how concrete the theory was and the clarity of the examples. They develop a theory of database updates as a stream of changes, inspired by digital signal processing techniques. You can apply various operators and functions that are composed in circuits to these streams. They then show how you can rewrite any relational algebra and SQL operators into circuits that operate on these streams (see Table 1) and expand this up to general incremental recursive programs. For instance, you can rewrite a <code>UNION</code> as an <code>+</code> between streams and a <code>distinct</code> operator. This greatly improves the efficiency of view maintenance, since the circuits can give an incremental version of a view&rsquo;s query that returns the change delta. This theory has <a href="https://github.com/tchajed/dbsp-theory">been formalized</a> although this is just barely touched on in the paper. DBSP is the highest-level of these three approaches, constraining time and state management in exchange for a simpler model — as a developer, DBSP circuits most easily match my existing mental models. As for real-world applications, <a href="https://www.feldera.com/">Feldera</a> is a startup focusing heavily on incremental processing based on DBSP.</p>
<p>So far, it seems like these three approaches are the frontier of the field, and there&rsquo;s a very <a href="https://arxiv.org/abs/2404.17679">cutely titled paper</a> addressing additional advances. As you can surmise from the choice of topics for this calendar, this sort of live, reactive, incrementally updating dataflow is fascinating to me on an aesthetic level. I&rsquo;m eager to see where the research goes next, particularly if the Postgres proposal goes anywhere or if these techniques are integrated into any more mainstream databases.</p>
<h2 id="day-17-DBMS-Overviews">17. Timberlake on Databases</h2>
<p>As should be evinced by these calendar entries so far, I have a soft spot for punny and cute titles. I saw <a href="https://people.cs.umass.edu/~yanlei/courses/CS691LL-f06/papers/SH05.pdf"><em>What Goes Around Comes Around</em></a> in the supplemental readings for my DB course, so when I saw <a href="https://db.cs.cmu.edu/papers/2024/whatgoesaround-sigmodrec2024.pdf"><em>What Goes Around Comes Around… And Around…</em></a> I added it to my TBR list with a little extra relish<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.</p>
<p>The former is a chapter in the famous <a href="http://www.redbook.io/">Red Book</a> of DBMS research, which I also need to get through at some point. It was published in 2005, chronologing the major trends in prior databases research and application til that point, from hierarchical data to ER to the XML heyday. Clearly, a lot has changed since then — I saw vestiges of XML only in my early web-dev days, and we joke more about web-scale databases than ORMs nowadays. So, about twenty years later, the latter follow-up paper was published to bring us up to date.</p>
<p>WGACAAA is structured in two parts: one with a series of history-lesson-like summaries of various paradigms, and one on hotter topics in DBMSes. Even at a first glance, these keywords feel more familiar: document DBs, vector DBs, columnar stores, blockchain. Some of my favourite anecdotes mentioned:</p>
<ul>
<li>The term <a href="#day-10-MapReduce">MapReduce</a> is thrown around so often that I&rsquo;d always figured it&rsquo;d persisted, but the paper covers a throwdown between Google and the DBMS community, and the eventual demise of MapReduce and the budding vendors that&rsquo;d grown up around it.</li>
<li>Cloud databases took the compute time-sharing of the late 70s and shared-disk DBMS ideas and made them trendy cash cows again.</li>
</ul>
<p>This was also the first time I&rsquo;d heard of a few concepts, including:</p>
<ul>
<li>Column-family DBMSes (as opposed to columnar data stores), as used in Google&rsquo;s BigTable. These are like document DBs but only allow one level of key nesting along with arrays in values.</li>
<li>Array databases, primarily used in scientific computing. These are challenging due to arbitrary or irregular nesting of array dimensions. This is a reason there are few vendors, and none in the major clouds.</li>
<li>Hardware-accelerated DBMSes, which recently mean using FPGAs and GPUs. It&rsquo;s quite interesting that as early in the 80s vendors were already trying to fiddle with hardware.</li>
</ul>
<p>I learned about some of these DBMS models completely separately from traditional RDBMSes, so it&rsquo;s interesting how the paper ties back each feature to new SQL standard updates. For example, I&rsquo;ve spent some time working with Neo4j and thought the graph traversals were very distinct from a typical SQL use-case, but it turns out SQL:2023 introduced graph traversal support. Array database features can likewise also be simulated by SQL:2022 features. And of course, document DB JSON features are finding their way into SQL systems. The conclusion of the first part of the paper makes a strong case that all mainstream, profitable roads lead back to SQL and RDBMSes.</p>
<p>I like one of the closing lines of the paper: &ldquo;In other words, stand on the shoulders of those who came before and not on their toes.&rdquo; (see above point about cute lines). This might be the first survey paper I&rsquo;ve read, and its balance between cautionary tales and future-looking forecasts is executed very well. It&rsquo;s a fairly recent overview, and thanks to the slower DB research cadence it should remain relevant for a while. It&rsquo;s very easy to digest and provides valuable retrospectives on the historical provenance and controversies in the DB community so I&rsquo;d certainly recommend perusing it. This feels like a fitting way to wrap up this DB section, and it&rsquo;s comforting to see I&rsquo;ve already covered quite a few of the foundational systems mentioned in this overview paper.</p>
<h2 id="day-18-Dapper">18. Without a Trace</h2>
<p>One of my sidequests this summer was trying to improve the performance of some action in a service. I was helpfully pointed to where I could find the traces, only to realize there was something wrong with the spans — they were overflowing their parents and reappearing in odd ways. I ended up dropping that work since I couldn&rsquo;t understand the spans, but I recently figured it&rsquo;d be good to go back to one of the first big distributed tracing papers to see if it might help in the future.</p>
<p>Dapper is a Google paper describing their homegrown distributed tracing setup and how internal teams use it. Its two main goals were to be sufficiently low-level enough to deployable everywhere and to be continuously running for as many services as possible.</p>
<p>They mention that there are two typical types of tracing systems: black-box ones that use statistical regression to link events, and annotation-based ones that have better accuracy in exchange for the development cost of adding annotations. Dapper is based on the latter school of thought, although given the wide range of services deployed at Google, they had unique concerns about where to put this annotation. They ended up annotating events at a very low level in common libraries, like the thread-local storage, the thread scheduler, and the RPC library. This avoids requiring application-specific annotations, although these are also supported.</p>
<p>The base unit of a trace is a span, which has timestamped records and timing data and everything else you&rsquo;d expect. All spans in a trace execution keep a parent span ID and share a globally unique trace ID, which makes setting up the root span a bit more expensive. This span data is written first in local log files, then pulled by the Dapper daemons (what a name) on a periodic basis, then finally written into a relational database, BigTable.</p>
<p>All this ideally occurs within a short few minutes&rsquo; delay to enable more realtime firefighting when issues arise. Indeed, the paper spends a lot of time explaining how they made design decisions to optimize for lowering client overhead and latency. They found trace generation overhead to be negligible, on the order of nanoseconds, and even the expensive local log writing takes place asynchronously in the background with little overhead. I thought it was interesting that they explicitly called out that trace collection and analysis could be turned off more easily, hence are less pressing performance-wise, but they also schedule the processes to be lowest-priority in the kernel.</p>
<p>Spans are sampled twice for latency and storage improvements: once at the service level and once at the BigTable storage level. They used to tune this globally at a sample rate of 1/1024, but were rolling out adaptive sampling at the time of writing. One aspect I didn&rsquo;t see covered in the paper was how much financial, not just performance, cost Dapper imposed. I know Datadog and other tracing SaaSes are extremely expensive even with heavy sampling, so I&rsquo;m curious how much the log retention was given the adaptive sampling.</p>
<p>The paper goes into detail about how the Ads team onboarded to Dapper along with some other anecdotal experiences, but the most interesting use case they mentioned was how Dapper was used for security audits. Application-level annotations were made opt-in explicitly to avoid leaking PII by logging function values in spans. In addition, the span data in BigTable can then be used to automatically verify and enforce security policies: for instance, ensuring services don&rsquo;t unintentionally share private information with one another. I thought this was a super cool usecase and tied in well with the vague aesthetics of what I&rsquo;m researching now.</p>
<p>I found the case studies and future work proposed in the paper quite interesting and applicable too — there&rsquo;s neat ideas for how tracing could be applied in general to other services. My mysterious spans from this summer were no more elucidated, but at least I understand how to implement a such tracing system now.</p>
<h2 id="day-19-Temporal">19. It&rsquo;s About Time</h2>
<p>Temporal is a relatively famous runtime tool that executes functions and workflows durably, guaranteeing that code will keep running even if it crashes midway and abstracting away reliability and recovery work. I&rsquo;d heard about Temporal both from working at Stripe and while at OpenAI — both times I sat next to another intern working on a Temporal-related integration project. I&rsquo;d gotten the gist of <a href="https://temporal.io/blog/what-is-durable-execution">&ldquo;workflows are durable&rdquo;</a> but had always wondered how exactly it made good on its code replayability promises.</p>
<p>Temporal consists of a cloud service, operated either on-prem via their open source platform or by them, as well as worker processes. The worker processes actually run the functions, and the Temporal server choreographs these executions. One of the main Temporal abstractions is Workflows, which manage their own local state, execute deterministically, and can communicate with other Workflows via message passing. The core promise of Temporal is that Workflows are run exactly once and to completion, even if this takes years and in the face of extensive network or server failures.</p>
<p>The other main abstraction is the concept of Activities, which are normal function executions that complete a single, well-defined task, like fetching from the database or so on. Activities may be retried repeatedly, requiring idempotency keys for side effects. There can be code outside of Activity steps, but anything that might fail or otherwise requires interaction with the Temporal Service needs to be in an Activity. When Activities complete, the worker sends the result back to the Temporal service in <a href="https://docs.temporal.io/workflow-execution/event#activity-events">an event</a>.</p>
<p>It&rsquo;s these events that build up an <a href="https://docs.temporal.io/workflow-execution/event#event-history">Event History</a>, which is durably persisted, presumably in the DB that accompanies the Temporal service. Besides tasks finishing, many other lifecycle events are recorded in events and logged. This Event History can then be <a href="https://docs.temporal.io/encyclopedia/event-history/event-history-python?utm_source=chatgpt.com#How-History-Replay-Provides-Durable-Execution">replayed</a> to restore execution state in case of failures. Temporal considers workers (and hence the Activity tasks they&rsquo;re running) to have failed if they time out, and will automatically retry it from that point.</p>
<p>To continue executing code, the new worker requests the Event History from the Temporal Service. It can rerun code directly, since Workflows are deterministic, so they don&rsquo;t actually have to store or track state anywhere fancy. This deterministic piece is key to <a href="https://docs.temporal.io/encyclopedia/event-history/event-history-python?utm_source=chatgpt.com#Example-of-Non-Deterministic-Workflow">ensure new executions match existing event history</a>. When the new worker reaches an already-executed Activity, it can look back in the Event History to find the task&rsquo;s completion event and directly use its result, instead of enqueuing a new Activity task to the Temporal Service. As the docs emphasize, the Worker thus doesn&rsquo;t <em>re-execute</em> the Activity, so there&rsquo;s no way execution results can differ on this new worker. When the new worker encounters an Activity that wasn&rsquo;t completed (i.e. the site of the crash), it can start executing code live and enqueue new Activity tasks.</p>
<p>Temporal surfaces its internals less prominently than other dev infra companies I&rsquo;ve written about here. I had to go digging quite far in the docs (on a language-specific SDK page!) to get to a satisfactory level of detail regarding how it maintains durability, for instance. While I think Temporal could do a better job at centralizing and clarifying their design deep dives, it&rsquo;s already so popular that this wouldn&rsquo;t help with adoption. Swyx, the former Head of Developer Experience also articulates that Temporal is intended to be an <a href="https://www.swyx.io/why-temporal">iPhone moment</a>, so maybe they&rsquo;re explicitly not optimizing for people to look at their internals and instead focusing on the experience and ubiquity. (I actually recommend this iPhone post as a precursor to reading the Temporal docs since it builds better intuition about how you&rsquo;d naturally come to a similar solution.)</p>
<p>It was quite neat to finally learn about a platform I&rsquo;d heard so much about from fellow interns. I&rsquo;d like to build something with Temporal one day just to feel the magic myself — hopefully I&rsquo;ll have a chance with an upcoming class project soon.</p>
<h2 id="day-20-DBOS">20. Who&rsquo;s D Boss?</h2>
<p>This last summer in the Bay, I was finally able to attend the mythical SF and South Bay Systems meetups, events I&rsquo;d followed for a while from the sidelines. I managed to go to the South Bay Systems meetup where Andy Pavlo spoke, and I noticed one of the organizers was wearing a DBOS shirt. My friend (another organizer) had mentioned DBOS offhandedly to me in DMs a while back as well. The original vision sounded incredibly interesting: a database-based operating system where you can freely log and inspect system state as just another table would be a field day for Quantified Self, local-first, and malleable software folks alike. Now, DBOS has evolved from its research roots into a cloud company that supports durable functions as a service.</p>
<p>If you read yesterday&rsquo;s post on Temporal, you&rsquo;ll see why I grouped these two together. Luckily, DBOS folks have put together a helpful couple paragraphs to differentiate the two, which I think is useful to get a better sense of what DBOS does in terms of what we already know about Temporal. Both provide durability, but Temporal is a framework that requires re-architecting your application around its concepts of Activities and Workflows, whereas DBOS enables drop-in annotations. DBOS is limited to Postgres only and fewer languages than Temporal supports, but their <a href="https://www.dbos.dev/blog/durable-execution-coding-comparison">blog post describing the differences</a> is quite compelling. In particular, they mention how durable code is usually at the heart of business applications and hence more expensive and risky to refactor or segregate out. The library approach that DBOS champions seems easier to integrate and avoids depending on yet another orchestrator service.</p>
<p>The DBOS library provides abstractions for workflows, queues, and messaging, backed by a Postgres database, described <a href="https://docs.dbos.dev/explanations/system-tables">here</a>. Workflows are annotated in existing code and checkpointed into Postgres (<code>dbos.workflow_status</code>). As with Temporal, workflows are required to be deterministic; DBOS workflows also need to be idempotent, whereas Temporal merely suggests it. Workflows can receive messages in stream topics. Workflow recovery is similar to Temporal: inputs and outputs are stored in the DBOS Postgres and restarted with old inputs/outputs until it reaches a step with no checkpoints, hence continuing execution normally after the point where the system crashed. This poses interesting issues when the code needs to change, as inputs and outputs may be malformed for new versions, so DBOS also tracks application version and only recovers workflows with matching versions, otherwise putting the burden on developers to manually remap tasks safely.</p>
<p>DBOS also enables durable queues of workflows. Its implementation is unique: instead of there being separate worker servers, as in Temporal, all application servers can poll for workflows. One key point is that these application servers would be owned by you, as opposed to the separate, Temporal-controlled worker servers. Recovery from these distributed application servers proceeds automatically, detecting when executors may be unhealthy and enqueueing tasks to another server.</p>
<p>Take a brief look through the <a href="https://docs.dbos.dev/explanations/system-tables">Postgres schema</a> if you haven&rsquo;t already — it&rsquo;s quite intuitive and there&rsquo;s not anything surprising. This is actually how I found out about the message streaming communication feature in the first place, as I hadn&rsquo;t seen it in the architecture overview.</p>
<p>To be honest, I was slightly more drawn to the grandiose hopes of a database-oriented OS, but perhaps the current DBOS Cloud offering is a more realistic entryway onto that path (or perhaps just a good funding source for continuing more experimental research). For those interested in other durable workflow runners, I have few links besides the Temporal writeup I did yesterday; for those interested in other speculative &ldquo;OS&rdquo; ideas, on the other hand, I quite like <a href="https://alexanderobenauer.com/">Alexander Obenauer&rsquo;s lab notes</a> and the very aesthetic <a href="https://www.mercuryos.com/">MercuryOS by Jason Yuan</a>.</p>
<h2 id="day-21-Waldo">21. Where&rsquo;s Waldo?</h2>
<p>I came across the <a href="https://eprint.iacr.org/2021/1661.pdf">Waldo paper</a> when I was working on <a href="https://kewbi.sh/blog/posts/250223/">Kintsugi</a>: I&rsquo;d read <a href="https://cs.stanford.edu/~edauterman/">Emma Dauterman</a>&rsquo;s <a href="https://www.usenix.org/system/files/osdi20-dauterman_safetypin.pdf">SafetyPin</a> paper quite extensively while doing my literature search, so I&rsquo;d seen Waldo but not gotten around to reading it yet. The combination of databases and secret sharing caught my eye. Aesthetically, I&rsquo;m drawn to odd experimental databases, and the promise of privacy that was fast enough to approach practical usage was quite alluring.</p>
<p>Waldo is a time series database that protects both the data it stores and the queries that operate on it. Something like this would be applicable in many sensitive fields, and they mention use cases like health data where even queries can reveal PII/private information about a patient. Two more obvious ways to approach this would be to use <a href="https://en.wikipedia.org/wiki/Oblivious_RAM">oblivious RAM</a> or generic multi-party computation techniques, but their latency and cost make them impractical solutions. Waldo instead creatively applies function secret sharing (FSS), expressing the private data into data structure and using FSS to operate queries with multiple parties. Instead of thinking of a <code>SELECT</code> as a SQL query, for example, we can express it as a function that returns 1 for certain input values, and this more mathematical rendition is what&rsquo;s shared between nodes. Waldo also supports more index types than prior work: additive aggregates over multiple predicates and arbitrary aggregates over a time interval are both basic features in typical databases that Waldo is also able to provide.</p>
<p>If you just take FSS for granted and squint through the rest of the paper, the building blocks are themselves already really neat. For example, FSS usually deals with public data, since the servers evaluating the function need the same input. Instead Waldo splits the data up structurally into a one-hot index encoding per feature. Each possible data value gets a row, with the entry (x, y) having value 1 if the record x had data value y. Each entry gets multiplied by the FSS evaluation, which is 0 if it&rsquo;s not at the right row anyways, so in the end Waldo can hide the data value as well, with the added upshot of cheap appends. To combine multiple predicates, Waldo takes vector FSS outputs, which can be more easily combined with another predicate&rsquo;s results. To support arbitrary predicates, Waldo lets the user define an aggregation tree that itself is FSS&rsquo;ed to hide both data and query.</p>
<p>However, I thought it&rsquo;d be a good chance to build on my prior forays into secret sharing to also learn about FSS. <a href="https://eprint.iacr.org/2018/707">This paper</a> introduces the idea. While I&rsquo;d expected FSS to be for sharing typical polynomial &ldquo;math&rdquo; functions, for which you can simply Shamir-secret-share the coefficients of the polynomial, it turns out the paper focuses on distributed point functions <code>f_α(x)</code>, which are 0 everywhere except a point (α, β). Instead of doing the typical Lagrange interpolation, they instead turn to a tree model. A dealer generates <code>n</code> keys, <code>k1</code> through <code>kn</code>, and for the FSS evaluation function <code>Eval</code>, <code>Eval(k1, x) ⊕ … ⊕ Eval(kn, x) = f_α(x)</code>. Each key has a root seed which determines the structure of a pseudo-randomly generated tree. Each key&rsquo;s tree is the same, except along the path α, letting the final ⊕ operation produce a value at α. To make these trees agree on all other paths, there&rsquo;s an XOR mask, called a correction word, generated by the dealer to fix the other paths. These correction words are also stored in the keys. The pseudo-random generation makes the tree (and thus key) look random, but with all the shares the function can be evaluated. You can use this same tree idea to construct decision trees (i.e. if conditions) that can be privately evaluated as well, which I assume are what Waldo uses behind the scenes. I skimmed over the rest of the FSS math in the paper for how the DB features are implemented because I didn&rsquo;t quite have the capacity to wrangle more numbers in my head, but I do want to come back to it sometime!</p>
<p>I thought Waldo was a super interesting idea, and I spent some time afterward going down a rabbithole of NDSS presentations on FSS being applied in other privacy-aware contexts. I&rsquo;m eager to see if private databases like Waldo gain popularity as folks start wanting to protect their personal data from ubiquitous LLM agents, or if people in general trust labs to get things right.</p>
<p>I think there was a gap in my background that made the details in the paper a bit harder to approach: like most crypto-adjacent papers it&rsquo;s quite dense and even the <a href="https://www.youtube.com/watch?v=ygUZ61JZEE4">overview video</a> left me a bit confused. I was drawn in by the FSS application yet had to wrestle with the intuition for it, so ultimately I don&rsquo;t think I got as much out of this paper as I&rsquo;d have liked. I want to read more on oblivious RAM and MPC (the two infeasible techniques) at some point to contextualize current work, for instance. As well, I&rsquo;d might try to implement some FSS stuff: it took me at least a week to really get my head around <a href="https://eprint.iacr.org/2022/971.pdf">dynamic proactive secret sharing</a> so perhaps FSS will come more naturally after some focused time.</p>
<h2 id="day-22-Parfait">22. Pass the Yogurt</h2>
<p>Every so often, it&rsquo;s fun to poke around various distributed research labs&rsquo; latest papers. It&rsquo;s a direct reflection of what a particular microcosm of researchers are thinking most about, and it&rsquo;s neat to start to trace interests together. This is how I found this paper and the next: both caught my attention because of the interesting formal twists they proposed. Today, I&rsquo;ll be discussing <a href="https://pdos.csail.mit.edu/papers/parfait:sosp24.pdf">Parfait</a>, a collection of frameworks and a modular approach for verifying HSMs; tomorrow&rsquo;s paper covers a framework to verify distributed systems.</p>
<p>Parfait is a stack of tools, that when chained together, ensures that information doesn&rsquo;t leak from a supposedly secure system using secure hardware like HSMs. For example, there might be minute timing differences in computation that could leak information about the data a program is processing. Existing leakage models are either not end-to-end from software all the way down to registers and hardware, or don&rsquo;t scale the iterative refinement process well.</p>
<p>Parfait uses a host of different proof tools and languages: its five stages are an app specification in <a href="https://fstar-lang.org/">F*</a>, then a verified app implementation in <a href="https://dl.acm.org/doi/10.1145/3110261">Low*</a>, which compiles via the KaRaMel compiler to C, after which the verified CompCert compiler generates the assembly, which is finally proved by physical simulation. The hardware is also Verilog-verified. These stages each can catch certain classes of bugs: for instance, software logic bugs are caught at the Low* level, and timing attacks can be prevented at the last hardware stage. Parfait is made up of two sub-frameworks that encompass these stages: Starling comprises the software steps, and Knox2 takes care of the hardware verification. Here, only the app spec and the eventual client driver are part of the trusted computing base.</p>
<p>Between these stages, equivalence is proved using <em>information-preserving refinement</em> (IPR). The IPR from F* to Low* is done via a lockstep proof, and the IPR from assembly to hardware is via proving the existence of an observationally equivalent mapping. This stack seems convoluted, but lets developers leverage existing tools and codebases, like <a href="https://github.com/hacl-star/hacl-star">HACL*</a> for formally-verified cryptography. It also makes Parfait more realistically applicable rather than relying on custom tooling every step of the way. The examples of each of these proofs and their mappings are very concrete. I thought they got a bit repetitive, but maybe I&rsquo;m not the right audience because I just trusted it would work.</p>
<p>I liked the case studies they showed for ECDSA signing and password hashing, based on two different processors. The developer-time arguments seem very convincing: both prototypes are implemented in very few lines of F* and were ported to different platforms in a couple of hours. The verification time also seems more or less reasonable — I liked the practical note about reducing loop bounds as a way to reduce verification time.</p>
<p>I also wanted to call out the graphs and figures in this paper! It&rsquo;s clear a lot of time and thought went into them, as they were laid out very orderly with color-coded matching captions to boot. I&rsquo;m also just a sucker for nice diagrams with consistent colors and fonts, especially across figures in the same paper. This is certainly something I&rsquo;ll aspire to in future work.</p>
<p>Overall, this was a very interesting read, especially on the framing level, and I learned a lot presentation-wise from this paper. We just received the reviews back for a recent paper submission and there&rsquo;s a lot of inspiration I&rsquo;ll draw from how the verification correctness and performance arguments are made here in our next submission.</p>
<h2 id="day-23-Grove">23. Miss The Grove For The Trees</h2>
<p>Today is another MIT PDOS verification paper — a completely different vein than yesterday but just as interesting! <a href="https://pdos.csail.mit.edu/papers/grove:sosp23.pdf">Grove</a> is a Rocq library for verifying Go distributed systems at their implementation level in a modular, easy-to-reason-about way. In particular, it focuses on time-based leases and the concept of ownership, which prior verification libraries don&rsquo;t yet support. For example, Grove can help verify read consistency with lease-based systems that avoid coordination, ensure the durability and crash recovery of systems, and check that reconfiguration correctly waits for leases to expire.</p>
<p>Grove is based on concurrent separation logic (CSL), which is a very intimidating term to just mean thread-based logic that treats subsystems as modular black boxes. This helps &ldquo;separate&rdquo; concerns — one doesn&rsquo;t have to worry about the other concurrently executing code to prove invariants on the current thread. Some core concepts include assertions, which ascribe ownership onto something (e.g. this thread has control over this object) and thus enable modularity because they limit the layer of concern to just this thread. As well, there are ghost resources, which can span multiple nodes, providing a higher level of abstraction, but can be owned by specific nodes. An example of this might be the epoch object, which can be read by all nodes but is owned by a central reconfiguration service. CSL involves proofs that take some input state, apply a specific function, then assert some output state, which makes it a natural fit for verifying implementation-level concerns.</p>
<p>I think it was a bit unclear what Grove was at first, without having read some of the other PDOS verification papers and without knowing what CSL was. I understood that Grove was a &ldquo;library&rdquo;, but as an implementation-driven person I was wondering if it was a Go import, a new DSL, or some new IDE. It becomes clearer only near the end of the paper in the implementation section that Grove is indeed a Rocq library. It&rsquo;s built off of their prior <a href="https://github.com/mit-pdos/perennial">Perennial</a> library, which is itself built on <a href="https://github.com/goose-lang/goose">Goose</a>, which converts a subset of Go to Rocq for verification. Maybe it&rsquo;s obvious from context if one has a bit more verification background, but I was a bit lost initially.</p>
<p>I found the library&rsquo;s CSL abstractions quite interesting, though I admit I skimmed over them. Grove supplies special per-node assertions, network and file content resources, and time-bounded invariants for lease assertions. In general, I thought their explanation of the &ldquo;separation&rdquo; bit and the composability was compelling: proven ownership enforces well-behaved code, since a thread can&rsquo;t make a mistake and trigger some other unintended behaviour in another thread via a shared RPC.</p>
<p>Their demo system, vKV, was also quite impressive. It requires about the same amount of proof-to-code as other prior work. It&rsquo;s also very high performance: 67-73% of Redis, which is crazy for verified code. I like that they specifically mentioned how implementing leases into the library and the system improved performance. For instance, read only operations could then bypass expensive run-only-once code — an optimization I&rsquo;ve used in my own prior work.</p>
<p>I had two other main questions while reading the paper:</p>
<ul>
<li>Why not just use TLA+? I&rsquo;m regularly surrounded by TLA+ folks and have been working somewhat adjacent to the ecosystem for a while so naturally thought of it when I saw &ldquo;verifying distributed systems&rdquo;.
<ul>
<li>The paper points out, though, that Grove aims to catch implementation-level bugs, not just the abstract modelling-level ones TLA+ is usually used to reason about. I&rsquo;d been thinking about <a href="https://www.cs.ubc.ca/~bestchai/papers/asplos23-pgo.pdf">PGo</a> so long I forgot we had the implementation translation to worry about.</li>
</ul>
</li>
<li>Why are liveness properties not supported? This is explicitly called out in the paper but alas it was never explained why it was out of scope.
<ul>
<li>Maybe they had Grove research separate from their <a href="https://people.csail.mit.edu/nickolai/papers/leung-shipwright-arxiv.pdf">other work on liveness</a>, and only started the liveness piece more recently?</li>
</ul>
</li>
</ul>
<p>Just like <a href="#day-22-Parfait">Parfait</a>, I liked some presentational aspects of the paper, particularly how they present modularity and composability. The intro has specific callouts for different audiences (systems vs. verification folks) and how they could get value from the same paper. It was nice framing that helps with papers like this that straddle both fields, since each&rsquo;s norms and lenses are a bit different. It would have been nice to interleave some Grove excerpts earlier on and clarify what it was to the unwitting reader, but I also appreciated how they had motivating examples and built intuition up slowly upfront. Overall, plenty of inspiration to be drawn from the writing style of Grove as well!</p>
<h2 id="day-24-Fizzbee">24. The (Fizz)bee&rsquo;s Knees</h2>
<p>In my past research, I&rsquo;ve tended to work adjacent to formal verification (implementing a specification, writing libraries for specs) while never actually doing any of it. I&rsquo;ve tried my best to pick things up over time, but specification languages like TLA+ still seem verbose and intimidating, far removed from the imperative code I&rsquo;m used to writing. <a href="https://ahelwer.ca/post/2025-07-04-tla-contracts/">Andrew Helwer</a> wrote a post about how companies can see the value of TLA+ but still not want to invest in engineers themselves working on the specs or to continue iterating in-house after a contract&rsquo;s over. So it seems existing formal methods tooling is scary, hard to pick up, and difficult to truly integrate into the development cycle. Fizzbee, a Python-inspired specification language to visualize and test distributed systems, aims to make formal verification more accessible.</p>
<p>I&rsquo;d first heard of Fizzbee via LinkedIn: for some reason, its creator <a href="https://x.com/jayaprabhakark">JP</a> had sent a connection request. I have a policy of declining connection requests unless I directly know the person, so I declined as I hadn&rsquo;t heard of JP at the time (sorry). The formal verification mention in their headline caught my eye, though, and I remember clicking through to the Fizzbee website for a skim. This last summer, I saw Fizzbee mentioned again at a couple of Bay Area systems meetups, so I resolved to include it when building this reading list.</p>
<p>At first glance, the Fizzbee syntax and <a href="https://fizzbee.io/design/tutorials/quick-start/">quickstart guide</a> feel refreshing. There&rsquo;s a certain mental barrier that comes with reading through TLA+ disjunctions and fairness operators: it&rsquo;s like my eyes are glazing over of their own accord. But Fizzbee&rsquo;s syntax is recognizable and intuitive, reading like plain Python. The website is also designed cleanly, which beats having to page through a scan of Specifying Systems.</p>
<p>Fizzbee has a couple main concepts, including roles, actions, functions, and invariants. Roles seem a bit like classes and can encapsulate ephemeral state. Actions are intended to represent external triggers, while functions are called by actions or other functions. Both are vaguely reminiscent of TLA+ (maybe I just can&rsquo;t think of anything else when I see <code>Init</code>) but read more like Python functions. The language supports something like list comprehensions with a <code>for</code> syntax, but requires an <code>atomic</code> keyword to prevent state space explosion from the loop iterations. The invariant definitions can be used to specify liveness and safety properties as expected. In particular, the fairness keywords seem nicely intuitive: instead of <code>&lt;&gt;[]</code> we have <code>eventually</code> and <code>always</code>. Each Fizzbee spec also has some frontmatter that can be used to constrain state exploration, like limiting the maximum number of executions of an action. It seems like some care needs to be taken when writing specs to avoid explosion: there are plenty of warnings and information panels littered throughout the guide.</p>
<p>JP has an interview with Chris Riccomini on <a href="https://materializedview.io/p/fizzbee-tla-and-formal-software-verification">Materialized View</a>, where he makes a similar point as Helwer above: no one really wants to read TLA+ because it&rsquo;s too elusive. Even if you can convince your team (and yourself) to go through the upfront modelling, it&rsquo;s rare for enthusiasm to stay high and for specs to stay relevant given the high mental load. I think this is where Fizzbee&rsquo;s similarity with Python really shines. This is great for adoption, since the amount of new syntax you have to learn is limited, and the spec can stay relatively close to the code. If I&rsquo;m writing a spec, thinking about system properties and invariants is draining enough; I don&rsquo;t want to have to remember if <code>&lt;&gt;[]</code> or <code>[]&lt;&gt;</code> was the fairness property I intended. This is touched on a bit in the interview, but I wonder if it&rsquo;ll be easier to get LLMs to help with writing specifications given an implmentation, since models likely have plenty of Python training data but lack context for TLA+ or other more niche tools. On the flip side, I also wonder if the slight differences from Python will trip LLMs up in an ironic case of the uncanny valley.</p>
<p>I also wanted to highlight Fizzbee&rsquo;s interactive tooling, which also seem helpful for adoption. Fizzbee can automatically generate sequence diagrams and system visualizations that can be stepped through incrementally. This makes it easier to articulate traces and counterexamples instead of trawling through a TLC log. All this happens in a web playground so users don&rsquo;t have to futz around with TLC and Java. Some web tooling for TLA+ exists, like <a href="https://will62794.github.io/spectacle">Spectacle</a>, but I really appreciate how visualization is a first-party concept for Fizzbee.</p>
<p>Fizzbee&rsquo;s really impressive work for what seems to be a solo dev. The first time I read the landing page wasn&rsquo;t that long ago, but it still seems like a lot of progress was made. I&rsquo;m eager to see what features crop up next and how adoption plays out — perhaps I&rsquo;ll give it a try myself for the next systems paper I read.</p>
<h2 id="day-25-My-Research">25. Look Mom, No Hands</h2>
<p>I thought I&rsquo;d wrap up this calendar by briefly shilling some of my own recent research. It&rsquo;s a bit lighter reading and a good opportunity to checkpoint my progress.</p>
<p>Earlier this spring, I&rsquo;d settled in on my bachelor&rsquo;s honours thesis idea. My <a href="https://fhackett.com/">PhD mentor</a>&rsquo;d been working for a while on <a href="https://www.cs.ubc.ca/~bestchai/papers/asplos23-pgo.pdf">PGo</a>, a tool to automatically compile deployable distributed systems from formal specifications in a dialect of TLA+. He&rsquo;s also been working on a related <a href="https://www.cs.ubc.ca/~bestchai/papers/oopsla25-trace-link.pdf">TraceLink</a> project to map execution traces to the underlying model, which ensures neither deviates from the other. This work gets us a &ldquo;model → code → traces&rdquo; pipeline. My thesis was meant to close the loop by turning traces and code into formal specifications. These could then be compiled, and we&rsquo;d get this virtuous cycle of proven-correct nirvana.</p>
<p>At a first glance, this seems like an intractable synthesis problem. With traditional static techniques like <a href="https://docs.racket-lang.org/rosette-guide/index.html">Rosette</a> or static slicing through traces, this would be. But it&rsquo;s 2025, NeurIPS and ICLR are exploding at the seams, and I&rsquo;d be interning at a large American AI startup. The solution, of course, would be to throw LLMs at it and see what stuck. I jest, but not really. The model checker or proof assistant can indicate whether our generated spec is internally consistent, and with trace validation we&rsquo;ll know that we&rsquo;re not hallucinating orthogonal to the codebase too. This seems like a good start.</p>
<p>Verification is a really interesting domain for me because of the rigor and practical benefits that formal guarantees supply. These boons are generally recognized by companies big and small, who agree that formal methods would be great if only they didn&rsquo;t require so much manpower<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>. This seems like a good place to be in: we&rsquo;re limited by effort and ROI calculations, not for want of demand. Supposing we manage to significantly reduce the cost and burden of maintaining formal specs alongside code, it seems like even casual, curious adoption would see an uptick. Even putting aside the question of if we <em>should</em> specify software and how much it&rsquo;d <em>really</em> help, making verification a viable option to consider is a clear net positive. Right now, it feels a bit like this academic luxury, bottled AOC in eastern France and Switzerland: I think there&rsquo;s low hanging fruit to improve accessibility.</p>
<p>With all these grand motivations in mind, I set out to implement an agent and understand the current SOTA of TLA+ capability. By September, I was ready to triumphantly present my shoddy loop to my supervisor as a first step. The slight catch: someone else had done it already, and done it better. Over the summer, the <a href="https://foundation.tlapl.us/challenge/index.html">TLAi+ Challenge</a> had been taking place, and unbeknownst to us, a team from UIUC and NJU had won first prize for an agent to convert source codebases into TLA+ specs with thoughtful integrations of existing tools. Rats.</p>
<p>This story ends happily ever after, though. My supervisor sent an email regarding collaborations, and a week later we found ourselves in a Zoom call chatting about next steps. We&rsquo;ve been making progress based on their initial submission, <a href="https://github.com/specula-org/Specula/">Specula</a>, and we&rsquo;re currently refactoring it to provide better feedback to the agent. We&rsquo;ve also recently put out a benchmark, SysMoBench, measuring stock LLMs on their ability to produce syntactically correct and runtime-meaningful specs. The TL;DR is that they don&rsquo;t do very well, even just considering the issue of generating valid syntax, and there&rsquo;s still a ways to go before generating trace-validable specs becomes commonplace. You can see a preview of this work <a href="https://arxiv.org/abs/2509.23130">on arXiv</a>. I&rsquo;m really excited by this line of research and hope to have more results to share by the time I submit my thesis.</p>
<p>Anyways, this concludes this edition of my Advent CAPlendar (if you&rsquo;ve missed a post, consider checking out the others <a href="#calendar-grid">here</a>). I&rsquo;ve quite enjoyed getting exposed to so many interesting papers and ideas through this project. I hope to be able to keep up the habit, if a bit less intensely, throughout the rest of next year: compiling these posts has already given me so many new branching off points and further readings. We&rsquo;ll see how high my TBR papers list stacks up, and if I&rsquo;ll have a more focused line of inquiry, but for now I&rsquo;m planning to do another calendar next year. Happy holidays, and til then!</p>


<script>
for (let i = 1; i <= 25; i++) {
	const posts = document.querySelectorAll(`[id^=day-${i}]`);
	let isMystery = false;
	let title = "";
	let id = "";
	if (posts.length == 0) {
		isMystery = true;
		title = "—";
	} else {
		const post = posts[0];
		id = post.id;
		title = post.id.replace(`day-${i}-`, '').replaceAll("-", ' ', );
	}
	const element = document.createElement("div");
	element.classList.add("date");
	element.classList.add("grid-element");
	if (isMystery) {
		element.classList.add("date-mystery");
	}
	const number = document.createElement("p");
	number.classList.add("date-number")
	number.textContent = i;
	element.appendChild(number);
	const titleEl = document.createElement("p");
	titleEl.textContent = title;
	element.appendChild(titleEl);
	const parentWrapper = document.createElement("div");
	parentWrapper.classList.add("date-wrapper");
	if (!isMystery) {
		const wrapper = document.createElement("a");
		wrapper.href = "#" + id;
		wrapper.appendChild(element);
		parentWrapper.appendChild(wrapper);
		document.getElementById("calendar-grid").appendChild(parentWrapper);
	} else {
		parentWrapper.appendChild(element);
		document.getElementById("calendar-grid").appendChild(parentWrapper);
	}
}

document.getElementById('email-form').addEventListener('submit', async (e) => {
    e.preventDefault();
	const formEl = document.getElementById("email-form");
	const form = new FormData(formEl);

	fetch("https://courriel.kewbish.workers.dev/campaign/advent25/signup", {
	  method: "POST",
	  body: form
	});
	alert('Signed up!');
	document.getElementById('email').disabled = true;
	document.getElementById('email-submit').disabled = true;
});
</script>


<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>After spending a while doing infra work, it was refreshing to go back to my frontend roots and put this together! I learned some new tricks: did you know <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_counter_styles/Using_CSS_counters">CSS has counters</a>? Did you know you can <a href="https://www.markdownguide.org/extended-syntax/#heading-ids">set a separate header ID</a> or that you can <a href="https://stackoverflow.com/questions/27255591/using-forward-slash-as-id-attribute">have slashes in them</a>?&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>A mentor&rsquo;s nickname for the larger community of researchers, typically in Europe, working on CRDTs and sync technology.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>There&rsquo;s a joke in here somewhere about the Indian boar romping about on the cover of <em>Designing Data-Intensive Applications</em> and zookeepers needing to wrangle them…&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>See other examples of Andy Pavlo&rsquo;s banger titles on his <a href="https://www.cs.cmu.edu/~pavlo/publications.html">papers page</a>.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>See <a href="https://www.galois.com/articles/what-works-and-doesnt-selling-formal-methods">this post</a> and <a href="https://ahelwer.ca/post/2025-07-04-tla-contracts/">this post</a> to get a sense of adoption issues from folks much closer to the &ldquo;convincing people to do TLA+&rdquo; side.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>Cité des Dômes et du Design (EN)</title>
      <link>https://kewbi.sh/blog/posts/250420/</link>
      <pubDate>20 Apr 2025</pubDate>
      
      <description>On exploring Paris.</description>
      <content:encoded><![CDATA[

<div class="grid-element" style="margin-bottom: 0.5em">
    Salut ! La version originale en français est disponible <a href="https://kewbi.sh/blog/posts/250413/">ici</a>.
</div>


<h2 id="introduction">Introduction</h2>
<p><em>This post was originally written in French and has been translated back to English with slight edits, so doesn&rsquo;t quite flow as naturally as my other writing.</em></p>
<p>I&rsquo;m currently in Paris, doing an internship at Sorbonne Université. Since I landed the internship, I&rsquo;ve been really looking forward to settling into a new country and research environment, and I&rsquo;d wanted to improve my French. Fun fact: I&rsquo;m minoring in French at UBC, so I&rsquo;m supposed to be able to speak more or less fluidly (spoiler alert: I can&rsquo;t.)</p>
<p>I didn&rsquo;t know anyone when I arrived in Paris. I&rsquo;ve always been an independent type of person, so I made the most of moments alone in new places and museums. I spent many weekends flâneuring around the city. I&rsquo;m from Vancouver, where there aren&rsquo;t many old buildings, so I was always astonished by what I saw. For example, the library where I occasionally worked was older than Canada — this was crazy to me! I walked all around, and I found magnificent architecture: sculptures above doors, Corinthian columns, and the most interesting of all, domes.</p>
<p>I explored by looking at the horizon and looking for interesting spires. I learned to find my way around by the domes and other landmarks of Paris. If I saw a gold spire, I would know that it was the dome of the Hôtel des Invalides. If I knew that I was around the 5th arrondissement, I would know that I was close to the Panthéon. It was a bit of a game: which historic places could I identify?</p>
<p>But, listen, when one is in the middle of tens of domes, they start to all look alike. I wanted a checklist: if there was gold, it&rsquo;d be the Invalides; if the dome was straight and small and beside the Seine, it&rsquo;d be the Institut de France, and so on. I looked for such a list, and was surprised to find <a href="https://en.wikipedia.org/wiki/List_of_d%C3%B4mes_in_France">this one</a> on Wikipedia (ironically only available in English, but it was just a list of French names). It was rather funny that someone&rsquo;d taken the time to write it up, but it was still insufficient to efficiently identify domes. So, I decided to write this post to describe them and share a bit of the stories that I&rsquo;d learned during my stay here.</p>
<p>About the title of this post: I noticed that in Paris, there&rsquo;re many programs and buildings that are named like « Cité de &hellip; ». There&rsquo;s the website <a href="https://www.cite-sciences.fr/fr/au-programme/lieux-ressources/cite-des-metiers">Cité des métiers</a>, and I used to pass the <a href="https://www.citemodedesign.fr/fr/">Cité de la Mode et du Design</a> in the bus every day. According to ChatGPT, this is because « cité » indicates the idea of a central space specialized in something — I don&rsquo;t entirely believe it, but it might make some sense. In short, I decided to call this post « Cité des Dômes et du Design » as a little nod, because Paris really is a city specialized in magnificent domes.</p>
<h2 id="panthéon">Panthéon</h2>
<p>I think the Pantheon is the most well-known dome — it&rsquo;s in the 5th arrondissement, close to the Jardin du Luxembourg. It&rsquo;s where we&rsquo;ve buried the great men and women of France, like Victor Hugo, Voltaire, and Marie Curie. It&rsquo;s very big, so it&rsquo;s easy to see and a good landmark to make your way around in Paris.</p>
<figure><img src="https://upload.wikimedia.org/wikipedia/commons/8/80/Pantheon_of_Paris_007.JPG"
         alt="Figure 1. Panthéon (Moonik, CC BY-SA 3.0, via Wikimedia Commons)"/><figcaption>
            <p><em>Figure 1. Panthéon (Moonik, CC BY-SA 3.0, via Wikimedia Commons)</em></p>
        </figcaption>
</figure>

<p>The Panthéon was built by Louis XV, who had sworn that he would renovate the church formerly on those grounds if he survived a serious illness. He recovered, so he charged <a href="https://en.wikipedia.org/wiki/Jacques-Germain_Soufflot">Soufflot</a> with architecting a new church. The Panthéon was first a church, then during the Revolution, a secular space where mass no longer took place, then again an active church, and is finally a secular space to bury important folks. It&rsquo;s dedicated to Saint Geneviève (the library facing the Panthéon is also named after her) and the walls are painted with works depicting her life, from her pious childhood to her death. There&rsquo;s also a series of paintings dedicated to Joan of Arc and several others for other saints.</p>
<p>I&rsquo;d recommend taking the time to lift your gaze and get lost in the decoration of the roof. It&rsquo;s full of details and paintings: my favourite are those under the main dome where the pendulum is, above the sculptures of the virtues.</p>
<p>To identify the Panthéon&rsquo;s dome, look for a white dome with a second cupola above. Just the colour would be enough to find it, since I think it&rsquo;s the only big dome in Paris that&rsquo;s white.</p>
<p>If you&rsquo;re looking for a croissant close by the Panthéon, I&rsquo;d recommend going to <a href="https://g.co/kgs/QboR7f5">La Maison d&rsquo;Isabelle</a>. A friend recommended it and I found that it really does live up to expectations. Their croissants are the crunchiest and crackliest, if that&rsquo;s the type of croissants you prefer.</p>
<h2 id="hôtel-des-invalides">Hôtel des Invalides</h2>
<p>There are two (possibly more) gold domes in Paris, but the biggest is that of the Hôtel des Invalides, the former hospital for the injured veterans and today a museum complex. Napoleon I is buried under the dome, in a coffin with six layers of wood, lead, and stone. There are four other large coffins under the dome, and its chapels are magnificent. The dome itself is lit up at night, and one can see it just as well as the Eiffel Tower from the Jardin du Trocadéro.</p>
<figure><img src="https://upload.wikimedia.org/wikipedia/commons/4/4e/Cath%C3%A9drale_Saint-Louis-des-Invalides%2C_140309_2.jpg"
         alt="Figure 2. Cathédrale Saint-Louis-des-Invalides (Daniel Vorndran / DXR, CC BY-SA 3.0, via Wikimedia Commons)"/><figcaption>
            <p><em>Figure 2. Cathédrale Saint-Louis-des-Invalides (Daniel Vorndran / DXR, CC BY-SA 3.0, via Wikimedia Commons)</em></p>
        </figcaption>
</figure>

<p>In the buildings around the dome, there&rsquo;s the Museum of the Order of the Liberation, displaying artifacts from the Second World War; an exposition on Charles de Gaulle; and many other collections. The gardens are rather nice as well.</p>
<p>The dome is easy to find, because it&rsquo;s really gold. The cupola looks like a crown, and the golden spire is rather large. It&rsquo;s another good landmark, like the Panthéon.</p>
<h2 id="sorbonne-université">Sorbonne Université</h2>
<p>The dome is in the main campus of the Sorbonne University, close to the Panthéon. I don&rsquo;t think one can normally enter the campus if not enrolled in the university, especially on weekends, because there&rsquo;s security guards at the entrance checking student cards. Otherwise, the best view of the dome outside of the campus is from the little plaza facing the entrance, where the <a href="https://maps.app.goo.gl/w8PPHh44HTk3N8FL8">Statue August Comte</a> is.</p>
<figure><img src="https://upload.wikimedia.org/wikipedia/commons/8/81/Chapelle_Sainte-Ursule_de_la_Sorbonne%2C_Paris_001.jpg"
         alt="Figure 3. Chapelle Sainte-Ursule de la Sorbonne (Moonik, CC BY-SA 3.0, via Wikimedia Commons)"/><figcaption>
            <p><em>Figure 3. Chapelle Sainte-Ursule de la Sorbonne (Moonik, CC BY-SA 3.0, via Wikimedia Commons)</em></p>
        </figcaption>
</figure>

<p>The dome is primarily dark blue, but has white stripes. It&rsquo;s smaller than the other domes, so it&rsquo;s a bit difficult to see it, because the other surrounding buildings are about the same height.</p>
<h2 id="institut-de-france">Institut de France</h2>
<p>The dome of the Institute of France is directly facing the Pont des Arts and the Louvre. It houses the famous grand academies, like the Académie Française that manages the formal rules of the French language.</p>
<figure><img src="https://upload.wikimedia.org/wikipedia/commons/6/68/Institut_France.jpg"
         alt="Figure 4. Institut France (Benh LIEU SONG, CC BY-SA 3.0, via Wikimedia Commons)"/><figcaption>
            <p><em>Figure 4. Institut France (Benh LIEU SONG, CC BY-SA 3.0, via Wikimedia Commons)</em></p>
        </figcaption>
</figure>

<p>In the institute, there&rsquo;s also the Bibliothèque Mazarine, which is really marvellous. It&rsquo;s entirely free to enter and the vibes are very somber and academic. It&rsquo;s not really dark and definitely not light academia, but something rather appealing in between.</p>
<p>The dome is quite similar to the dome of the Sorbonne: dark grey with white stripes. However, the Sorbonne&rsquo;s dome has single stripes, whereas the Institut&rsquo;s has its stripes in pairs.</p>
<h2 id="basilique-du-sacré-coeur">Basilique du Sacré-Coeur</h2>
<p>The Basilique du Sacré-Coeur is at the peak of the Montmartre hill, looking out on the city of Paris. I&rsquo;ve mentioned it at the end of htis post, because I think it&rsquo;s fun to go there and test everything that I&rsquo;ve learned so far on identifying domes and different buildings. From there, at least the dome of the Panthéon is visible — I think the others might be a little too short.</p>
<figure><img src="https://upload.wikimedia.org/wikipedia/commons/c/c5/Le_sacre_coeur.jpg"
         alt="Figure 5. Le Sacré-Coeur (Tonchino, CC BY-SA 3.0, via Wikimedia Commons)"/><figcaption>
            <p><em>Figure 5. Le Sacré-Coeur (Tonchino, CC BY-SA 3.0, via Wikimedia Commons)</em></p>
        </figcaption>
</figure>

<p>The Basilique du Sacré-Coeur is one of the minor basilicas in France, of which there are 176 total in the country. It was built thanks to the whinings of Napoléon III, who attributed his military defeats to the &ldquo;moral decline of the French people&rdquo;. I personally wouldn&rsquo;t have put it like this, but in the end the resulting basilica is amazing. There&rsquo;re a lot of stained glass windows that light up the inside in bright colours when it&rsquo;s nice out.</p>
<p>The basilica has five &ldquo;domes&rdquo;, of which the biggest is the central dome. It&rsquo;s the only place I&rsquo;ve mentioned here that has several domes, so it&rsquo;s easy to identify. The domes are white and elongated, with little towers. One can climb the dome for a spectacular view of Paris, and it&rsquo;s cheaper than going up the Eiffel Tower. I don&rsquo;t think that the domes are visible from central Paris, so it&rsquo;s not a useful landmark, but it&rsquo;s worth a visit anyways.</p>
<h2 id="conclusion">Conclusion</h2>
<p>I&rsquo;ve focused on the domes that one can easily see from Paris&rsquo; skyline, but there are several others that I haven&rsquo;t visited but seen:</p>
<ul>
<li><a href="https://www.tribunal-de-commerce-de-paris.fr/">The Tribunal de commerce</a> on the Île de la Cité, which looks a bit like the Sorbonne&rsquo;s.</li>
<li><a href="https://www.pinaultcollection.com/fr/boursedecommerce">The Bourse du commerce</a> in the 1st arrondissement, which is made from glass and flatter than the others.</li>
<li><a href="https://www.operadeparis.fr/">The Palais Garnier</a> in the 9th arrondissement, which is very flat as well and uniquely green.</li>
</ul>
<p>Thanks for tolerating my unorganized thoughts on this very niche topic. Writing the original version in French was a useful writing exercise and way to relive my best memories of Paris. I hope that this might be useful if you find yourself in Paris one day.</p>
<p>Finally, I&rsquo;d like to shill a new event in Paris that my friend <a href="https://uzpg.me/">Uzay</a> and I have launched. It&rsquo;s a <a href="https://socratica.info/">Socratica</a> node called Parenthèse. It&rsquo;s always been my goal to get more deeply involved in a Socratica node, so seeing as there wasn&rsquo;t a node here and that both of us were looking for a community during our time in Paris, we started one. Parenthèse takes place every Saturday at 11:00 at <a href="https://www.digital-village.com/villages/paris">Digital Village Paris</a> (we&rsquo;re usually at a big table to the left of the bar). If you&rsquo;re interested, subscribe to our <a href="https://lu.ma/a5q5tgxy">Luma</a> — we&rsquo;d be happy to see you!</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Cité des Dômes et du Design</title>
      <link>https://kewbi.sh/blog/fr/posts/250413/</link>
      <pubDate>03 Apr 2025</pubDate>
      
      <description>Sur l&#39;exploration de Paris.</description>
      <content:encoded><![CDATA[

<div class="grid-element" style="margin-bottom: 0.5em;margin-top: 2em">
	Hi there! I wanted to practice my French writing, so I decided to work on this blog post about exploring Paris's various domes. This post is only available in French for now, but an English version is coming shortly and will be linked here as soon as it's up.
</div>


<h2 id="introduction">Introduction</h2>
<p>Comme tu peux voir, cet article est un peu different de ceux que j&rsquo;écrivais d&rsquo;habitude. Je suis actuellement à Paris afin de faire un stage à la Sorbonne Université. J&rsquo;avais tellement hâte de m&rsquo;installer dans un nouveau pays et environnement de recherche. De plus, j&rsquo;espérais améliorer mon français. Anecdote : ma matière secondaire à UBC est le français, alors je suis cénsé pouvoir parler français plus ou moins courrament (c&rsquo;est pas le cas).</p>
<p>Je ne connaissais personne quand je suis arrivée. Je suis toujours une personne independante, alors j&rsquo;ai vraiment profité des moments seules en découvrant des nouveaux endroits et musées. Je passais beaucoup de week-ends en flânant autour de ville. Je viens de Vancouver, où il n&rsquo;y a pas beaucoup des vieux bâtiments. C&rsquo;est la raison pour laquelle j&rsquo;étais toujours étonnée par ce que je voyais. La bibliothèque où je travaille de temps en temps est plus vieille que mon pays — c&rsquo;est une dinguerie ! Je marcherais n&rsquo;import où et je trouverais d&rsquo;architecture magnifique : des sculptures au-dessus des portes, des colonnes Corinthiennes, et, le plus attrayant, des dômes.</p>
<p>J&rsquo;explorerais en regardant le horizon et en cherchant des flèches intéressantes. J&rsquo;apprenais à me diriger par les dômes et les autres repères de Paris. Si j&rsquo;ai vu une flèche en or, je saurais que c&rsquo;était le dôme du Hôtel des Invalides. Si je savais que j&rsquo;étais dans le 5e arrondissement environ, je saurais que j&rsquo;étais près du Panthéon. C&rsquo;était un peu comme un jeux : quels sont les endroits historiques que je peux repérer ?</p>
<p>Mais, écoute, quand on est au milieu d&rsquo;une dizaine des dômes, ils commencent à se ressembler. Je voudrais une liste de contrôle : s&rsquo;il y a d&rsquo;or, c&rsquo;est les Invalides; si le dôme est droit et petit et à côté de la Seine, c&rsquo;est l&rsquo;Institut de France; etc. Ainsi j&rsquo;ai cherché une telle liste des dômes, et j&rsquo;étais surprise de trouver <a href="https://en.wikipedia.org/wiki/List_of_d%C3%B4mes_in_France">ce liste</a> sur Wikipedia (ironiquement seulement disponible en anglais, mais c&rsquo;est juste une liste des noms en français, alors ça va). C&rsquo;était plûtot drôle qu&rsquo;on a pris le temps de faire ça, mais ça reste insuffissant de réperer efficacement les dômes. Alors, j&rsquo;ai décidé d&rsquo;écrire cet article pour les décrire et partager un peu des histoires que j&rsquo;ai apprises pendant mon séjour ici.</p>
<p>Concernant le nom d&rsquo;article, j&rsquo;ai constaté qu&rsquo;à Paris, il y a beaucoup de programmes et des bâtiments qui s&rsquo;appelle genre « Cité de &hellip; ». Il y a le site web <a href="https://www.cite-sciences.fr/fr/au-programme/lieux-ressources/cite-des-metiers">Cité des métiers</a>, et je passais tous les jours le <a href="https://www.citemodedesign.fr/fr/">Cité de la Mode et du Design</a> en bus. Selon ChatGPT, c&rsquo;est parce que « cité » indique l&rsquo;idée d&rsquo;un espace central et spécialisé dans quelque chose — ça, je ne crois pas entièrement, mais peut-être ça a du sens. Bref, j&rsquo;ai décidé d&rsquo;appeler cet article « Cité des Dômes et du Design », parce que Paris est une ville vraiment spécialisée dans les dômes magnifiques.</p>
<p>D&rsquo;abord, il faut que je m&rsquo;excuse : désolée pour les fautes eventuelles en français. Ce n&rsquo;est evidemment pas ma langue maternelle, et même si j&rsquo;ai pris trois ans des cours jusqu&rsquo;à un niveau présumé B1, je continue à faire des erreurs fondamentales. J&rsquo;ai essayé de ne pas utiliser ChatGPT etc., j&rsquo;étais seulement accompagné par <a href="https://www.linguee.com/">Linguee</a> — j&rsquo;aime bien son moteur de recherche qui présente immédiatement les résultats en tapant. J&rsquo;ai fait mon mieux de corriger cet article moi-même. Alors, merci pour ta compréhension et allons découvrir les dômes de Paris !</p>
<h2 id="panthéon">Panthéon</h2>
<p>Je crois que le Panthéon est le dôme le plus connu — c&rsquo;est dans le 5e près du Jardin du Luxembourg. C&rsquo;est l&rsquo;endroit où on a enterré des grands hommes et femmes de France, comme Victor Hugo, Voltaire, et Marie Curie. C&rsquo;est très grand, alors c&rsquo;est facile à voir et c&rsquo;est un bon repère pour se diriger dans Paris.</p>
<figure><img src="https://upload.wikimedia.org/wikipedia/commons/8/80/Pantheon_of_Paris_007.JPG"
         alt="Figure 1. Panthéon (Moonik, CC BY-SA 3.0, via Wikimedia Commons)"/><figcaption>
            <p><em>Figure 1. Panthéon (Moonik, CC BY-SA 3.0, via Wikimedia Commons)</em></p>
        </figcaption>
</figure>

<p>Le Panthéon était construit par Louis XV qui a juré qu&rsquo;il rénovererait l&rsquo;ancienne église là s&rsquo;il surmontait sa maladie sérieuse. Il a repris du poil de la bête, puis il a chargé <a href="https://en.wikipedia.org/wiki/Jacques-Germain_Soufflot">Soufflot</a> avec l&rsquo;architecture d&rsquo;une nouvelle église. Le Panthéon était une église, puis un lieu laïque où il n&rsquo;y avait pas de messes pendant la Revolution, puis encore une église et finalement c&rsquo;est un lieu laïque pour enterrer les gens importants. C&rsquo;est consacré à Sainte Geneviève (la bibliothèque en face du Panthéon est nommée pour elle aussi) et les murs sont peints avec des oeuvres montrant sa vie, de son enfance pieuse à sa mort. Il y a aussi une série des tableaux consacrée à Jeanne d&rsquo;Arc et quelques autres pour les autres saints.</p>
<p>Je te recommande de prendre le temps pour relever la tête et profiter du toit et sa décoration. C&rsquo;est plein de détails et tableaux. Mes préférés sont lesquels sous le dôme principal où le pendule se situe, dessus les sculptures des vertus.</p>
<p>Pour repérer le dôme du Panthéon, cherche un dôme blanc avec un deuxième coupole dessus. Seulement la couleur suffit pour l&rsquo;identifier, je pense que c&rsquo;est le seul grand dôme à Paris étant blanc.</p>
<p>Si tu cherches un croissant près du Panthéon, je te conseille d&rsquo;aller chez <a href="https://g.co/kgs/QboR7f5">La Maison d&rsquo;Isabelle</a>. Un pote me l&rsquo;a recommandée et j&rsquo;ai trouvé que ça vit vraiment aux attentes. Leurs croissants sont les plus croquants et « crackly » si c&rsquo;est le genre des croissants que tu préfères.</p>
<h2 id="hôtel-des-invalides">Hôtel des Invalides</h2>
<p>Il y a deux (ou plus) dômes dorées à Paris, mais le plus grand est lequel du Hôtel des Invalides, l&rsquo;ancien hôpital pour les invalides de l&rsquo;armée et maintenant un complèxe des musées. Napoléon I est enterré sous le dôme, dans un cerceuil avec six couches de bois, plomb, et pierre. Il y a quatre autres grands coffrets sous le dôme, et ses chapelles sont magnifiques. Le dôme est allumé en soirée et on peut le voir comme la Tour Eiffel au Jardin du Trocadéro.</p>
<figure><img src="https://upload.wikimedia.org/wikipedia/commons/4/4e/Cath%C3%A9drale_Saint-Louis-des-Invalides%2C_140309_2.jpg"
         alt="Figure 2. Cathédrale Saint-Louis-des-Invalides (Daniel Vorndran / DXR, CC BY-SA 3.0, via Wikimedia Commons)"/><figcaption>
            <p><em>Figure 2. Cathédrale Saint-Louis-des-Invalides (Daniel Vorndran / DXR, CC BY-SA 3.0, via Wikimedia Commons)</em></p>
        </figcaption>
</figure>

<p>Dans les bâtiments environnant le dôme, il y a le Musée de l&rsquo;order de la Libération, montrant des artifaits de la Seconde Guerre Mondiale; une exposition sur Charles de Gaulle; et beaucoup d&rsquo;autres collections. Les jardins sont plûtot agréable en plus.</p>
<p>Le dôme est facile à repérer, parce que c&rsquo;est vraiment dorée. Le coupole se ressemble à une couronne et la flèche dorée est plutôt grande. C&rsquo;est un autre bon repère, comme le Panthéon.</p>
<h2 id="sorbonne-université">Sorbonne Université</h2>
<p>Ce dôme se situe dans le campus principal de la Sorbonne Université près du Panthéon. Je ne crois pas qu&rsquo;on peut normalement entrer le campus si on n&rsquo;est pas inscrit à l&rsquo;université, particulairement les week-ends, parce qu&rsquo;il y a des agents de sécurité à l&rsquo;entrée qui vérifient des cartes d&rsquo;identité d&rsquo;étudiants<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. Mais autrement, la meilleure vue de ce dôme dehors du campus est à la petite place en face de l&rsquo;entrée où il y a la <a href="https://maps.app.goo.gl/w8PPHh44HTk3N8FL8">Statue August Comte</a>.</p>
<figure><img src="https://upload.wikimedia.org/wikipedia/commons/8/81/Chapelle_Sainte-Ursule_de_la_Sorbonne%2C_Paris_001.jpg"
         alt="Figure 3. Chapelle Sainte-Ursule de la Sorbonne (Moonik, CC BY-SA 3.0, via Wikimedia Commons)"/><figcaption>
            <p><em>Figure 3. Chapelle Sainte-Ursule de la Sorbonne (Moonik, CC BY-SA 3.0, via Wikimedia Commons)</em></p>
        </figcaption>
</figure>

<p>Ce dôme est principalement bleu foncé mais il a des rayures blanches. C&rsquo;est plus petit des autres dômes. C&rsquo;est un peu difficile de le voir parce que les autres bâtiments environnants sont des hauteurs pareils.</p>
<h2 id="institut-de-france">Institut de France</h2>
<p>Le dôme d&rsquo;Institut de France est directement en face du Pont des Arts<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> et du Musée du Louvre. Il s&rsquo;abrit les grandes académies célèbres, comme l&rsquo;Académie Française qui gère la formalisation de la langue française.</p>
<figure><img src="https://upload.wikimedia.org/wikipedia/commons/6/68/Institut_France.jpg"
         alt="Figure 4. Institut France (Benh LIEU SONG, CC BY-SA 3.0, via Wikimedia Commons)"/><figcaption>
            <p><em>Figure 4. Institut France (Benh LIEU SONG, CC BY-SA 3.0, via Wikimedia Commons)</em></p>
        </figcaption>
</figure>

<p>Dans l&rsquo;institut, il y a aussi la Bibliothèque Mazarine qui est merveilleuse. C&rsquo;est entièrement gratuit d&rsquo;entrer et l&rsquo;ambiance est vraiment sombre et academique. C&rsquo;est pas forcément « dark academia » et certainement pas « light academia » mais quelque chose agréable au milieu.</p>
<p>Le dôme est vraiment pareil que le dôme de la Sorbonne, étant gris foncé avec des rayures blanches. En revanche, le dôme de la Sorbonne a des rayures singles, mais ce dont l&rsquo;Institut de France a des rayures en paires.</p>
<h2 id="basilique-du-sacré-coeur">Basilique du Sacré-Coeur</h2>
<p>La Basilique du Sacré-Coeur se trouve à la pointe de la butte Montmartre, donnant sur la ville de Paris. Je la mentionne à la fin d&rsquo;article, parce que je trouve que c&rsquo;est amusant d&rsquo;y aller pour contrôler tout ce que j&rsquo;ai appris sur le repérage des dômes et d&rsquo;autres bâtiments. Au moins le dôme du Panthéon est visible, je crois que les autres sont un peu trop petits.</p>
<figure><img src="https://upload.wikimedia.org/wikipedia/commons/c/c5/Le_sacre_coeur.jpg"
         alt="Figure 5. Le Sacré-Coeur (Tonchino, CC BY-SA 3.0, via Wikimedia Commons)"/><figcaption>
            <p><em>Figure 5. Le Sacré-Coeur (Tonchino, CC BY-SA 3.0, via Wikimedia Commons)</em></p>
        </figcaption>
</figure>

<p>La Basilique du Sacré-Coeur est une des basiliques mineures en France dont il y a 176 en total. C&rsquo;était construit grâce aux plaintes de Napoléon III, qui avait attribué ses défaits en guerre au « déchin moral de France ». Dans ce cas, moi, je ne dirais pas ça, mais enfin le résultat est superbe. Il y a beaucoup de vitrails qui allument vivement la basilique quand il fait beau.</p>
<p>Il y a cinq « dômes » de la basilique dont le plus grand est le dôme central. C&rsquo;est le seul endroit que j&rsquo;ai décrit qui a plusieurs dômes, alors c&rsquo;est facile de les repérer. Ils sont blancs et allongés avec des petits tours. C&rsquo;est possible de monter le dôme pour une vue spectaculaire sur Paris, moins cher qu&rsquo;une ascension à la Tour Eiffel. Je ne pense pas qu&rsquo;ils soient visible au centre-ville de Paris, alors c&rsquo;est pas un repère utile, mais ça mérite une visite quand même.</p>
<h2 id="conclusion">Conclusion</h2>
<p>J&rsquo;ai concentré sur les dômes qu&rsquo;on peut voir facilement dans le horizon de Paris, mais il y a quelques autres dômes que je n&rsquo;ai pas visité mais que j&rsquo;ai vu:</p>
<ul>
<li><a href="https://www.tribunal-de-commerce-de-paris.fr/">Le Tribunal de commerce</a> sur l&rsquo;Île de la Cité, qui se ressemble un peu ce dont la Sorbonne.</li>
<li><a href="https://www.pinaultcollection.com/fr/boursedecommerce">La Bourse du commerce</a> dans le 1e, qui est en verre et un peu plus plat que les autres.</li>
<li><a href="https://www.operadeparis.fr/">Le Palais Garnier</a> dans le 9e, qui est très plat aussi et uniquément vert.</li>
</ul>
<p>Merci d&rsquo;avoir supporté mes pensées mal structurées sur un sujet entièrement niche. C&rsquo;était un exercise utile afin de pratiquer mon écriture et de revivre mes meilleurs souvenirs de Paris. J&rsquo;espère que ça sera utile si tu te trouves n&rsquo;importe quand à Paris.</p>
<p>Enfin, je voudrais vulgariser un nouveau événement à Paris que j&rsquo;ai fondé avec mon pote <a href="https://uzpg.me/">Uzay</a>. C&rsquo;est un node de <a href="https://socratica.info/">Socratica</a> qui s&rsquo;appelle Parenthèse. C&rsquo;était toujours mon but d&rsquo;assister plus régulièrement un node Socratica, alors en voyant qu&rsquo;il n&rsquo;y avait pas de Socratica ici et que nous cherchions une communauté, on en a commencé un. Ça a lieu tous les samedis à 11h à <a href="https://www.digital-village.com/villages/paris">Digital Village Paris</a> (on est toujours à une grande table à gauche du bar). Si tu t&rsquo;y interesses, abonne-toi à notre <a href="https://lu.ma/a5q5tgxy">Luma</a>, nous serons ravis de te voir !</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Je suis plutôt heureuse que j&rsquo;ai réussi à accéder au campus — je les ai montré mon convention de stage pour l&rsquo;autre campus (Jussieu) et ils m&rsquo;ont permis d&rsquo;entrer. Peut-être le fait que je leur ai demandé en français m&rsquo;a aidé un peu (ils m&rsquo;ont démandé d&rsquo;où je viens et ils ont deviné le Japon.) Comme on dit sur tech Twitter, tu peux juste faire des choses !&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Anecdote : le Pont des Arts est le pont célèbre où tous les amoureux mettaient des « serrures d&rsquo;amour », mais c&rsquo;était trop lourd pour le Pont, alors ils étaient tous coupés et maintenant il n&rsquo;y a que du plexiglas. C&rsquo;est plûtot mignon qu&rsquo;il y avait des gens travaillant de les couper de temps en temps, c&rsquo;est un rappel qu&rsquo;il faut rester pratique même avec des questions de coeur.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>A Captive Audience</title>
      <link>https://kewbi.sh/blog/posts/250316/</link>
      <pubDate>16 Mar 2025</pubDate>
      
      <description>On free WiFi portals.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Finding free WiFi has been very ingrained into how I use the Internet. Until last summer, I didn&rsquo;t have a data package on my phone plan. Whenever I&rsquo;d be out and about off-campus, I had to rely on free WiFi. I remember scurrying around for scraps of signal on the boundaries of whatever coffee shop had guest WiFi in order to ask my friends where they were or check maps. When I open my laptop in any new library, café, or airport, the first thing I do is connect to the free WiFi. On Etsy, there are thousands of results when I search for &ldquo;free wifi sign&rdquo;, so business owners and digital artists alike must be aware that free WiFi is highly in demand.</p>
<p>I&rsquo;ve been lucky to grow up in a city and in contexts where WiFi was more or less available when needed: I put off getting data for as long as I did because I was always either at university, or at home, or on the bus between the two, all of which had free WiFi. WiFi has never felt like a commodity or resource to be carefully rationed. I&rsquo;ve always just kind of expected that it&rsquo;d be there.</p>
<p>This last year, I&rsquo;ve had the opportunity to do a lot of travelling, and that also means connecting to a lot of free WiFi while in transit and in new environments. Fun fact: if you use <a href="https://networkmanager.dev/">NetworkManager</a> on Linux, you can run <code>sudo ls /etc/NetworkManager/system-connections</code> to get a list of all of your previous connections, which by default are saved to enable autoconnection whenever the network is in range again. I&rsquo;ve accumulated a bunch of train networks: I have the <a href="https://www.upexpress.com/en">UP Express</a>, <a href="https://www.viarail.ca/en">VIA Rail</a>, and <a href="https://eurostar.com/">Eurostar</a> connections saved. I also have many a coworking space and café, as well as a sprinkling of hotels.</p>
<p>All this travelling and all this free WiFi also meant a lot of WiFi <a href="https://en.wikipedia.org/wiki/Captive_portal">captive portals</a>. Captives are the interstitial page that usually displays the company&rsquo;s information and the standard usage terms and conditions before you&rsquo;re able to meaningfully access the network. You might have to provide your email or phone number for data harvesting purposes, and in the world of airline WiFi, they&rsquo;re also used for requiring payment upfront, but I won&rsquo;t be focusing on paid WiFi here. After you submit the form, you&rsquo;re released from your Internet captivity and can begin normally using the network. They&rsquo;re a staple of free WiFi access, and I&rsquo;d suspect most folks&rsquo; brains don&rsquo;t even process the form before just trying to click through as fast as possible.</p>
<p>Despite these portals popping up on an everyday basis, have you ever wondered how they work — how they&rsquo;re triggered, usually without your interaction? In April last year, I was in Seattle for <a href="https://kewbi.sh/blog/posts/240602/">OSSNA2024</a>, and after getting back to my hotel, I was trying to log in to the guest WiFi network to check my email. I signed my phone in first, which worked without a hitch: a terms and conditions page popped up and asked for my room number before I could get access. Same goes for my parents&rsquo; devices. But on my laptop, I was only able to connect to the network — my status bar applet was showing full connection and I had an IP address assigned — but I couldn&rsquo;t figure out how to get the captive portal to show up so I could actually get access to anything. After some educated futzing around with the URLs of past free WiFi portals that were still in my browser history, I managed to open the hotel&rsquo;s WiFi portal from an old, unrelated <a href="http://www.gstatic.com/generate_204"><code>gstatic.com/generate_204</code></a> link. Since then, I&rsquo;d been wondering how this worked — why did captive portals seem to be all using this <code>gstatic</code> thing? Why did my phone always get a portal while my laptop was hit-or-miss?</p>
<p>This is a post about captive portals, HTTP hijacking, and the hardcoded URLs that tech companies use to trigger popups. I&rsquo;ll also take a brief detour into the world of Parisian libraries and how I automated one of their captive page logins. I hope this&rsquo;ll serve as a fun behind-the-scenes look at what happens when you hit up a new coffee shop and connect to their network for the first time.</p>
<h2 id="o-popup-wherefore-art-thou">O Popup, Wherefore Art Thou?</h2>
<p>When you connect to a WiFi network that uses a captive portal, the IP address assignment takes place as usual with <a href="https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol">DHCP</a>. Your machine and the DHCP server (usually your router) perform a two round-trip exchange to offer, request, and acknowledge your assigned IP address.</p>
<p>From this point onwards, there&rsquo;s a couple ways that captive portals are implemented.</p>
<ul>
<li>For one, DHCP servers can return DNS server addresses in <a href="https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol#Options">their responses</a>. A free WiFi network can set their firewall to only allow captive users to use the network&rsquo;s DNS server for all resolution at first, and then only return the IP address of the captive portal page for all lookups. The firewall can be configured to allow noncaptive users to use other DNS servers once they&rsquo;ve authenticated.</li>
<li>Otherwise, networks might use HTTP redirects: all web traffic is pointed first at an intermediary server that returns a redirect to the captive portal page. When your client device first connects to the network, it sends a request to a standard captive portal detection URL) and checks the status code to see if the network allows unfettered access or if it redirects to a portal page. This only works with HTTP sites, though, because with HTTPS, the router can&rsquo;t do anything with the intercepted traffic: it doesn&rsquo;t have the decryption keys, so it can&rsquo;t modify it; if it returns a different certificate with the response, the browser will show a certificate error. If a site uses <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Strict-Transport-Security">HSTS</a>, any attempts to downgrade the traffic to HTTP to redirect to the captive portal will fail. HTTPS sites are therefore difficult for captive portals to intercept.</li>
<li>Besides all this hijacking, there&rsquo;s an <a href="https://datatracker.ietf.org/doc/html/rfc8910">RFC</a> to introduce a new standardized DHCP option for routers to inform clients of the captive portal enforcement. The proposed option would contain the URI of the captive portal page. For networks not using a captive portal, the RFC specifies a sentinel value to use to avoid the client having to perform captive portal detection. There&rsquo;s another <a href="https://datatracker.ietf.org/doc/rfc8908/">RFC</a> for specifying a similar HTTP-based API.</li>
</ul>
<p>The exact URLs used for captive portal detection differ per client. The <a href="http://www.gstatic.com/generate_204"><code>gstatic.com/generate_204</code></a> that I used in Seattle to trigger the captive portal redirect is controlled by Google, and <a href="https://www.chromium.org/chromium-os/chromiumos-design-docs/network-portal-detection/">Chrome has a similar link</a>. iOS tries to load <a href="http://captive.apple.com/">captive.apple.com/</a> when connecting. Both of these are HTTP links, so can be redirected or hijacked as necessary by the portal. If a captive portal is in place, the <code>204</code> status or success page won&rsquo;t load (and may replace itself with the captive portal&rsquo;s page). Another site that&rsquo;s often recommended is <a href="https://neverssl.com"><code>neverssl.com</code></a>, which exists for the sole purpose of having a reliable HTTP url to load (that won&rsquo;t be redirected by the browser to an upgraded HTTPS connection). I use this to force the captive portal page to load whenever Chrome&rsquo;s detection doesn&rsquo;t quite work.</p>
<p>I think my captive portal detection was hit-or-miss in the past because I would <em>sometimes</em> have HTTP sites open in Chrome previously, which would auto-load after starting my laptop or reconnecting to WiFi, and sometimes I&rsquo;d have tabs full of HTTPS only. The latter is more frequently the case because most sites nowadays default to HTTPS. Chrome will remember which sites do support HTTPS and automatically load the HTTPS versions, even if you type in a HTTP URL. However, Chrome has a whole <a href="https://www.chromium.org/chromium-os/chromiumos-design-docs/network-portal-detection/">page on portal detection</a>, so I&rsquo;m not sure why their solution has been so spotty in the past.</p>
<p>Linux&rsquo;s <code>NetworkManager</code> also has some configuration options to <a href="https://wiki.archlinux.org/title/NetworkManager#Captive_portals">automatically open captive portal pages</a> on desktop managers that don&rsquo;t automatically do so. You&rsquo;ll have to write a dispatcher script to start a browser instance if <code>NetworkManager</code> detects that the network&rsquo;s connectivity state is a portal. <code>NetworkManager</code>&rsquo;s portal detection uses the <a href="http://ping.archlinux.org"><code>ping.archlinux.org</code></a> URL, but this can be configured via the <a href="https://wiki.archlinux.org/title/NetworkManager#Checking_connectivity"><code>[connectivity]</code> option</a>. I haven&rsquo;t set this up, since just opening the browser and visiting a HTTP site has worked well enough for me.</p>
<p>The upshot for captive portals on the Internet is that you&rsquo;ll need some sort of web browser on any device, and I&rsquo;d bet this is part of why web browsers are so prevalent on devices like TVs where the interaction feels awkward. There&rsquo;s a lot of complaining on various electronics community forums about one&rsquo;s TV not connecting to captive networks: these browsers likely don&rsquo;t implement proper portal detection, so folks have had to come to the same conclusion of putting in the portal&rsquo;s URL directly, since sometimes even redirects don&rsquo;t work.</p>
<h2 id="sainte-geneviève-meet-networkmanager">Sainte-Geneviève, Meet NetworkManager</h2>
<p>The <a href="https://www.bsg.univ-paris3.fr/iguana/www.main.cls">Bibliothèque Sainte-Geneviève</a> is one of the main public libraries in Paris. It&rsquo;s right by the <a href="https://fr.wikipedia.org/wiki/Panth%C3%A9on_(Paris)">Panthéon</a>, which is a nice view if you get stubbornly stuck in the hours-long queue for a spot that sometimes snakes across the end of the block<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. I really like working there when I&rsquo;m not in the lab — in general, high ceilings and big windows are my jam, and the BSG&rsquo;s vaguely Art Nouveau roof design and vaulted ceiling with tonnes of windows checks all my boxes.</p>
<p>What&rsquo;s more, the BSG provides both a lovely working environment <em>and</em> free WiFi for registered readers. WiFi is blocked behind a captive portal asking for your library card number and account password: by default your birthday in DDMMYYYY format. I was writing part of this post at the BSG, so out of curiosity I poked around at the portal page.</p>


<style>
/* https://github.com/lonekorean/gist-syntax-themes */
@import url('https://cdn.rawgit.com/lonekorean/gist-syntax-themes/d49b91b3/stylesheets/one-dark.css');
body .gist .gist-meta {
  color: #ffffff;
  background: var(--sub-colour); 
}
body .gist .gist-meta a {
  color: #ffffff
}

.gist ::-webkit-scrollbar {
background: #141414;
}

.gist {
--borderRadius-medium: 8px;
}

</style>
<script src="https://gist.github.com/kewbish/9f9664bd268d7a640ff6390ab700826b.js"></script>


<p>This page just <a href="https://gist.github.com/kewbish/9f9664bd268d7a640ff6390ab700826b#file-bsg-captive-portal-html-L65">fires an AJAX login request</a> based on the card number and password you provide. The <code>zoneid</code> variable is set by a separate <code>zone.js</code> script, which seems to statically set it to 0.</p>
<p>Some networks, particularly those that require a login and password, do a better job at remembering users after captive portals. However, because this captive portal came up every time I visited, I wondered if I could automate login with a simple shell script so I wouldn&rsquo;t have to fill the form in myself.</p>
<p>I learned that Linux&rsquo;s <code>NetworkManager</code> supports <a href="https://askubuntu.com/questions/13963/call-script-after-connecting-to-a-wireless-network">dispatcher scripts</a>, which are run whenever a connection changes. It passes several environment variables, including the <code>CONNECTION_ID</code> SSID, and two arguments: the interface name and event type (like &ldquo;up&rdquo;). The scripts must have <a href="https://man.archlinux.org/man/NetworkManager-dispatcher.8.en">certain permissions set</a> as well. You can reference <a href="https://man.archlinux.org/man/NetworkManager-dispatcher.8.en">the dispatcher documentation</a> for more details.</p>
<p>For my use case, a simple script wrapping a <code>curl</code> command was enough. This script will run the <code>curl</code> on every connection change to an &ldquo;up&rdquo; state on all connections with the SSID &ldquo;BSG Public&rdquo;<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span><span style="color:#75715e"># in /etc/NetworkManager/dispatcher.d/bsg.sh</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># run:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># $ sudo chown root:root /etc/NetworkManager/dispatcher.d/bsg.sh</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># $ sudo chmod 755 /etc/NetworkManager/dispatcher.d/bsg.sh</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> <span style="color:#f92672">[</span> <span style="color:#e6db74">&#34;</span>$2<span style="color:#e6db74">&#34;</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;up&#34;</span> <span style="color:#f92672">]</span> <span style="color:#f92672">&amp;&amp;</span> <span style="color:#f92672">[</span> <span style="color:#e6db74">&#34;</span>$CONNECTION_ID<span style="color:#e6db74">&#34;</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;BSG Public&#34;</span> <span style="color:#f92672">]</span>; <span style="color:#66d9ef">then</span>
</span></span><span style="display:flex;"><span>	curl -X POST <span style="color:#e6db74">&#34;http://portail.bsg.univ-paris3.fr:8000/api/captiveportal/access/logon/0/&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>		 -H <span style="color:#e6db74">&#34;Content-Type: application/json&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>		 -d <span style="color:#e6db74">&#39;{&#34;user&#34;: &#34;[USERNAME]&#34;, &#34;password&#34;: &#34;[DDMMYYYY]&#34;}&#39;</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">fi</span>
</span></span><span style="display:flex;"><span>exit $?
</span></span></code></pre></div><p>And with that, I no longer had to fumble for my library card to log in every time I connected. I haven&rsquo;t tried poking into other captive WiFi network pages to see what they require behind the scenes, but I figure it&rsquo;d be easy enough to scrape pages to find the &ldquo;accept conditions&rdquo; checkbox and network request and automate logins with a similar curl command.</p>
<h2 id="conclusion">Conclusion</h2>
<p>TL;DR: whenever you connect to a new network, if you get certificate errors on all your HTTPS sites or can&rsquo;t access anything and don&rsquo;t get a captive portal page, open <a href="neverssl.com"><code>neverssl.com</code></a>, which should get you to the captive portal page.</p>
<p>I wrote this post primarily to share this <code>neverssl.com</code> tip, which would have saved me a lot of time retrying free WiFi connections with no luck and resigning myself to doing work on my phone. Whenever I encounter networking problems, I tend to chalk it up to it &ldquo;just being a Linux thing&rdquo;, but in this case, captive portal implementations are unstandardized and slightly janky on all providers. In this case, Linux and <code>NetworkManager</code> actually let you hook into network connections more easily, and as I&rsquo;ve shown, we can automate portal logins relatively easily even if the underlying portal implementation doesn&rsquo;t support it.</p>
<p>In my earlier section on captive portal implementations, I mentioned there were a couple RFCs on better standardizing captive portal interactions. While the RFCs themselves date to around five years ago, it seems like there&rsquo;s been some progress on supporting them in both <a href="https://developer.apple.com/news/?id=q78sq5rv">iOS</a> and <a href="https://developer.android.com/about/versions/11/features/captive-portal">Android</a>, but there&rsquo;s still work to be done for adding support to <a href="https://news.ycombinator.com/item?id=41922225">other major DHCP servers and router hardware</a>. Overall, though, providers seem to have converged on a set of detection and portal display techniques that work for most cases, so I don&rsquo;t know if there&rsquo;ll be much movement in this area going forward.</p>
<p>Diving into captive portals was fun — there&rsquo;s so many other similar interactions on the Web that are ubiquitous but never something that you really think about. Getting to dig to the spec and RFC level for a small, self-contained area as well as coming up with a little artifact is a nice way to get an overview of an API or area. Writing shorter technical posts is also quite refreshing after my recent run of heavier explainers: more rabbitholes (and café free WiFi connections) to come in the future!</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>As I&rsquo;m finishing this blog post, the queue is to the end of the block, so I&rsquo;ve been resigned to sitting on one of the little rock benches across the street just barely in range of the WiFi.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>If you want to limit the script to only a specific connection (and not just all networks called &ldquo;BSG Public&rdquo;), you can also look into the <code>.nmconnection</code> files in <code>/etc/NetworkManager/system-connections</code> to find its UUID and update the script to check the <code>$CONNECTION_UUID</code> variable instead. This UUID will <a href="https://unix.stackexchange.com/a/351010">differ per device</a> even on the same connection, so for the sake of making this script more universal I opted to use the <code>CONNECTION_ID</code>.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>FOSDEM Follies</title>
      <link>https://kewbi.sh/blog/posts/250302/</link>
      <pubDate>02 Mar 2025</pubDate>
      
      <description>On Brussels and treasure hunts.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>A couple weeks ago, I gave a talk about <a href="https://kewbi.sh/blog/posts/250223/">Kintsugi</a> at <a href="https://fosdem.org/">FOSDEM</a>, a massive free and open-source software conference welcoming thousands of developers around the world at the <a href="https://www.ulb.be/fr/plans-et-acces/solbosch">ULB Solbosch</a> campus in Brussels. I hustled myself onto the first Eurostar to Brussels early Saturday morning and returned Sunday evening with my bag laden with goodies and a camera roll significantly fuller with incredible architecture. I&rsquo;ve never been in Brussels before and wanted to explore the city a bit despite only being there for the weekend, so I had a very packed Saturday at FOSDEM and an equally hectic trek around downtown Brussels on Sunday. But in between, I still got to run into friends, meet new folks, and have a stop at neat museums (for free!) — a very fulfilling trip.</p>
<p>The most surprising thing about FOSDEM was its sheer scale and professional organization despite being entirely free and not requiring registration. The hallways were very crowded, not to speak of the main stands rooms and larger keynotes, and there were constant flows of people everywhere. The organizers&rsquo; communication, prep, and signage was super clear, however, and there were plenty of information volunteers milling around. Everything seemed to run like a well-oiled machine (it was the 25th time they ran this, after all) and I&rsquo;m very impressed that this was all a volunteer effort with no formal governing body. The website estimates &gt;8K people attending, and even from the main train station and tram stops in Brussels it seemed like you could pick out at least one attendee wherever you were (wearing bright tech merch and a well-stickered laptop were usually dead giveaways.) It felt like throngs of people were flooding through the city to get to various meetups, beer nights, and talks. In spite of this, however, apparently even locals were unaware of the event: I had some friends from Brussels that I met at <a href="https://kewbi.sh/blog/posts/241006/">DARE</a> who had only found out because I&rsquo;d told them about my talk. Perhaps locals chalked up the surges of hoodie-clad, Android-wielding folks to a little mid-winter peak of tourism.</p>
<p>The scale of FOSDEM means that it&rsquo;s just a lot to take in, especially if it&rsquo;s your first time. I&rsquo;ll confess I went through a few survival guides before attending, such as <a href="https://petersouter.xyz/fosdem-survival-guide/">this</a>, <a href="https://marcin.juszkiewicz.com.pl/2019/10/15/how-to-survive-fosdem/">this</a>, and <a href="https://ounapuu.ee/posts/2024/02/12/fosdem-2024/">this</a> to get a sense of how things worked. Still, I&rsquo;m not sure if any survival guide (including this one) can really prepare you for it. For instance, a common thread was not being able to get into popular talks and devrooms, which makes sense, especially for popular technologies like Rust and Go and so on. I wasn&rsquo;t expecting to not be able to get into even the Security devroom — but more about this later. The advice in other survival guides was certainly helpful, particularly for day-of logistics and recommendations for side events and other attractions in Brussels. While I maybe didn&rsquo;t engage as deeply in the event as the seasoned veterans that&rsquo;ve written other guides, I wanted to share thoughts from a first-timer&rsquo;s perspective and compare it to <a href="https://events.linuxfoundation.org/open-source-summit-north-america/">OSSNA</a><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<p>This is a post about my FOSDEM experience and survival tips, my attempt to solve the treasure hunt that was run, and my haphazard trip planning process. I very much enjoyed the experience — major props to all the organizers, volunteers, and team members for putting together such a vibrant and well-put-together-yet-scrappy conference.</p>
<h2 id="-le-jour-j-2">« Le Jour-J »<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></h2>
<p>Between my talk getting accepted and actually speaking, I spent most of my preparation time finishing up the implementation and making sure it was in a presentable state instead of prepping for the talk itself. I&rsquo;d given a similar presentation on Kintsugi once internally at <a href="inkandswitch.com">Ink &amp; Switch</a>, so I&rsquo;d already prepared slides and was banking on reusing them mostly unedited for my FOSDEM talk. What I didn&rsquo;t account for was that this first presentation was &lt;20min in length total, whereas FOSDEM slots were 25min. You&rsquo;d be surprised how long coming up with 5min of content takes, from writing a script to making diagrams, but I ended up spending a decent chunk of the time I&rsquo;d allotted to preparing for FOSDEM on revamping the slides with some new cryptography background. As with my <a href="https://kewbi.sh/blog/posts/240609/">OSSNA talk</a>, I ended up with a slightly uncomfortable lack of time to run through the talk, but I was able to get to a decent level of fluency between talking points, and the talk went alright after all.</p>
<p>On the day-of, I was feeling very confident given the smooth trip to Brussels and no hitches on public transport. I arrived to the campus 15 minutes before the Security devroom, which I was presenting in, would start, so I figured I&rsquo;d be able to grab a seat early and check that my HDMI adapter worked. Alas, by the time I rounded the stairwell to the room, I saw the corridor packed with people and a &ldquo;This room is full!&rdquo; sign on the door. The room itself was a smaller lecture theatre, with a capacity of 50 or so. I also had the misfortune of speaking directly after the <a href="https://fosdem.org/2025/schedule/event/fosdem-2025-4204-tightening-every-bolt/">curl maintainer</a>, which was the second talk of the devroom, so everyone had the same idea of coming early for the first talk to snag a seat for the later talks, just like all those FOSDEM guides recommended. I tried to squeeze in before the curl talk, but no one really budged, so I wasn&rsquo;t able to get in until they were calling for the speaker in the hallway for my session. Lesson learned: FOSDEM is much bigger than I&rsquo;d expected, and many more people than I&rsquo;d thought are into security. There were a couple other talks that I wanted to get to, but I ran into the same problems with a full room, so it might be worth just going to soak the atmosphere in in a single room (something like the Lightning Talks room would have been interesting with the amount of variety) and watching through other talks that you&rsquo;re most interested in content-wise afterwards online.</p>
<p>You can find a recording of my talk <a href="https://fosdem.org/2025/schedule/event/fosdem-2025-5266-kintsugi-a-decentralized-e2ee-key-recovery-protocol/">here</a>. You can also read a blog post version with a few more notes on implementation details <a href="https://kewbi.sh/blog/posts/250223/">here</a>. I won&rsquo;t go over my talk content again, but I did want to say that all the questions and couple of email exchanges with folks afterward were very meaningful and honestly heartwarming. Someone even said my talk was their <a href="https://chaos.social/@neverpanic/113930981715315172">favourite of the day</a>, which really made my day.</p>
<p>The projector setup was fairly painless — I was very surprised that my HDMI adapter worked first-try, since it usually gives me some headaches until I apply the correct series of suspend and login incantations to get my displays to be recognized. They gave me a lapel mic and told me to stand within a certain space at the lectern to stay in the camera frame. All of the live streaming and recording was very smooth, with participants being able to chat in the Element room and view the talk in real time. I was very pleasantly surprised with how smooth the video infrastructure was, and perhaps <a href="https://fosdem.org/2025/schedule/event/fosdem-2025-6714-fosdem-infrastructure-review/">their talk</a> might go into this in more depth? After the conference, they emailed me a link to review my video and choose the start/end crop times, which I thought was very professionally done.</p>
<p>A group of students caught me as I was leaving the room to say they liked my talk, which I really appreciated! We chatted for a bit about how I prepared for the talk: my presentation style was more practiced and script-based than the other talks that I&rsquo;d listened to. I&rsquo;ve found that writing out a script and essentially memorizing it helps counter my tendencies towards filler-words and not being very grammatically coherent. While I&rsquo;ve been warned that practicing too much would make one sound stiffer, I think this is where limited prep time actually helped: I knew the gist of the flow, and key verbs and flows of sentences, but it wasn&rsquo;t so rote that I&rsquo;d be reciting the script word-for-word, and I&rsquo;d switch up the order of points or restructure phrases each time I&rsquo;d run through the talk. This underpreparing-to-overprepare technique probably has downsides that I haven&rsquo;t come to notice yet, though, so I don&rsquo;t know if I&rsquo;d recommend this.</p>
<p>In terms of speaker experience, I&rsquo;d say it was pretty good: the recording process was very well-done with a fast turnaround and the audience was very engaged (and it was a full room!) I would have appreciated some earlier guidance on speaker prep, which I later found was published <a href="https://fosdem.org/2025/manuals/program/speaker/">here</a>, and a little more day-of information on when/where to turn up, especially as a first-timer. Major props to the volunteers manning the Security devroom, though — they were very supportive and helpful!</p>
<h2 id="around-fosdem">Around FOSDEM</h2>
<p>FOSDEM manages to have a very high-production-value structure and be incredibly well-run while retaining the right touches of cheeky, hackery culture. The treasure hunt, for example, was a nice touch, and it was hilarious to see the accessories and costumes that different companies put on (I recall an electric-blue PostgreSQL elephant suit and a sea of VLC cone hats). There was also reportedly beer and Club Maté being sold at a couple &ldquo;bars&rdquo; dotted around, and I&rsquo;ve been told this is peak hacker-core. In the hallways, everyone would good-naturedly grumble together about missing this or that talk or the endless lines, but it seemed that everyone was having a great time.</p>
<p>My first FOSDEM tip would be to spend an hour or so beforehand looking through the devroom list, mapping out where the most interesting ones are (see next point). There&rsquo;s so many talks (this year had 1104 events) that it&rsquo;s very difficult to narrow things down as you&rsquo;re wandering between buildings day-of. As I mentioned above, it might be helpful to limit yourself to just a couple devrooms even if there&rsquo;s particular talks that you really want to see, since it takes time to go between rooms and shuffle through the crowd. I think a great part of FOSDEM is just sitting and soaking up the energy, which is a bit harder to do if you&rsquo;re darting around between each talk. Devrooms also operate on different cadences (e.g. 20min talks, 15min talks) and start times, so it gets tricky to coordinate when to switch between physical locations. Come up with an ideal talk itinerary, then fill it in with other talk options. Leave time to explore the stands too, and don&rsquo;t get too attached to your schedule if something else catches your eye day-of.</p>
<p>Another tip would be to look up the campus map and familiarize yourself a bit with where the major buildings are (U, which has A/B/C/D blocks, H, etc.) ahead of time. The signage is good once you get into a building, but in the main avenue I did have to stop to look at the large signposts and pinpoint where buildings were a couple times. There&rsquo;s <a href="https://play.google.com/store/apps/details?id=be.digitalia.fosdem&amp;hl=en">FOSDEM companion apps</a> (<a href="https://apps.apple.com/us/app/fosdem-app/id1513719757">iOS</a>) that help visualize this. These apps can also help you keep track of your talk wishlist as well as figure out which other devrooms are close by should you miss your first choice or if it&rsquo;s full.</p>
<p>There&rsquo;s a few nice green spots on campus: there&rsquo;s a lawn with steps by the parking lot by the K building, facing some smaller buildings and a bit further away from the bustle of the main road. I saw lots of people picnicking here, especially with the sunny weather. The law building at the end of the campus (the one with the nice clock tower spire) also has a little green space that&rsquo;s a little less crowded. I think the buildings that host the info booth, etc. change each year, but these tend to be the biggest buildings with the most popular talks, and thus also very busy. I didn&rsquo;t explore too much beyond the main FOSDEM buildings, so I&rsquo;m sure there&rsquo;s other spots I&rsquo;m missing.</p>
<p>In terms of food, there aren&rsquo;t too many options directly on the campus besides the couple of food trucks down the main avenue. They&rsquo;re not too overpriced, given that they&rsquo;re food trucks at a conference, but it&rsquo;s mostly finger food like burgers and fries and sweets. I might be tainted by my experience of more American events, with a lot more, slightly gimmicky, options. I&rsquo;m not sure if the cafeteria on campus was running, but I saw folks with takeaway containers of pasta, which wasn&rsquo;t sold by any food trucks, so I think some of the sandwicheries and cafés might also have options. Make sure to look for the Mozilla cart giving out free cookies! It&rsquo;s good sustenance while you brave the miles-long queue for any of the trucks. Definitely bring a water bottle — I wasn&rsquo;t looking for water fountains but didn&rsquo;t immediately notice many. You could probably also do with protein bars and skip the food truck lines, but either way there&rsquo;ll be options on campus.</p>
<p>FOSDEM was only my second major conference, so my expectations for swag and boothing were on-par for the largest North American open-source conferences with hefty ticket prices and even steeper booth costs. FOSDEM&rsquo;s stands are very different — much more casual and less corporate presentation posters, and more demos and folks talking about the tech. This also means that there&rsquo;s not much free swag besides stickers, and in fact most of the shirts and bags and whatnot are at a fixed price to help support the tools. I like this approach more, since you can directly financially contribute to the projects and have a bit more of a check and balance before grabbing tons of free swag that you won&rsquo;t ever use back home. I was looking for a neat tote bag to bring back, but I instead got some <a href="https://nlnet.nl/hex/">NLNet stickers</a> and a cute 3D printed Grafana mascot. If you&rsquo;re looking for FOSDEM-specific merch, you can buy them at the information desk with the stands in building K as well as the other info desk in building H. One point where I&rsquo;d like to contradict other survival guides is that you don&rsquo;t have to rush to get merch: where other folks have suggested that you go first thing, I thought the line was absurdly long (although the volunteers kept it moving super quick) and there was plenty of stock left over at the end of the day in all sizes, from what I could see. Since there&rsquo;s only two places you can buy the official merch, it makes sense to avoid the bottlenecks and go later in the day or on Sunday.</p>
<p>I&rsquo;d also suggest seeing if any open-source communities you&rsquo;re involved with or are interested in are hosting side events — as they say, the best part of FOSDEM is the <a href="https://en.wiktionary.org/wiki/hallway_track">hallway track</a> and the fringe events. You can browse through the list of <a href="https://fosdem.org/2025/fringe/">fringe events</a> on the site for larger events, and you should ask around in your friend circles to see if anyone else&rsquo;s going. I unexpectedly bumped into a friend from UBC that was studying abroad in a different country and somehow made their way to Belgium, and I caught up with some friends that I met at a distributed systems summer school. Some communities might not have formal stands, like the <a href="https://summerofcode.withgoogle.com/">Google Summer of Code</a>, who staked out a spot by the entrance to the K building unofficially, and there are plenty of unofficial casual gatherings as well. Historically, there&rsquo;s been a beer night at the <a href="https://www.deliriumvillage.com/bar/delirium-cafe/">Delirium Café</a> Friday/Saturday night, and a big hacker party with light displays and live music at <a href="https://bytenight.brussels/">ByteNight</a> at the Brussels hackerspace. On my part, I went to a closed GSOC mentors/students meetup at BrewDog, and was debating going to ByteNight but instead walked around at night, which was probably a much better idea since Brussels looks so different at night.</p>
<p>One last FOSDEM tip: if you&rsquo;re interested in <a href="https://volunteers.fosdem.org/">volunteering</a>, you can certainly register ahead of time, but you can also just turn up day-of and go to the information booth to ask if they need help. One of my friends came on Saturday, realized that he could volunteer (for the CV points, but still), and asked on the spot if he could volunteer on Sunday. I&rsquo;m sure it&rsquo;s more of a the-more-the-merrier situation, so you could also consider this if you find yourself bored and unable to get into any talks.</p>
<h2 id="treasure-hunt">Treasure Hunt</h2>
<p>This year, FOSDEM also ran a <a href="https://fosdem.org/2025/news/2025-02-01-update-treasure-hunt/">treasure hunt</a> on the side — I think it might have been the first year that one was run, at least from what I found online. I initially passed over the hunt, knowing I&rsquo;d only have a day at FOSDEM and wanting to make the most of it by attending talks, but my curiosity was piqued when I realized that the odd noises playing from a speaker outside Building H were part of the puzzle. I&rsquo;d initially assumed someone had set up some public art, was doing some experimentation, or had connected their speakers to the wrong wires, but I later realized it was because <a href="https://fosdem.org/2025/news/2025-02-01-update-treasure-hunt/">the original treasure hunt signs were stolen</a> and they&rsquo;d had to set up a makeshift station outside.</p>
<p>I was super into puzzling and riddles when I was younger — the <a href="https://www.ted.com/search?q=TED-ED+riddle">TED riddle series</a> was a favourite to binge-watch, and I only started playing CTFs in the pursuit of the guessiest &ldquo;crypto&rdquo; challenges that valued knowledge of obscure ciphers rather than actual math. So it&rsquo;s no surprise that I got nerdsniped, and spent my last hour on the ULB campus trying to decipher the riddles instead of listening to the talk I was sitting in for.</p>
<p>No one managed to solve the riddles during the conference, and given that the signs were made available online, the hunt&rsquo;s still going on and the prize is still up for grabs, at least as far as I know. I didn&rsquo;t manage to find anyone else to drag into this, so I wanted to leave my thoughts as a starting point for others who might still be poking into this. Again, you can see the signs <a href="https://fosdem.org/2025/news/2025-02-01-update-treasure-hunt/">here</a>.</p>
<p>For the first riddle, the little &ldquo;1&rdquo; measure and the mention of the radius made me think of counting up the radius of the concentric circles, which ended up being 15. I&rsquo;m not sure what the &ldquo;one set spins as two&rdquo; or the &ldquo;cradle of worlds&rdquo; were meant to lead to. Googling &ldquo;cradle of worlds&rdquo; brought up some notes about various historic sites, but I can&rsquo;t imagine having to pinpoint any of them in relation to the diagram. The &ldquo;set&rdquo; and &ldquo;union&rdquo; made me think of something related to finding how to split the circles up such that their rotation would reveal something, but that seems farfetched to me. It might also have something to do with the building names and where the location of the next sign in the original hunt would have been, since both S and U were buildings where FOSDEM activities were taking place, but there aren&rsquo;t any room numbers mentioned <a href="https://fosdem.org/2025/schedule/rooms/">here</a> that start with &ldquo;15&rdquo;, and I doubt the organizers would have wanted more people crowding in these rooms.</p>
<p>For the second riddle, it immediately looked like a logical-and equation. I&rsquo;m not sure what base the numbers are supposed to be interpreted as. I tried base-10: doing the chunk-wise and (e.g. <code>102 &amp; 111</code>) and converting back to an ASCII character gave me &ldquo;fped&rdquo;. I also tried octal, which gave me &ldquo;@HA@&rdquo;, and hex, which gave me &ldquo;ĀĐāĀ&rdquo;. Out of these, the base-10 looks most plausible. The question mark being upside down at the bottom of the equation made me think about flipping the results visually, which just looks like &ldquo;pedt&rdquo; and didn&rsquo;t give me any more leads. The &ldquo;speech alone could not house our dreams&rdquo; might also lead to <a href="https://en.wikipedia.org/wiki/Braille_ASCII">Braille ASCII</a>, although the 2/4 digits wouldn&rsquo;t make sense, or other non-text-related encodings.</p>
<p>For the third riddle, I didn&rsquo;t get anything promising from either the excerpt or the diagram. I thought it might have something to do with a map overlaid on top of the ULB campus, but it doesn&rsquo;t seem to match up. I did note that not all the potential intersections are marked, so if you&rsquo;d have extended the lines, there would have been more intersections, but if the treasure hunt was originally designed to be played at one single location, they probably wouldn&rsquo;t want folks to mark up the sign while solving, so I don&rsquo;t think this leads anywhere. There are also some intersections that are super close together, which made me think about processing the coordinates via some scripts, but again, the hunt was originally designed to be played in person without direct access to the graphics. I did count the intersections, of which there are 28.</p>
<p>For the fourth riddle, the audio puzzle, I awkwardly stood in front of the speaker for a few minutes to record the sample, which attracted some odd looks and curious questions. The sample&rsquo;s now available online, which saves you the effort. The ungainliness of recording the speaker was actually a silver lining though. A passersby mentioned that the initial jingle in the soundbite is the <a href="https://en.wikipedia.org/wiki/Lincolnshire_Poacher_(numbers_station)">Lincolnshire Poacher number station</a>&rsquo;s theme. A <a href="https://en.wikipedia.org/wiki/Numbers_station">numbers station</a> is a radio station containing loops of numbers that are ostensibly directed at intelligence agents working abroad to receive messages or decrypt transmissions. This matched what the audio file seemed to be quite well. The audio contains the Lincolnshire Poacher theme followed by groups of numbers, each repeated three times. The deduplicated numbers were:</p>
<pre tabindex="0"><code>10 14 11 15 13 23 6 26 7 8 5 12 19 3 2 17 20 24 16 18 21 4 9 25 22 1
</code></pre><p>It was hard to hear the recording in-person given the background noise and speaker quality, so the <a href="https://ftp.belnet.be/mirror/FOSDEM/video/2025/treasurehunt/four.ogg">OGG online</a> was very helpful. I noted that all the numbers were less than 26, which likely means that they correspond to entries in the alphabet. Applying the <a href="https://www.dcode.fr/letter-number-cipher">A1Z26 cipher</a> gives you <code>JNKOMWFZGHELSCBQTXPRUDIYVA</code>. There are also exactly 26 numbers, and no numbers are repeated, so this led me to believe it&rsquo;s some substitution cipher key, but I&rsquo;m not sure where the ciphertext is.</p>
<p>For the fifth puzzle, the text &ldquo;all threads converge&rdquo; made me think this was likely combining the results of the first three puzzles. Add 103 to the result of the first puzzle (15), take the first three characters of the second riddle&rsquo;s answer (ped, if we consider flipping), then multiply the third riddle&rsquo;s result (28) with 754. If we assume an ASCII encoding, the first character should be 118, which decodes to &ldquo;v&rdquo;, so the first part might be &ldquo;vped&rdquo;. Not sure what the third riddle&rsquo;s result was, but if it was 28 then this bit should be &ldquo;21112&rdquo;. This might have been some location-based riddle, since there <a href="https://www.ulb.be/medias/photo/plan-solbosch01-09-22_1662016341684-png?ID_FICHE=8914">was a V building on the Solbosch campus</a>, but it wasn&rsquo;t marked as part of the FOSDEM activities. In that case the &ldquo;21112&rdquo;, or whatever the number really was, might be a reference to a room number that led you to the last riddle. It&rsquo;s hard to figure out which clues were meant to lead to which physical locations and which were for the riddle at hand, but perhaps the signs&rsquo; riddles weren&rsquo;t meant to lead you to the next riddle and the locations of signs was just a general scavenger hunt?</p>
<p>For the sixth puzzle, it seems pretty obviously ASCII as hex, which gives you <code>aulnvzmhdat</code>. I&rsquo;m not sure if this is too obvious? I figure this might have to do with the substitution cipher key from the fourth riddle, but decrypting it with <code>JNKOMWFZGHELSCBQTXPRUDIYVA</code> as an alphabet gave me more gibberish. This might also depend on the fifth puzzle, since the organizers mentioned puzzles were intended to be sequential.</p>
<p>Other miscellaneous notes:</p>
<ul>
<li>The appendix names and volume numbers, in order, are &ldquo;M3B459&rdquo;</li>
<li>The reference to &ldquo;sol&rdquo; in &ldquo;the book of sol&rdquo; might be a reference to the <em>Sol</em>bosch campus, or maybe the FOSDEM logo that, if you squint, could look like a sun</li>
<li>None of the riddles should require image processing, making a copy of the signs to mark them up, or fixing extremely precise coordinates/measurements, given how the hunt was originally designed for in-person play</li>
<li>The riddles should have some sequential aspect as mentioned by the organizers, so there may be clues I&rsquo;ve missed that need to bleed into the next riddles (or maybe this just refers to the fifth riddle and how you need to solve the first three first?)</li>
<li>I can&rsquo;t make out specific references to what might be a room number or location clue versus what&rsquo;s about the final ciphertext, particularly while playing online</li>
</ul>
<p>Feel free to <a href="https://kewbi.sh/about/#contact">email me</a> if you make any more progress or solve the puzzle — I&rsquo;d love to read a writeup!</p>
<h2 id="tant-de-conseils-touristiques3">Tant de conseils touristiques<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></h2>
<p>I also wanted to say a few words about Downtown Brussels. Because the trip was so short and because I&rsquo;d been so busy moving and settling in in the preceding weeks, this was my most last-minute trip planning ever. I didn&rsquo;t buy tickets for anything ahead of time and was sitting on Google Maps the night before my train trying to piece together an itinerary. I think embracing the fact that I had very limited time and was only going to do very touristy things helped cut down on a lot of the decision fatigue, since planning then reduces to a constraint satisfaction problem of what order to visit the top 10 Brussels must-sees in and what meals/foods I wanted to try. I hadn&rsquo;t learned much about Brussels before, so I had absolutely no idea what the major monuments or foods were, and thus also no expectations for what things would be like.</p>
<p>For my itinerary, I took liberal inspiration from the <a href="https://fosdem.org/2025/sightseeing/">tours that FOSDEM organizes</a> for partners and kids who might not be interested in a tech event. I also had a few recommendations from friends, who, being from Brussels, were mildly jaded about things not being very interesting or authentic, but of course coming from a much younger city and drastically different culture I was fascinated by everything. Here&rsquo;s what I went by:</p>
<ul>
<li>the <a href="https://www.atomium.be/">Atomium</a>, a scaled-up version of an iron atom made for the &lsquo;58 expo with very neat modern art installations</li>
<li>the <a href="https://www.hallegatemuseum.be/">Porte de Halle</a>, one of the last remaining parts of Brussels&rsquo; original city walls</li>
<li>the <a href="https://www.visit.brussels/en/visitors/venue-details.Brussels-Palace-of-Justice.232780">Palais de Justice</a>, the courthouse with amazing panoramic views (and when I was there, an <a href="https://www.thebulletin.be/giant-octopus-symbolising-fantastical-brussels-enjoys-temporary-home-place-poelaert">octopus statue</a> outside)</li>
<li><a href="https://www.ot-honfleur.fr/en/Honfleur/unmissable-in-honfleur/saint-catherine-church/">Sainte Catherine Church</a> and the <a href="https://www.cathedralisbruxellensis.be/en/">Brussels Cathedral</a></li>
<li>the <a href="https://www.brussels.be/grand-place-brussels">Grand Place</a>, the town hall and trades buildings, with some very gilt detailing and the Brussels Museum — my favourite part of the trip and absolutely worth seeing both lit up at night and during the day</li>
<li>the <a href="https://en.wikipedia.org/wiki/Manneken_Pis">Manneken</a> (and Jeanneken and Zinneke) <a href="https://en.wikipedia.org/wiki/Manneken_Pis">Pis</a>, statues of a little boy/girl/dog peeing</li>
<li>the <a href="https://www.grsh.be/">Galeries Royales Saint-Hubert</a>, a covered shopping street not unlike the Parisian covered galleries</li>
<li>The quadrifecta of Bruxellois gustatory delights are waffles, « moules-frites » (mussels with fries), beer, and chocolate. I&rsquo;ll let you make your own conclusions on which places to try and if they&rsquo;re all that they&rsquo;re stacked up to be…</li>
<li>I also just walked around a lot near the Grand Place looking at neat buildings and taking it all in!</li>
</ul>
<p>The list above is generally ordered in how you could potentially approach a day in Brussels, starting from the Atomium, which is a little further out, and ending in the centre of the city by dinner with plenty of time to go on side quests in between. It&rsquo;s easy to get around by tram and bus, with plenty of convenient stops, which you can tap in/out of with your phone&rsquo;s wallet app. Google Maps is perfectly sufficient for navigation: sometimes the French/Dutch names of lines or stops will differ, but it&rsquo;s obvious by context where you are. If I remember correctly, most of the transit instructions/signs are also available in English.</p>
<p>As I mentioned above with the FOSDEM swag, my backpack space was fairly limited, so in terms of souvenirs, I wanted to get something small. I have this thing that I call strategic souvenirs: back home, I have a collection of little engraved glass skylines of cities (<a href="https://www.google.com/search?q=3d+glass+paperweight">like this</a>) and my parents are working on a Christmas ornament collection from around the world. This makes self-control much easier, since I usually only ever buy things in those two categories, and as a bonus both are small and light. I ended up getting a little Atomium keychain that we can use as an ornament, and it&rsquo;s super cute.</p>
<h2 id="conclusion">Conclusion</h2>
<p>I&rsquo;m very grateful for the opportunity to have attended FOSDEM and to share my work to so many. The timings aligned so well this year, making travelling to attend much more feasible. This might be the only time in a while that I&rsquo;ll be able to attend given I&rsquo;m normally based in Vancouver — flying across the Atlantic for a weekend conference is a bit much, especially if it&rsquo;s open-registration and there aren&rsquo;t any travel grants. There&rsquo;re quite a few academic-focused conferences in Europe that I&rsquo;d loved to attend as a volunteer/student, like <a href="https://www.eurosys.org/">EuroSys</a>, but the times don&rsquo;t overlap as well with my current trips and again it&rsquo;s not really worth it if I&rsquo;m normally in Vancouver. Nevertheless, it was interesting to get a glimpse into the European open-source community and travel to a new country, and returning to FOSDEM is definitely on my radar if I&rsquo;m ever in Europe this time of year again.</p>
<p>Speaking of travel, I really enjoyed taking a short weekend trip like this: it feels very low-commitment and overwhelming-in-a-good-way. Europe is so small that you can cross country borders within a couple of hours by train, and it&rsquo;s even faster by flight. Compared to North American travel, where 4H can <em>maybe</em> get you between two cities in the same province (Toronto to Ottawa), transportation feels so fast and is fairly inexpensive. The differences in architecture, landmarks, languages, and local vibe change so drastically in such a small distance. Daytrips to completely different regions and one-night stays in other countries feel so tractable now, and even exploring in a single city has so much to offer. I realize this is a very North American take, since it&rsquo;s just that everything seems so shiny and possible and new since I haven&rsquo;t been exposed to much of the platonic ideal of Europe, but it&rsquo;s been fun exploring so far and I hope to do more short sojourning.</p>
<p>I plan to be back to writing shorter posts more frequently soon. My last few posts have all been on the order of 5K words, which is fun to brain-dump and write, but it leads to more investment in each post, especially if it&rsquo;s more technical and there&rsquo;s a lot of research or explanatory diagram creation required. This makes me procrastinate on writing, and my backlog of smaller ideas just keeps growing. There&rsquo;s a few (shallower) rabbitholes that I&rsquo;d like to write about here that will probably fit in more digestible chunks, so expect to see more of that in the coming months.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Like the Netflix-famous <a href="https://en.wikipedia.org/wiki/Emily_in_Paris">Emily in Paris</a>, I&rsquo;m here to provide more of <del>an American</del> a Canadian perspective.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>&ldquo;D-Day&rdquo; in French. Slightly unrelated: <a href="https://en.wikipedia.org/wiki/Languages_of_Belgium">some notes on French vs. Dutch (and German) in Belgium</a>. It&rsquo;s a fairly major political issue in Belgium, and from what I&rsquo;ve heard, there are entirely separate government systems in each language.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>&ldquo;So much tourism advice&rdquo; in French.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>Kintsugi</title>
      <link>https://kewbi.sh/blog/posts/250223/</link>
      <pubDate>19 Feb 2025</pubDate>
      
      <description>On decentralized E2EE key recovery.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Last term, I wrote a couple of posts on cryptography concepts (on <a href="https://kewbi.sh/blog/posts/241020/">OPRFs</a> and on <a href="https://kewbi.sh/blog/posts/241229/">Lagrange interpolation</a>). In the conclusion of each of these posts, I&rsquo;ve alluded to a new, related project that I&rsquo;ve been working on this last term. That project is <a href="https://github.com/kewbish/kintsugi">Kintsugi</a>, and I&rsquo;ve recently had the opportunity to present it at <a href="https://fosdem.org">FOSDEM</a>, widely regarded as the biggest open-source conference around. I realized that I&rsquo;d never shared in-depth details about Kintsugi or how it works, and I thought I&rsquo;d edit and republish my talk in article format. This post is a capstone to this series of crypto explainers, highlighting how we&rsquo;ve combined ideas from both OPRFs and Lagrange interpolation to create a decentralized key recovery protocol for E2EE platforms.</p>
<p>Kintsugi&rsquo;s elevator pitch goes as follows: Using E2EE apps mean that you&rsquo;re solely responsible for keeping a copy of your keys, but what happens when you lose your device or backup files? E2EE apps like WhatsApp and Signal have recovery mechanisms, sure, but these schemes rely on one centralized provider that manages all the special hardware and servers that might be needed. Kintsugi is instead decentralized, based on a P2P network of recovery nodes. To recover your key, you use your password and communicate a threshold of your recovery nodes. Thanks to some fun crypto, though, these recovery nodes can&rsquo;t brute-force access to your data offline, and even colluding recovery nodes can&rsquo;t gain access to your account. Kintsugi doesn&rsquo;t rely on all of these nodes being reachable, just a threshold of them, so it can remain operational even if some recovery nodes go offline. Kintsugi therefore lets you use a familiar password-based authentication scheme while retaining E2EE security properties and protecting against brute-force and recovery node collusion.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<p>This post will first cover some prior E2EE key recovery work, including the schemes that are used by the most popular E2EE apps today. If you&rsquo;re impatient, you can skip through to the demo <a href="#demo">here</a>. Then, I&rsquo;ll detour into the basics of a couple cryptography primitives to recap this series&rsquo; prior two posts before covering the specific communication flows between recovery nodes. I&rsquo;ll also add some of my hard-learned lessons from implementing Kintsugi in Rust with libp2p and Tauri.</p>
<p>If you&rsquo;d prefer to watch through a video, you can find the recording <a href="https://fosdem.org/2025/schedule/event/fosdem-2025-5266-kintsugi-a-decentralized-e2ee-key-recovery-protocol/">here</a>. The slides are also available <a href="https://emilie.ma/talks/fosdem2025">here</a>. Tab back over to <a href="#implementation-details">the implementation details section</a> after you&rsquo;re done watching, since I don&rsquo;t cover the actual system architecture or the P2P app architecture in much depth in my live talk.</p>
<h2 id="background-context">Background Context</h2>
<figure><img src="/img/250223/1.png"/>
</figure>

<p>So, to contextualize the problem space a bit, let&rsquo;s consider what happens when you lose your phone but want to regain access to data on some app you&rsquo;d installed. With a non-E2EE app, you can just download the app again and log in with the same username and password. Your password can be checked against whatever the app server stores, and the server can return your data. However, if the app is E2EE, then this isn&rsquo;t possible. The app server doesn&rsquo;t store your password, recovery key, or anything to help you decrypt whatever encrypted backup data is stored on the server. There are a couple main recovery mechanisms: to name a few, you can set a recovery PIN, some apps let you designate a recovery contact, or you could store recovery codes or hard copies of recovery files.</p>
<figure><img src="/img/250223/2.png"/>
</figure>

<p>Unfortunately, these schemes each have tradeoffs. For example, Signal and WhatsApp <a href="https://signal.org/blog/secure-value-recovery/">both</a> <a href="https://faq.whatsapp.com/2183055648554771/">use</a> some sort of PIN-based system for recovery. You set your account&rsquo;s recovery PIN, a 4-to-6 digit code, from which a recovery key is derived and used to decrypt your backups. However, such a short code is easy to brute-force, so services must provide some form of rate-limiting. This usually relies on secure hardware, like <a href="https://en.wikipedia.org/wiki/Hardware_security_module">hardware security modules</a>, which will ensure that recovery attempts can only be performed once every so often. These HSMs are fairly expensive to run, though — Signal&rsquo;s preprod system, with a very limited scale, cost over $2K a month to run — and can be difficult to deploy.</p>
<p>There are also recovery contacts, where you can designate a friend or a group of contacts, to help you recover your account. <a href="https://support.apple.com/en-us/102641">Apple iCloud</a> is one such platform that lets you set up a recovery contact. The issue with social recovery schemes, though, is that you have to deeply trust your contacts. While, in some cases, you&rsquo;ll have to provide some personal information during recovery, as iCloud requires, this is often guessable. On some platforms like <a href="https://preveil.com/">PreVeil</a> that enable groups of recovery contacts, a threshold of contacts are able to collude behind your back to reconstruct your recovery key in entirety without your input or knowledge. This isn&rsquo;t good, as it essentially gives your recovery contacts all-powerful recovery access to your account (and remember that an account is only as secure as its least secure recovery option.)</p>
<p>Finally, some platforms like LastPass require you to store a set of recovery codes. Along the same lines, crypto wallets usually require you to store some seed phrase of 24 words or so, from which your private key is derived. These recovery codes are less prone to brute force concerns, since they&rsquo;re longer and higher-entropy, so you don&rsquo;t have to worry about expensive specialized hardware. On the other hand, this requires keeping a copy of the recovery codes or files around. Folks tend to misplace these, either digitally or physically, leading to total account lockout if you ever lose the files altogether. Also, because recovery codes are high-entropy and not as memorable as a password, it&rsquo;s much more difficult to commit them to memory.</p>
<figure><img src="/img/250223/3.png"/>
</figure>

<p>These recovery schemes also rely on some centralized app provider or source of trust, and this centralization comes with its own drawbacks. The model of a central authority controlling all the servers and hardware associated with an app simply doesn&rsquo;t work for some contexts. For example, apps that require metadata privacy, like <a href="https://www.torproject.org/">Tor</a>, wouldn&rsquo;t work well with an all-powerful, potentially-malicious provider. Other applications may be at risk of having their infrastructure shut down if they need to run everything on a single cloud provider, or perhaps the service lacks the concept of a responsible, central authority in the first place, as is the case with instance-based forums, for example. Infrastructure can also be cost-prohibitive, especially if the platform&rsquo;s design requires specialized hardware, like the HSMs that Signal and WhatsApp use. Running these HSMs is complex and expensive, which limits the amount of decentralization possible — you can&rsquo;t really use a volunteer-based operator model like Tor does if the hobbyist contributors can&rsquo;t afford specialized hardware. Overall, while centralized recovery mechanisms work for existing apps, we wanted to explore a new space for recovery protocols in our research, particularly exploring schemes that were completely decentralized and didn&rsquo;t require any specialized hardware.</p>
<figure><img src="/img/250223/4.png"/>
</figure>

<p>This is where Kintsugi comes in! It&rsquo;s a decentralized key recovery protocol based on a peer-to-peer network of recovery nodes. These nodes might be recovery servers run by different providers, end-user devices of social recovery contacts, like your friends, or a mix of the two. With Kintsugi, users recover their keys by providing their password and contacting some threshold <code>t+1</code> of recovery nodes. These recovery nodes each store some share of a secret that&rsquo;s needed to help the user reconstruct their key. Users can also update their recovery nodes at any time, which is something that&rsquo;s ideal in a context where we plan to apply Kintsugi to, for example, recover access to a collaborative doc where you have changing collaborators. Kintsugi defends against brute force attacks on the password, malicious, colluding recovery nodes, and can remain operational with offline nodes.</p>
<p>Before I go further into how Kintsugi works, though, I wanted to go through a quick demo just so you can get a sense of what Kintsugi can help us do.</p>
<h2 id="demo">Demo</h2>

<video src="/img/250223/demo.mp4" controls="controls" muted="muted" style="max-width:100%;border-radius: 8px;margin-bottom: .8em"></video>

<p>You can check out the implementation for this demo <a href="https://github.com/kewbish/kintsugi">here</a>. Kintsugi is implemented as a P2P app in Rust, with <a href="https://libp2p.io/">libp2p</a> for the communication layer and <a href="https://v2.tauri.app/">Tauri</a> with React for the demo app&rsquo;s UI — more about this implementation <a href="#implementation-details">later</a>. This demo just shows the UI of the &ldquo;user&rsquo;s&rdquo; node, but each Kintsugi node can serve as a recovery node for other Kintsugi nodes, and there&rsquo;s no distinction between &ldquo;users&rdquo; and &ldquo;recovery servers&rdquo;.</p>
<p>In this video here, I walk through a user registering with a service that uses Kintsugi. Let&rsquo;s say this service is an encrypted notepad service, and for simplicity, all you can do is read and write to a locally encrypted file. In this case, Kintsugi is used to protect the key used to encrypt this file.</p>
<ul>
<li>In the video, we first walk through registration. The user chooses their username and password, then configures their recovery nodes. The service can also do this opaquely in the background for the user. We&rsquo;re just using a set of bootstrap nodes that I&rsquo;m running separately in the background here.</li>
<li>Then, we write to the encrypted notepad.</li>
<li>Next, we delete the local login file to simulate device loss. This file holds the key that was used to encrypt the notepad, and is used for local login. So now, we no longer have a local copy of the key we need to decrypt the notepad.</li>
<li>We then initiate recovery from our recovery node peers. The mapping from username to recovery nodes is stored in a <a href="https://en.wikipedia.org/wiki/Distributed_hash_table">distributed hash table</a> that can be queried globally without knowing any particular recovery data. We then initiate a recovery request with our username and password, which starts communicating with our recovery nodes behind the scenes.</li>
<li>Now, we can see that the keypair was successfully recovered and that we&rsquo;re logged back in. As well, the notepad can now be decrypted, and we can read back what we wrote earlier. Finally, local login works again, since we&rsquo;ve persisted our key locally again.</li>
<li>We can also change the set of recovery nodes or the threshold for recovery and repeat the same recovery process. This is Kintsugi&rsquo;s recovery node refresh flow.</li>
</ul>
<h2 id="background-context-bis">Background Context, bis</h2>
<p>(This section will duplicate content from the previous blog posts about OPRFs and Lagrange interpolation, but I&rsquo;m keeping it in here because someone mentioned that the diagrams very clearly explained the math, and to be honest I&rsquo;m still riding the high from the acknowledgement!)</p>
<figure><img src="/img/250223/5.png"/>
</figure>

<p>I&rsquo;ll keep the crypto light to keep things accessible, but I wanted to briefly cover the basic concepts behind the main building blocks of Kintsugi here. For one, the cryptographic primitive we use to preserve E2EE properties and ensure that the recovery nodes can&rsquo;t find out anything about the user&rsquo;s password or final recovery key was via an <em>Oblivious Pseudo-Random Function</em>, or an OPRF.</p>
<p>An OPRF is a type of function where a user inputs a secret value, which we can call <code>U</code>, and their server inputs their own secret value, which we can call <code>S</code>. Evaluating an OPRF requires both the user and server to provide their secret values. However, only the user learns the function&rsquo;s output, <code>F(U, S)</code>, whereas the recovery nodes that participated don&rsquo;t learn anything at all: specifically, not the output of the function. Neither party learns anything about the other&rsquo;s secret value.</p>
<p>How does this work? The key is in what we call <em>blinding</em>. You can think of this as wrapping a secret present, like a top hat, in some wrapping paper. The user first blinds their secret value <code>U</code> before sending it to the server, so the server can&rsquo;t actually see what <code>U</code> is. Then, the server operates on this blinded value — maybe think of this as some magic machine that transmutes the secret present into pure gold and gives it back to you. Then, you get to unwrap your present — unblind your output — and enjoy the end result. The server doesn&rsquo;t see what your secret was, but you also don&rsquo;t know metaphorically how the server turned your secret present into gold: by analogy, you don&rsquo;t learn the server&rsquo;s secret <code>S</code> during this evaluation either.</p>
<figure><img src="/img/250223/6.png"/>
</figure>

<p>The other main concept used in Kintsugi is <em>secret sharing</em>. This is how we&rsquo;re able to decentralize the key recovery and distribute trust across multiple recovery nodes. If you&rsquo;ve heard of it before, we used an extension of the ideas in <a href="https://en.wikipedia.org/wiki/Shamir%27s_secret_sharing">Shamir Secret Sharing</a> (SSS). Forgive me if your eyes are glazing over at the sight of a polynomial on the slide, but I promise this is intuitive.</p>
<p>With SSS, there&rsquo;s some secret <code>S</code> we want to split up into shares, and we want to make sure we require at least <code>t+1</code> shares to reconstruct <code>S</code>.</p>
<p>Consider some function with a constant term of <code>S</code>. We can define some polynomial as the SSS polynomial as long as it passes through <code>S</code> at <code>x = 0</code>, so we&rsquo;re able to choose these coefficients at random. We can then take points on the polynomial at different x values, which will be the secret shares. With a technique called <a href="https://en.wikipedia.org/wiki/Lagrange_polynomial">Lagrange interpolation</a> and given enough shares, you can &ldquo;connect the dots&rdquo; given by each of the points and eventually find the unique polynomial that passes through all of the shares. Then you can compute the function at <code>x = 0</code> again, which gives you <code>S</code>. If a polynomial has <code>t</code> of these extra coefficient terms, then you have <code>t</code> variables, represented by a, b, and z, and so on, that you need to find the values of, and taking into consideration the extra <code>S</code> variable you need to also find, you need <code>t+1</code> points in order to solve for these <code>t+1</code> variables. The core TL;DR is that you take your secret <code>S</code>, draw some squiggly polynomial that goes through <code>S</code>, then take points on the curve that you can then connect back later to find your secret <code>S</code> again.</p>
<figure><img src="/img/250223/7.png"/>
</figure>

<p>We used extensions of both OPRFs and Shamir Secret Sharing to actually build Kintsugi. In Kintsugi, we use threshold OPRFs, which are just like an OPRF, except that you have multiple servers, each with their own secret share. You send blinded requests to each of these servers, and you need to get a blinded execution back from t+1 of them to combine together before you unblind things to get your actual encryption key. This concept is explained in more detail in the <a href="https://eprint.iacr.org/2017/363">TOPPSS paper</a> by Jarecki et al.</p>
<p>As well, instead of Shamir Secret Sharing, we use a variant called dynamic, proactive secret sharing, where each recovery nodes&rsquo; secret values can be refreshed while keeping the overall shared secret <code>S</code> the same. This can be thought of as changing the points in the graph in the earlier slide, while keeping the overall value of <code>S</code> the same. We use dynamic, proactive secret sharing so the user can change their recovery nodes at any time — for example, if one becomes untrustworthy or goes permanently offline. Users can then make sure that old recovery nodes can&rsquo;t participate in any new reconstruction efforts, while keeping the shared secret, later used to derive their recovery key, the same. This is described in the <a href="https://eprint.iacr.org/2022/971">Long Live the Honey Badger paper</a> by Das et al.</p>
<h2 id="registration">Registration</h2>
<p>Now, let&rsquo;s finally get into the three main flows you saw in the demo video: registration, recovery, and updating the recovery nodes.</p>
<figure><img src="/img/250223/8.png"/>
</figure>

<p>The first flow we saw in the demo is registration: users start with their password, which they blind. Recall that blinding is an operation that&rsquo;s easy for the user to perform and undo but that the server can&rsquo;t break, which prevents the recovery nodes from finding out the user&rsquo;s password. Users send this blinded password (represented by the yellow present) to their recovery nodes. Users need to get responses from at least <code>t+1</code> recovery nodes, where this <code>t</code> is a threshold that the user can choose during registration. Each of these <code>t+1</code> recovery nodes hold a share of a recovery secret. Note that you don&rsquo;t have to reach out to all of your recovery nodes during this process, so if some of them are down during registration that&rsquo;s fine.</p>
<p>The user takes the blinded password and combines it with the node&rsquo;s secret share via a threshold OPRF, represented by the blue present. Remember how I mentioned with threshold OPRFs, the other parties don&rsquo;t learn anything about the user&rsquo;s password or the final output, which only the user sees? In this case, the user takes the final output, unblinds it, and uses the OPRF evaluation result as another key. This OPRF result key is used to encrypt their recovery key and any recovery data they want to save. The blinding and the threshold-based communication here prevent the recovery nodes from mounting an offline brute-force attack. We call this encrypted backup the encrypted envelope, which is sent to the recovery nodes to be persisted.</p>
<h2 id="recovery">Recovery</h2>
<figure><img src="/img/250223/9.png"/>
</figure>

<p>During recovery, the user performs a similar exchange: they blind their password and send it to the recovery nodes. Again, you don&rsquo;t have to reach all of your recovery nodes, just a threshold, so if some nodes are offline you can still proceed with recovery. The recovery nodes each combine the blinded password with their respective shares and return their blind OPRF evaluations to the user along with the encrypted envelope that they&rsquo;ve stored. The user reconstructs their encryption key via the returned OPRF evaluations and with Lagrange interpolation, and decrypts the encrypted envelope to get their recovery key back.</p>
<figure><img src="/img/250223/10.png"/>
</figure>

<p>These nodes also perform rate-limiting as an additional layer of brute-force protection without requiring any secure hardware. Because you need to wait for a threshold of nodes to return results before you can attempt to reconstruct the key, you&rsquo;re limited by the slowest recovery node. As long as one recovery node correctly implements a recovery attempt rate limit (which we assume in our threat model), the attempts will be rate-limited overall as well. This gets you some nice decentralized rate limiting as a result.</p>
<p>Also, because no single recovery node has the whole recovery secret, you&rsquo;ll need a whole threshold of at least <code>t+1</code> nodes to collude in order to get access to the shared recovery secret. Even then, these colluding nodes must still perform a brute-force attack, since the user&rsquo;s password was blinded, so they can&rsquo;t directly decrypt the encrypted envelope that they&rsquo;ve persisted. This provides protection against colluding recovery nodes.</p>
<h2 id="refreshing-update-nodes">Refreshing Update Nodes</h2>
<figure><img src="/img/250223/11.png"/>
</figure>

<p>If the user wishes to update their recovery nodes, they send a notification to the old recovery nodes. A threshold of the old recovery nodes then use dynamic, proactive secret sharing to refresh their shares, communicating second-order shares of shares to the new recovery nodes (represented by the pink robots). These new recovery nodes might overlap with the old recovery nodes significantly, or indeed be the same set of nodes, or be completely different. The new recovery nodes reconstruct their new shares with Lagrange interpolation, which are then used from this point onwards.</p>
<p>Intuitively, consider that the original secret is split into shares once at each of the original, blue recovery nodes. Each of those shares is split up again in the second step in passing on shares to the new recovery nodes (pink robots). This broadcast just changes which pink robots hold which sub-shares of the original secret, but the underlying shared recovery secret remains the same. If you&rsquo;re interested, I wrote a <a href="https://kewbi.sh/blog/posts/241229/">blog post</a> that explains dynamic proactive secret sharing more, but let&rsquo;s wrap up all this math and take a look at the concrete implementation instead.</p>
<h2 id="implementation-details">Implementation Details</h2>
<p>Kintsugi is written in Rust, and the GitHub repository is available <a href="https://github.com/kewbish/kintsugi">here under the MIT License</a>. The P2P communication builds on the <a href="https://libp2p.io/">libp2p</a> framework, primarily based on the <a href="https://docs.rs/libp2p-request-response/latest/libp2p_request_response/"><code>request-response</code></a> module. libp2p is used <a href="https://docs.libp2p.io/concepts/introduction/users/">by the likes of</a> Ethereum, IPFS, and Filecoin, to name a few. You can read more about its basic model <a href="https://docs.libp2p.io/concepts/fundamentals/overview/">here</a> and see the Rust tutorial <a href="https://docs.rs/libp2p/latest/libp2p/tutorials/ping/index.html">here</a>. The gist of libp2p is that it&rsquo;s structured as an event-based loop, with everything from peer discovery to actual protocol messages implemented as so-called <a href="https://docs.rs/libp2p/latest/libp2p/tutorials/ping/index.html#network-behaviour"><code>NetworkBehaviour</code>s</a> that can define event handlers.</p>
<p>The client UI was developed with the <a href="https://v2.tauri.app/">Tauri</a> framework, using React as a frontend framework for familiarity&rsquo;s sake. Tauri lets you invoke commands, which are defined in Rust, from JS, and likewise emit events, which are handled in JS, from Rust. Tauri requires you to restructure your Rust and web frontend a little: for example, it needed to hold all the mutable state, and it needed to get the right same async runtime plugged in. There were few resources with using it with another async library like libp2p — I found <a href="https://rfdonnelly.github.io/posts/tauri-async-rust-process/">this blog post</a> very helpful for figuring out how to wire libp2p and Tauri together.</p>
<p>The event loop has to be written as a separate future to be managed by the Tauri app handle, and Tauri and libp2p have to be connected via a separate <a href="https://doc.rust-lang.org/std/sync/mpsc/fn.channel.html"><code>mpsc::channel</code></a>. Here&rsquo;s the general architecture:</p>
<figure><img src="/img/250223/architecture-diagram.png"/>
</figure>

<p>You can see this event loop <a href="https://github.com/kewbish/kintsugi/blob/master/src/main.rs#L334">here</a>. For UI events, the commands that are invoked by the Tauri JS side forward extra messages through the <code>mpsc::channel</code> to initiate libp2p requests. There&rsquo;s a receiver <a href="https://github.com/kewbish/kintsugi/blob/master/src/main.rs#L337">here</a> in the event loop that takes in messages from this channel. You need to forward messages via this channel due to how Tauri is managing node state and where the libp2p communication objects are available, scope-wise. Specifically, the <code>#[tauri::command]</code> commands have to take in all parameters from the UI on the JS side, so it doesn&rsquo;t have access to libp2p components. All these commands are responsible for is sending a message to . These libp2p messages are processed <a href="https://github.com/kewbish/kintsugi/blob/master/src/main.rs#L107">here</a> and <a href="https://github.com/kewbish/kintsugi/blob/master/src/main.rs#L146">here</a>, sending responses back or kicking off yet more requests.</p>
<p>We initially started building Kintsugi with the <a href="https://docs.rs/libp2p-gossipsub/latest/libp2p_gossipsub/">gossipsub module</a>, but there were some issues with <a href="https://github.com/libp2p/rust-libp2p/discussions/5696">gossipsub reporting <code>InsufficientPeers</code></a> even when there were multiple recovery nodes subscribed to a topic as well as problems with <a href="https://github.com/libp2p/rust-libp2p/discussions/5731">topic re-creation</a> that blocked development. Someone mentioned in the comments that it might have just been an issue with the <a href="https://github.com/libp2p/rust-libp2p/discussions/5696#discussioncomment-11786456">crate tagging</a>, but I didn&rsquo;t verify if this was the fix since I&rsquo;d already begrudgingly ported everything to the request-response module already anyways. The request-response model worked better with our final design anyways, since the user individually blinds their password for each recovery node and since the recovery nodes only want to communicate directly with the user instead of broadcasting message information around willy-nilly.</p>
<p>Peer discovery is handled by the <a href="https://docs.rs/libp2p/latest/libp2p/mdns/">mDNS module</a>, and for now only takes place on the local network. For distributing global information like the user to recovery node mapping, we use the built-in <a href="https://docs.rs/libp2p/0.55.0/libp2p/kad/index.html">Kademlia DHT module</a>. We had to add some record verification to check signatures and the well-formedness of messages (<a href="https://github.com/kewbish/kintsugi/blob/master/src/kad_interactions.rs#L91">here</a>), but the Kademlia module&rsquo;s event-based structure led to some very annoying callback hell and required a lot of state to be passed around (<a href="https://github.com/kewbish/kintsugi/blob/master/src/kad_interactions.rs#L88">example</a>). If you want to enable message filtering, for message verification, for instance, you have to write event listeners both for the first attempted write to the DHT as well as the filtering request that has to perform the actual write. This complicated event structure was one of my main frustrations with libp2p. It&rsquo;s likely because I don&rsquo;t come from an event-driven programming background, but I found the model to lead to convoluted code that was hard to visualize without drawing out an explicit data flow graph.</p>
<p>There were also limited examples for what I needed references for (e.g. a very stripped down Kademlia basic example without peer discovery), although the docs and <a href="https://github.com/libp2p/rust-libp2p/tree/master/examples">existing example apps</a> were mostly enough to piece things together. I also hadn&rsquo;t gotten much experience with async Rust beforehand, so there was an extra learning curve on top of the framework details, even after going through the <a href="https://rust-lang.github.io/async-book/">Async Rust handbook</a>.</p>
<p>One very nice thing about libp2p was that the network communication (at least over a local network, I didn&rsquo;t mess with any NAT hole-punching) worked flawlessly. As I was explaining my libp2p woes to someone one day, they mentioned that with P2P networks, it&rsquo;s usually always the network that causes issues. I&rsquo;m happy to report this wasn&rsquo;t the case at all. This was fairly surprising, since I was developing on <a href="https://en.wikipedia.org/wiki/Eduroam">eduroam</a> for most of the project, and I&rsquo;d expect there&rsquo;s plenty of firewalling and broadcast limitations implemented.</p>
<p>Some other implementation notes:</p>
<ul>
<li>I wouldn&rsquo;t recommend integrating Tauri with all the libp2p events in one fell swoop. Instead, I&rsquo;d suggest starting from a smaller example and adding on only when the basic communication is working, since this limits the amount of refactoring you need to do if something all goes wrong (e.g. if you need to switch <code>NetworkBehaviour</code>s). This sounds very intuitive, but it&rsquo;s very tempting to build out all the libp2p communication, then just slap Tauri on top, but it took ages to debug.</li>
<li>Sometimes when libp2p messages don&rsquo;t seem to be getting sent properly, check if you have a mutex deadlock somewhere, particularly if you&rsquo;re also letting Tauri manage your shared node state with an <code>Arc&lt;Mutex&lt;T&gt;&gt;</code> or something similar.</li>
<li>It was easy to overlook in the docs for <a href="https://v2.tauri.app/develop/state-management/#async-commands">async commands</a>, but make sure the Tauri app handler has its async runtime set properly (<a href="https://github.com/kewbish/kintsugi/blob/master/src/main.rs#L312">here</a>).</li>
<li>Migrating to Tauri&rsquo;s minor idiosyncrasies with all the annotations, and the event-based loop that libp2p required, gave the project a natural tendency towards putting all the code in one file so various callbacks would be in one place and there wouldn&rsquo;t be any scoping or visibility issues. At one point, my <code>main.rs</code> was some &gt;2K LoC, which was still tenable but unsustainable for explaining the project to anyone. It was more straightforward to refactor than expected, but again I&rsquo;d advise starting with a more logically separated crate structure from the get-go.</li>
</ul>
<p>Overall, libp2p took some time to get used to, particularly when integrating it with Tauri, but the event-loop-based approach is starting to grow on me as I&rsquo;m becoming more comfortable debugging issues. If I were to reimplement Kintsugi, I&rsquo;d be happy going with this stack again.</p>
<h2 id="conclusion">Conclusion</h2>
<p>So that&rsquo;s been a whirlwind walkthrough of Kintsugi: I&rsquo;ve walked through some of the reasons why existing E2EE key recovery methods are lacking, gone through some fun intros to crypto, and finally touched on how Kintsugi works. With Kintsugi, our main improvement over existing recovery methods is our focus on decentralization. We also don&rsquo;t require any expensive hardware, can tolerate device loss, provide brute-force resistance, and protect against colluding recovery nodes. Here&rsquo;s the &ldquo;money slide&rdquo; conclusion from the talk:</p>
<figure><img src="/img/250223/12.png"/>
</figure>

<p>Kintsugi is currently perhaps on a bit of a pause. I&rsquo;ve finished the protocol demo that you can see on <a href="https://github.com/kewbish/kintsugi">GitHub</a>, after much libp2p wrangling and a frantic grind towards getting the demo fully functional for FOSDEM. After I wrapped up the MVP demo, I ended up needing to focus on moving to my next internship and prepping for the FOSDEM talk, and have since not had much time to make progress. There&rsquo;s still plenty to look forward to in the future though: I should really continue polishing up the implementation, as there&rsquo;s plenty of (noncritical) loose ends like Byzantine value agreeemtn that I haven&rsquo;t fully implemented yet. There&rsquo;ve also been some vague discussions of integration with the Ink &amp; Switch decentralized access control project <a href="https://www.inkandswitch.com/beehive/notebook/">Beehive</a>, and I&rsquo;ve gotten some other messages from and chatted with folks interested in potentially using Kintsugi&rsquo;s theory in their own E2E platforms.</p>
<p>Speaking of getting questions and emails, I&rsquo;m reminded of something my UBC supervisor said the last time I gave a talk, about how it&rsquo;s nice to take academic research to industry conferences, both since it encourages you to ensure your work can be repackaged and massaged into something that&rsquo;s immediately impactful to real developers, and for reaching a new audience beyond at-times stuffy academia with different considerations and goals. Getting to think through these questions is a nice side effect of the opportunity and a very-much-appreciated affirmation that my work matters in some small way. It&rsquo;s also unique feedback that helps you better think through and articulate the project in the future. For example, I got a couple reoccurring questions about what happens if some recovery nodes are down, or if not all the initial recovery nodes are reachable upon registration, so I&rsquo;ve tried to do a better job in this blog post to reiterate that only a threshold of nodes need to be reached.</p>
<p>I&rsquo;ll be presenting Kintsugi again at the International Workshop on Security Protocols in March, and the proceedings should be available not long after then. I&rsquo;ve set a reminder to update this post with a link to the paper once it&rsquo;s out, as it explains some of the finer-grained details with more nuance and clearer caveats. Leaving this as a placeholder here until I&rsquo;ve put it up — you can also feel free to subscribe to notifications on <a href="https://github.com/kewbish/kintsugi/issues/1">this GitHub issue</a>, as I&rsquo;ve also set a reminder to comment there once I can upload a version of the paper to the repo.</p>
<p>That&rsquo;s about it from Kintsugi for now, but I&rsquo;m also writing up my FOSDEM experience in a separate post. I only attended for the first day, but I&rsquo;ll still be touching on some of the other talks I attended and some &ldquo;survival tips&rdquo; in the vein of <a href="https://petersouter.xyz/fosdem-survival-guide/">these</a> <a href="https://ounapuu.ee/posts/2024/02/12/fosdem-2024/">posts</a>. If you&rsquo;re less concerned with cryptography and more interested in treasure hunts, tangents about trip planning, and survival guides, look forward to that being out next week!</p>


<style>
figure {
margin-bottom: .8em;
}
</style>


<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Elevator rides are &lt;2min, which converts to around ~260 words at an average speaking rate. This explainer was ~160 words, so maybe you&rsquo;d even have a chance to brandish a <a href="https://kewbi.sh/blog/posts/240811/">NFC ring</a> to exchange contact information to take further questions…&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>Snakes and Lagrangian Ladders</title>
      <link>https://kewbi.sh/blog/posts/241229/</link>
      <pubDate>29 Dec 2024</pubDate>
      
      <description>On secret sharing and interpolation.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>I recently had the pleasure of reading a dense crypto paper, emailing the corresponding author in frustration to ask questions, then double emailing two hours later to say I&rsquo;d figured it out. The paper in question was Yurek et al.&rsquo;s <a href="https://eprint.iacr.org/2022/971">&ldquo;Long Live The Honey Badger: Robust Asynchronous DPSS and its Applications&rdquo;</a>: it describes how to split up a secret into shares that can be distributed among a set of people and how to change the set of people and refresh shares while keeping the original secret intact. Secret sharing is important for sensitive but critical information: information that no single party should be able to read on their own, but that we can&rsquo;t afford to lose — think nuclear launch codes and encryption keys.</p>
<p>The problem I was emailing with was me not being able to understand the refresh process. Algorithm 3 in the paper opaquely says a few times to &ldquo;interpolate&rdquo; the desired new secrets. I&rsquo;m sure seasoned cryptographers can recalculate secrets from shares in their sleep, but detangling these three lines took more time than reading the entire rest of the paper. Typical secret sharing via <a href="https://en.wikipedia.org/wiki/Shamir%27s_secret_sharing">Shamir secret sharing</a> is very cleverly constructed already, and as I mentioned in <a href="https://kewbi.sh/blog/posts/241020/">my previous post</a>, crypto papers generally don&rsquo;t do a good job of explaining the basics in favour of just laying down novel material. This is great for brevity and conciseness in academia, but not so great for beginners to the field.</p>
<p>Once I&rsquo;d gotten more background on the very finicky math, the actual interpolation and refreshing wasn&rsquo;t hard to understand and implement. Still, I wish they&rsquo;d linked to a layman&rsquo;s explainer, although from a few cursory searches, none quite seems to exist. This post is the explainer I wish I&rsquo;d had before tackling this paper. I&rsquo;ll cover the very basics of Shamir secret sharing before summarizing the Honey Badger protocol&rsquo;s use of interpolation, explaining the key three lines of Algorithm 3 in the scheme, so you can hopefully save yourself the three hours of puzzling.</p>
<h2 id="eating-the-frog">Eating the Frog</h2>
<p>In my opinion, the hardest part of Shamir Secret Sharing is the Lagrange interpolation, so I&rsquo;m going to put it up front here while your brains are still fresh. Lagrange interpolation is a way of estimating the value of a polynomial at some input, given some points on the polynomial. Let&rsquo;s consider a bog-standard function: <code>f(x) = x^2 + 2</code>.</p>
<figure><img src="/img/241229/interpolation-1.png"
         alt="Figure 1. f(x) = x² &#43; 2."/><figcaption>
            <p><em>Figure 1. f(x) = x² + 2</em></p>
        </figcaption>
</figure>

<p>If we have no knowledge of the polynomial, we&rsquo;ll need three points to estimate the value of <code>f(x)</code> for any <code>x</code> back. This is because <code>f(x)</code> is a polynomial of degree two (highest power term is <code>x^2</code>). There&rsquo;s no <code>x</code> term in the function as I&rsquo;ve written it, but we can also write it as <code>f(x) = 1 * x^2 + 0 * x + 2</code>.</p>
<p>In order to recover <code>f(x)</code>, we need to solve for the coefficients of the terms — the 1, 0, and 2. If we know the values <code>y_i = f(x_i)</code> taken at few different points <code>x_i</code>, then we&rsquo;ll get something like the below:</p>
<pre tabindex="0"><code>y_1 = a * x_1^2 + b * x_1 + c
y_2 = a * x_2^2 + b * x_2 + c
y_3 = a * x_3^2 + b * x_3 + c
</code></pre><p>In this case, we know three values of <code>x</code>, and their associated <code>y</code> values, and we&rsquo;re solving for <code>a</code>, <code>b</code>, and <code>c</code>. Because we have three unknown coefficients, we&rsquo;ll need a system of three equations, based on three evaluated points. In general, for a polynomial of degree <code>t</code>, you&rsquo;ll need <code>t+1</code> points to solve for the polynomial — the extra <code>+1</code> comes from the unknown value of the constant term itself.</p>
<p>To figure out how to reconstruct the polynomial, let&rsquo;s start with a slightly easier function, <code>f(x) = 2x</code>. Let&rsquo;s say we know that <code>f(1) = 2</code> and <code>f(3) = 6</code>.</p>
<figure><img src="/img/241229/interpolation-2.png"
         alt="Figure 2. f(x) = 2x."/><figcaption>
            <p><em>Figure 2. f(x) = 2x</em></p>
        </figcaption>
</figure>

<p>We want to derive a polynomial that takes on the value 2 at <code>x = 1</code> and 6 at <code>x = 3</code>. This is one such polynomial:</p>
<pre tabindex="0"><code>y = (x - 3)/(1 - 3) * 2 + (x - 1)/(3 - 1) * 6
</code></pre><p>Note that when <code>x = 1</code>, the first fraction cancels out to 1 and the second fraction evaluates to 0, so <code>y = 1 * 2 + 0 * 6 = 2</code>, as expected. Similarly, when <code>x = 3</code>, the first fraction evaluates to 0 and the second fraction to 1, making <code>y = 0 * 2 + 1 * 6 = 6</code>. Also note that when we rearrange the terms:</p>
<pre tabindex="0"><code>y = (x - 3)/(-2) * 2 + (x - 1)/(2) * 6
  = -(x - 3) + 3(x - 1)
  = -x + 3 + 3x - 3
  = 2x
</code></pre><p>So we can recover our original function! As another example, let&rsquo;s take our original function <code>f(x) = x^2 + 2</code>. Because this polynomial is of degree 2, we&rsquo;ll need 3 points to reconstruct it. Let&rsquo;s take <code>f(1) = 3</code>, <code>f(2) = 6</code>, and <code>f(3) = 11</code>.</p>
<p>Let&rsquo;s look at the form of our fractional coefficients. We want each fraction to evaluate to 1 at the associated x-value, so the overall term evaluates to the right y-value. We also want the fraction to evaluate to 0 anywhere else. The general form of such a term (let&rsquo;s say, for the point <code>(x_i, y_i)</code>) is to have the numerator of the fraction be a product of <code>(x - x_m)</code>, and the denominator be <code>(x_i - x_n)</code>, for all of the other <code>x_m</code>. We exclude the <code>(x - x_i)/(x_i - x_i)</code> fraction, because when this is evaluated at <code>x_i</code>, we&rsquo;d get <code>0/0</code>, which is undefined. When we evaluate this fraction at <code>x_i</code>, the terms in the numerator all cancel with the terms in the denominator, so we get 1, as desired. When we evaluate this fraction at any of the other input x-values, however, one of the numerator terms <code>(x - x_m)</code> will be 0, preventing this particular y-value from affecting the overall polynomial&rsquo;s value. This fractional coefficient is also called the <a href="https://en.wikipedia.org/wiki/Lagrange_polynomial#Definition">Lagrange basis polynomial</a>, and we&rsquo;ll denote it <code>λ_i(x)</code>. Each y-value is multiplied by a different Lagrange basis polynomial, since the x-value changes, so the <code>i</code> in the basis polynomial also has to change.</p>
<figure><img src="/img/241229/interpolation-3.png"
         alt="Figure 3. Form of the Lagrange basis polynomial."/><figcaption>
            <p><em>Figure 3. Form of the Lagrange basis polynomial.</em></p>
        </figcaption>
</figure>

<p>For our example, our polynomial is thus:</p>
<pre tabindex="0"><code>y = (x - 2)(x - 3)/(1 - 2)(1 - 3) * 3 +
    (x - 1)(x - 3)/(2 - 1)(2 - 3) * 6 +
	(x - 1)(x - 2)/(3 - 1)(3 - 2) * 11
</code></pre><p>If we plug in <code>x = 4</code>, for example, we&rsquo;d expect to get <code>4^2 + 2 = 18</code>. Indeed, we get:</p>
<pre tabindex="0"><code>y = (4 - 2)(4 - 3)/(1 - 2)(1 - 3) * 3 +
    (4 - 1)(4 - 3)/(2 - 1)(2 - 3) * 6 +
	(4 - 1)(4 - 2)/(3 - 1)(3 - 2) * 11
  = 1 * 3 - 3 * 6 + 3 * 11
  = 3 - 18 + 33 = 18
</code></pre><p>Feel free to skip this expansion, but if you&rsquo;re interested, this is what we get when we rearrange the terms:</p>
<pre tabindex="0"><code>y = (x^2 - 5x + 6) * 3/2 +
    (x^2 - 4x + 3) * -6 +
    (x^2 - 3x + 2) * 11/2
  = (3/2 - 6 + 11/2) * x^2 +
    (-15/2 x + 24 x + - 33/2 x) +
	(9 - 18 + 11)
  = x^2 + 0x + 2 = x^2 + 2
</code></pre><p>Even when evaluating this interpolated polynomial at an x-value that wasn&rsquo;t originally provided, like <code>x = 4</code>, for example, we were still able to get the correct result. We can use the same interpolated polynomial to evaluate the y-value at <code>x = 2.5</code>, for example. Using the Lagrange interpolation to recover the y-value at an x-value not included in the list of input points is critical to how Shamir secret sharing works, as I&rsquo;ll describe in the next section.</p>
<h2 id="here-there-be-snakes-sss">Here There Be Snakes (SSS)</h2>
<p>Shamir Secret Sharing starts with the secret <code>s</code> that you want to be able to reconstruct. This might be something like a recovery key or private message. For the sake of explanation, assume this secret is a number. In more realistic scenarios, the secret is likely some series of bytes which need to be transcoded into, or at least interpreted as, large integers, but this is outside of the scope of the SSS algorithm.</p>
<p>We then define a polynomial <code>SSS(x) = s + a * x + b * x^2 + ... + z * x^t</code>. The degree of the polynomial (the choice of <code>t</code>) determines how many shares you&rsquo;ll need to reconstruct the secret <code>s</code>. The coefficients <code>a, b, ...</code> are all randomly chosen numbers. Note that the constant term is the secret <code>s</code> that we&rsquo;re trying to split into shares, and that <code>SSS(0) = s</code>. The SSS shares of the secret are now simply the point <code>(i, SSS(i))</code> for some index <code>i</code> — we need to keep track of the actual index from which a SSS share was calculated for the reconstruction later on. Many such shares can then be calculated and distributed to the other parties who want to be able to help reconstruct the secret.</p>
<p>To recover the secret, at least <code>t+1</code> parties must submit their shares. Using Lagrange interpolation, we can then recover the original <code>SSS</code> polynomial, or in particular, evaluate it at index 0. This gives us the value of <code>s</code> back.</p>
<p>As you might have guessed, SSS&rsquo;s primary application is data recovery. For example, <a href="https://www.preveil.com/">PreVeil</a> is an E2EE platform focusing on email and file collaboration. It <a href="https://www.preveil.com/wp-content/uploads/2024/06/PreVeil_Security_Whitepaper-v1.6.pdf">supports the notion of Approval Groups</a>, a recovery scheme that makes use of SSS, requiring some threshold of designated contacts to recover the user&rsquo;s encryption key. Trezor, the hardware cryptocurrency wallet, <a href="https://trezor.io/learn/a/what-is-shamir-backup?srsltid=AfmBOoq2BNTogcvxxynrx-o49LEF0cLjtxPRuM2F0kTxkLVIu5ZwNry_">makes use of SSS</a> to back up the user&rsquo;s recovery key. The HN- and GitHub-famous <a href="https://github.com/jesseduffield/horcrux">Horcrux</a> tool is one of my favourite SSS applications, just because it&rsquo;s such a fun concept: Horcrux lets you split up a file into encrypted shares, much like Voldemort did with his soul. I mentioned above that the SSS secret is usually some series of bytes interpreted as a large integer — here, Horcrux splits up the file into chunks of bytes to avoid integer overflow and repeats the SSS once per chunk, collecting the i-th shares of each chunk into the overall i-th share.</p>
<p>With SSS, you can choose both your recovery threshold, <code>t</code>, and your total number of shares, <code>n</code>. Having a large <code>n</code> is appealing, since you have more options for who can help reconstruct your secret — in the case where you&rsquo;re trying to recover a key from unreliable P2P nodes, for example, the higher availability from a large <code>n</code> might be desirable. On the other hand, an adversary also has more options for who to compromise in order to learn <code>s</code>. Having <code>n</code> be much larger than <code>t</code> can be problematic in this case.</p>
<p>One key caveat of SSS is that when these applications directly recover data (e.g. split up a recovery key directly into shares), they&rsquo;re vulnerable to malicious parties colluding to recover your data. SSS is usually applied in E2EE contexts as an alternative to the platform servers storing the user&rsquo;s key, so it&rsquo;s problematic if adversaries can compromise a threshold of nodes and directly recover your key. In the case when these parties are social recovery contacts (read: real people), social engineering is also a risk, since there&rsquo;s no easy way to authenticate reconstruction requests, so your contacts might accidentally send their share to a malicious impersonator and leak your recovery data. PreVeil&rsquo;s whitepaper doesn&rsquo;t mention any protections against this, and neither does Trezor. I think the collusion and social engineering concerns are fundamental to vanilla SSS without any additional security considerations.</p>
<p>One solution to these issues is the approach I took in my recent research project, <a href="https://github.com/kewbish/kintsugi">Kintsugi</a>, which involves adding an <a href="https://kewbi.sh/blog/posts/241020/">Oblivious Pseudo-Random Function exchange</a> that protects against the risk of collusion by requiring an additional brute-force step, and against social engineering by not requiring recovery request authentication. The math is quite neat, and this project is what made me look at dynamic proactive secret sharing in the first place. Speaking of, let&rsquo;s get into the Honey Badger protocol now that we have the background on the base SSS protocol.</p>
<h2 id="resharing-is-caring">(Re)sharing is Caring</h2>
<p><a href="https://eprint.iacr.org/2022/971">Honey Badger</a> is the dynamic, proactive secret sharing (DPSS) protocol proposed by Yurek et al. in their 2022 paper &ldquo;Long Live The Honey Badger: Robust Asynchronous DPSS and its Applications&rdquo;. DPSS is used to refresh the secret shares that SSS outputs while keeping the same overall secret <code>s</code>. DPSS both allows users to update the set of parties that possess shares, and invalidates former secret shares, preventing them from being used to reconstruct the secret <code>s</code>.</p>
<p>Recall that the SSS polynomial has the form <code>SSS(x) = s + a * x + b * x^2 + ... + z * x^t</code>. I&rsquo;ll use the term &ldquo;party&rdquo; to refer to the party that holds a secret share, be that a service provider, server, or friend. The core idea of Honey Badger is for each party at index <code>i</code> that has the old SSS secret share <code>s_i = SSS(i)</code> to generate a new SSS polynomial:</p>
<pre tabindex="0"><code>SSS&#39;_i(x) = s_i + a&#39; * x + b&#39; * x^2 ... + z&#39; * x^t&#39;
</code></pre><p>The constant term of this new polynomial is the former SSS secret share. The coefficients <code>a', b', ... z'</code> are new random coefficients, and the degree of the polynomial can change to a new threshold <code>t'</code>. The new secret share that results from the DPSS process will be different than <code>s_i</code>, and if they differ, a threshold of <code>t'</code> will be required to reconstruct <code>s</code> instead of a threshold of <code>t</code>. (The Honey Badger paper uses the notation <code>χ_i(x)</code> instead of <code>SSS'_i(x)</code>, <code>[s]^i_d</code> instead of <code>s_i</code>, and <code>d</code> instead of <code>t</code>.)</p>
<p>The party at index <code>i</code> then sends an evaluation to each other party at their respective index, <code>SSS'_i(j)</code>. which allows the party at index <code>j</code> to interpolate their new share <code>s_j'</code> (line 208 in Algorithm 3 of the paper). These new party shares can be further interpolated to recover the original secret <code>s</code>. Recall that each original <code>s_i = SSS(i)</code> and <code>s = s_0 = SSS(0)</code>. Similarly, each party&rsquo;s new share <code>s'_i</code> is interpolated as <code>SSS'_0(i)</code>, or alternative shares of <code>s_0 = s</code>. Once the party at index <code>i</code> has collected a threshold of shares of the form <code>SSS'_1(i)</code>, <code>SSS'_2(i)</code> and so on, they can interpolate :</p>
<pre tabindex="0"><code>s&#39;_i = λ_1(0) * SSS&#39;_1(i) + λ_2(0) * SSS&#39;_2(i) + ... + λ_{t&#39;}(0) * SSS&#39;_{t&#39;}(i)
    = SSS&#39;_0(i)
</code></pre><p>Thus, when these <code>s'_i</code> are interpolated again (during a normal SSS recovery operation, for example), the original <code>s</code> is recovered.</p>
<pre tabindex="0"><code>  λ_1(0) * s&#39;_1 + λ_2(0) * s&#39;_2 + ... + λ_{t&#39;}(0) * s&#39;_{t&#39;}
= λ_1(0) * SSS&#39;_0(1) + λ_2(0) * SSS&#39;_0(2) + ... + λ_{t&#39;}(0) * SSS&#39;_0(t&#39;)
= SSS&#39;_0(0)
= s_0 = s
</code></pre><p>Intuitively, consider that the original secret <code>s</code> is split into shares once, with each of those party shares being split up again. This broadcast changes which parties hold which sub-shares of the original secret, although the underlying shared data remains the same. <code>SSS'_i(x)</code> can have a different degree, and therefore a different reconstruction threshold <code>t'</code>, than <code>SSS(x)</code>, allowing users to add or remove recovery parties. This secret refresh can also be configured to run at some desired interval (e.g. once per day) to protect against recovery parties&rsquo; shares being leaked over time.</p>
<p>Alternatively, the paper describes this interpolation process more formally by framing the new <code>SSS'_i(x)</code> as a bivariate polynomial, or a polynomial with two variables, denoted as <code>SSS'(i, x)</code> (the paper uses <code>B(x, y)</code>, see line 207 of Algorithm 3 in the paper). This bivariate polynomial still has the form <code>SSS'(i, x) = s_i + a' * x + b' * x^2 ... + z' * x^t'</code>. Note that <code>SSS'(0, 0) = s_0 = s</code>. The idea of resharing is then to interpolate in one variable first, <code>i</code>, over the evaluations at <code>x = i</code> (this is the tricky part!) to get the various <code>SSS'(0, i) = s'_i</code> shares. Then, to reconstruct the original secret, you can interpolate in the other variable, <code>x</code>, to recover <code>SSS'(0, 0) = s</code>. If looking at concrete code makes this easier to understand, feel free to also take a look at <a href="https://github.com/kewbish/kintsugi/blob/master/src/polynomial.rs#L77">my Rust implementation</a> and its <a href="https://github.com/kewbish/kintsugi/blob/master/src/polynomial_tests.rs#L44">tests</a>.</p>
<h2 id="commitment-issues">Commitment Issues</h2>
<p>The above section focused on how the core resharing of Honey Badger worked. However, you might have noticed that the actual paper also mentions checking <a href="https://en.wikipedia.org/wiki/Commitment_scheme"><em>commitments</em></a>. Commitments hide an actual value and later prove, upon revealing the value, that you haven&rsquo;t changed it in the meantime. In Honey Badger, the commitments serve to prove that the nodes are resharing correct secret shares <code>s'_i</code> to the new set of parties.</p>
<p>Honey Badger uses <a href="https://www.rareskills.io/post/pedersen-commitment">Pedersen commitments</a>, of the form <code>s_i * G + ŝ_i * H</code> (see <a href="https://kewbi.sh/blog/posts/241020/#background-elliptic-curves">my other post</a> if you&rsquo;re less familiar with elliptic curves.) Here, <code>s_i</code> is the secret you&rsquo;re committing to, <code>G</code> is the generator point of the elliptic curve, <code>ŝ_i</code> is a random blinding factor used to keep <code>s_i</code> secret even in the case of brute-force attacks, and <code>H</code> is similarly a random elliptic curve point. Points <code>G</code> and <code>H</code> are public to both the old and new resharing parties. During refresh operations, <code>ŝ_i</code> is also calculated the same way <code>s_i</code> is reshared, with a new <code>SSS'_i(x)</code> polynomial with <code>ŝ_i</code> as its secret constant.</p>
<p>In Honey Badger, parties broadcast their new shares alongside a new commitment, which is then verified at various points (lines 205 of Algorithm 1, 303 of Algorithm 2, and 202 of Algorithm 3). Nodes must also keep track of other parties&rsquo; commitments, though, so they have something to verify new shares against: note that in Algorithm 1, the commitments of all shares are broadcast and stored by all parties. This means that the resharing process requires an additional step to interpolate the new commitments (line 209 of Algorithm 3). This isn&rsquo;t trivial (in contrast to the paper&rsquo;s rather flip suggestion to simply &lsquo;interpolate&rsquo;), because we don&rsquo;t know the other parties&rsquo; new secret shares <code>s'_i</code> or their blinding factors <code>ŝ'_i</code>, so we need to somehow interpolate their <em>new</em> commitments only based on public information.</p>
<p>Let&rsquo;s say that the party <code>j</code> wants to interpolate these new commitments. To set the scene, we start out by knowing <code>c_i = s_i * G + ŝ_i * H</code> for all of the other parties (without knowing their <code>s_i</code> or <code>ŝ_i</code>, or any of the new shares or blinding factors), as well as our new share <code>s'_j</code> and new blinding factor <code>ŝ'_j</code>. We want to figure out <code>c'_i = s'_i * G + ŝ'_i * H</code>.</p>
<p>The key here is to recall how <code>s'_i</code> is calculated with Lagrange interpolation, which we can also apply on the commitments themselves to interpolate the new <code>c'_i</code>. Recall that the Lagrange basis polynomial, <code>λ_i(x_i)</code>, is used as a coefficient for the point at index <code>i</code> to interpolate for the value of the polynomial at <code>x = x_i</code>. Now, instead of interpolating for <code>s_0</code> at <code>x = 0</code>, we want to interpolate for the commitment at index <code>i</code>. This can be written as:</p>
<pre tabindex="0"><code>  λ_1(i) * c_1 + λ_2(i) * c_2 + ... + λ_{t+1}(i) * c_{t+1}
</code></pre><p>We can then expand the form of the commitments <code>c_1</code>, <code>c_2</code>, etc.:</p>
<pre tabindex="0"><code>= λ_1(i)     * (s_1 * G     + ŝ_1 * H) +
  λ_2(i)     * (s_2 * G     + ŝ_2 * H) + ...
  λ_{t+1}(i) * (s_{t+1} * G + ŝ_{t+1} * H)
</code></pre><p>Then, multiply out the Lagrange basis polynomials and collect like terms to move <code>G</code> and <code>H</code> outside. You can then notice that the coefficients of <code>G</code> and <code>H</code> are then exactly the forms of the reshared <code>s'_i</code> and <code>ŝ'_i</code><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<pre tabindex="0"><code>= (λ_1(i) * s_1 * G + λ_2(i) * s_2 * G + ... + λ_{t+1} * s_{t+1} * G) +
  (λ_1(i) * ŝ_1 * H + λ_2(i) * ŝ_2 * H + ... + λ_{t+1} * ŝ_{t+1} * H)
= (λ_1(i) * s_1 + λ_2(i) * s_2 + ... + λ_{t+1} * s_{t+1}) * G +
  (λ_1(i) * ŝ_1 + λ_2(i) * ŝ_2 + ... + λ_{t+1} * ŝ_{t+1}) * H
= s&#39;_i * G + ŝ&#39;_i * H
= c&#39;_i
</code></pre><p>This lets us determine the value of <code>c'_i</code> from the other received commitments without actually needing to learn the secret shares of other parties. You can see this commitment interpolation in code <a href="https://github.com/kewbish/kintsugi/blob/master/src/dpss.rs#L82">here</a>, with tests <a href="https://github.com/kewbish/kintsugi/blob/master/src/dpss_tests.rs#L64">here</a>.</p>
<p>I&rsquo;ll admit I couldn&rsquo;t have figured out how to interpolate these commitments without leaning on ChatGPT — I was barely familiar with single-variable Lagrange interpolation and couldn&rsquo;t fathom how I could manage both the secret sharing and the blinding factor. This term was the first where I bothered to try using it as a learning tool, and I was pleasantly surprised by how decent it was for crypto in particular. Sometimes, it&rsquo;d get the explanation itself wrong, but while walking through it, I&rsquo;d manage to get a key insight that let me fill in the rest correctly. Its errors were fairly easy to spot, especially when translating its output into code: things like mixing up scalar and point addition that an additional reprompt was often enough to fix.</p>
<h2 id="conclusion">Conclusion</h2>
<p>I hope this explainer has highlighted that the crypto isn&rsquo;t as scary as it sounds, and that the primitives of a paper aren&rsquo;t so hard to understand either. I think it&rsquo;d be neat if we required authors to submit a &ldquo;from the basics&rdquo; guide alongside their work, even if just to call out what commonly-understood terms like &ldquo;interpolate&rdquo; mean and where to look for further information<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<p>This isn&rsquo;t intended at all as a dig to the paper, which was otherwise very thorough and well-written. It was easy to read, and I appreciated the various comparison tables and practical benchmarking sections. The paper was a fundamental building block to my recent research work, and I&rsquo;d definitely recommend <a href="https://eprint.iacr.org/2022/971.pdf">giving it a read</a>. There&rsquo;s plenty more aspects of the paper that I haven&rsquo;t covered here, including how high-threshold secret sharing is handled and how they optimize resharing by amortizing batched operations. I wanted to focus this blog post on the main interpolation mechanisms in Algorithm 3, but here&rsquo;s a few more less-fleshed-out notes about Honey Badger:</p>
<div class="grid-element" style="margin-bottom: 0.5em">
  <details>
    <summary>More pointers on Honey Badger&#39;s properties</summary>
    <hr />
    <p>In the resharing section, we learned how we can recover the original secret <code>s</code>, but we still need to distribute the shares to the various parties. The paper outlines two options: one is to assume the presence of some trusted dealer, with full (albeit perhaps temporary) knowledge of the secret, and the other is to use a <em>distributed key generation</em> function (DKG), which can be used for a trustless setup. If we want to go the DKG route, however, the hitch is that Honey Badger is <em>asynchronous</em>, meaning that it&rsquo;ll continue to operate in face of arbitrary network delays. This therefore means that the DKG we choose also needs to be asynchronous. This is tricky, since you&rsquo;d usually expect DKGs to require some element of synchronization and consensus over which nodes to &rsquo;listen to&rsquo; during key generation. Honey Badger calls out <a href="https://eprint.iacr.org/2021/1591">Das et al.&rsquo;s asynchronous DKG</a> in particular — if you&rsquo;re interested in detangling this, I&rsquo;d recommend watching <a href="https://www.youtube.com/watch?v=A-3ZhG-7SI0">their conference presentation</a> for the same paper first.</p>
<p>Besides asynchrony guarantees, Honey Badger is also Byzantine-fault tolerant up to a third of nodes. This means that even if up to a third of nodes go rogue and actively try to submit incorrect shares for resharing or otherwise misbehave, Honey Badger can still continue with its resharing and refreshing operations. This is thanks to the multi-valued validated Byzantine agreement (MVBA) protocols used to agree on which nodes have emitted correct shares. I&rsquo;ll discuss this more in the next section on polynomial commitments. You can find their example implementations <a href="https://github.com/tyurek/dpss/blob/main/dpss/broadcast/tylerba2.py">here</a> — I didn&rsquo;t fully get my MVBA prototype working in time.</p>
<p>One last note I&rsquo;ll make is a distinction between the types of failures that Honey Badger can tolerate. One failure, as I&rsquo;ve just mentioned, is a Byzantine-fault failure, with actively malicious nodes. Honey Badger can handle up to a third of total nodes being Byzantine. On the other hand, it can only tolerate <code>t</code> honest-but-curious nodes that follow the protocol correctly (e.g. don&rsquo;t submit false <code>s_i</code>) but collude. Any more, and they&rsquo;ll be able to reconstruct <code>s</code> due to how SSS works. Otherwise, the SSS can instead tolerate having <code>n - t - 1</code> nodes being offline, since only <code>t+1</code> nodes are required to successfully recover <code>s</code>. I ran into these differences while trying to describe the overall fault-tolerance of a protocol I was developing that made use of Honey Badger, so I think it&rsquo;s worth considering here.</p>

  </details>
</div>

<p>If you&rsquo;re interested in other types of secret sharing, there are so, so many extra offshoots built off the same primitives that you can explore:</p>
<ul>
<li><em>Proactive</em> secret sharing, as mentioned above, is secret sharing where the shares are refreshed while keeping the secret itself the same. <a href="https://link.springer.com/chapter/10.1007/3-540-44750-4_27">Here&rsquo;s one of the seminal papers on this topic.</a></li>
<li><em>Dynamic</em> secret sharing, as mentioned above, lets the set of parties holding shares to change. Usually, this requires proactive secret sharing — otherwise, former shareholders could collude to reconstruct the secret. This is sometimes also called <em>mobile</em> secret sharing. <a href="https://dl.acm.org/doi/10.1145/1880022.1880028">Here&rsquo;s another paper on this.</a></li>
<li><em>Computationally secure</em> secret sharing limits the computational resources required to store shares. Secret sharing can require a lot of storage — growing linearly in the number of shares created. <a href="https://www.cs.cornell.edu/courses/cs754/2001fa/secretshort.pdf">This approach allows for more efficient sharing.</a></li>
<li>Similarly, <em>batched</em> secret sharing makes sharing multiple secrets more space-efficient. The Honey Badger paper includes a batch-amortized variant, and <a href="https://www.sciencedirect.com/science/article/abs/pii/S0020025510004536?via%3Dihub">here&rsquo;s another paper on the topic.</a></li>
<li><em>Verifiable</em> secret sharing allows parties to validate the correctness of a share. <a href="https://eprint.iacr.org/2023/1196.pdf">Here&rsquo;s one paper on this.</a></li>
<li><em>Threshold signatures</em> are slightly different than threshold secret sharing, but there are <a href="https://eprint.iacr.org/2022/1656">interesting papers on this as well</a>.</li>
</ul>
<p>I&rsquo;m sure there are plenty more variants I&rsquo;ve missed above — it became a bit of a game during my initial literature review to figure out which magic keyword combination I needed to find relevant papers.</p>
<p>In other news, my workshop paper on decentralized E2EE key recovery was accepted! It builds on the concepts I&rsquo;ve mentioned in this post and <a href="https://kewbi.sh/blog/posts/241020/">my previous one on OPAQUE</a>. The project is called <a href="https://en.wikipedia.org/wiki/Kintsugi">Kintsugi</a>, a play on how the protocol mends together encryption key backups from distributed shares. The demo implementation is <a href="https://github.com/kewbish/kintsugi">on GitHub</a>, and I&rsquo;ll update with a copy of the paper when that&rsquo;s available. I&rsquo;m pretty proud of the fact I was able to go from knowing very little about cryptography to defining an interesting research direction to implementing and writing a whole paper in less than ten weeks. I&rsquo;m happy that the project had a solid implementation portion and a focus on applied work, since historically I haven&rsquo;t sat well with theory-only projects. Nonetheless, I appreciated the stretch to dive into the technical bits of cryptography, as opposed to glossing over it and reaching for existing libraries. There&rsquo;s still plenty of design considerations to be finalized and details to be polished on the demo, which I&rsquo;ll be working through next, but this project has been an extremely fun and challenging exercise in cryptography and protocol design.</p>
<p>There are still a few papers that I&rsquo;ve waded through and think could be explained much better, so I might make this type of cryptography/systems paper explanation a running series here. Wrestling with papers more deeply more often has been a goal of mine for a while, and distilling it into explainers <a href="https://muratbuffalo.blogspot.com/">à la Murat</a> is helpful for keeping you accountable for carefully reading all the details. I have my eye on a few <a href="https://eprint.iacr.org/2024/887">systems</a> <a href="https://pdos.csail.mit.edu/~petar/papers/maymounkov-kademlia-lncs.pdf">implementations</a> <a href="https://www.usenix.org/conference/osdi20/presentation/dauterman-safetypin">papers</a>, or perhaps a more theoretical look at a <a href="https://eprint.iacr.org/2021/1591">distributed key generation</a> protocol, so we&rsquo;ll see what I come up with.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>The keen-eyed reader might notice that we&rsquo;ve been mixing <em>scalar</em> and <em>point</em> addition for the interpolation process. This has been termed doing the interpolation <a href="https://eprint.iacr.org/2017/363">&lsquo;in the exponent&rsquo;</a> — although the elliptic curve notation used here implies group multiplication, the result is equivalent to as if we&rsquo;d done the commitment interpolation in field arithmetic via exponentiation, then multiplied by the appropriate points.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>It&rsquo;s occurring to me that this might just be what appendices are commonly used for, but ideally this explainer should be in more casual language! We don&rsquo;t need thirty pages of proofs and all these Greek symbols to understand how to connect the dots to find a polynomial.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>Making OPAQUE Clear</title>
      <link>https://kewbi.sh/blog/posts/241020/</link>
      <pubDate>20 Oct 2024</pubDate>
      
      <description>On becoming less oblivious to OPRFs.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>I am not a cryptographer. I&rsquo;ve participated in enough CTFs to have had the refrain &ldquo;never roll your own crypto&rdquo; drilled into my head. Leave the group-modulo-n wrangling to the professionals, as the undertone went. While I won&rsquo;t encourage you to do so, today I&rsquo;ll give you enough of the basics to implement your own key exchange protocol. I&rsquo;ll leave the deploying it to prod to you.</p>
<p>To build up some backstory: I recently met someone, who, when I asked to exchange contacts, told me to add them on Signal, instead of Discord or Whatsapp. This was new. A few years earlier, I&rsquo;d had the same reaction when I needed to join some group chats that were hosted on Telegram<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. I&rsquo;ve just finished <a href="https://mitpress.mit.edu/9780262548182/tor/">a book about Tor</a> for a reading group, and attended a couple talks about <a href="https://en.wikipedia.org/wiki/Mix_network">mixnets</a>. While it might have something to do with the fact that my office is in the <a href="https://www.cambridgecybercrime.uk/">Cybercrime Centre</a>, recent news like the <a href="https://en.wikipedia.org/wiki/Pavel_Durov#2024_arrest_and_indictment">arrest of the Telegram founder</a> and the <a href="https://docs.google.com/document/d/1iWCqmaOUKhKjcKSktIwC3NNANoFP7vPsRvcbOIup_BA/edit?tab=t.0">viral AR I-XRAY glasses</a> have been bringing up topics around privacy and security.</p>
<p>To me, a core thread running through these concerns is the question of who we can trust with our data. There are growing movements to <a href="https://en.wikipedia.org/wiki/DeGoogle">abandon big-tech platforms</a> and use dumb phones without internet. Folks hop between the secure messaging platforms de jour, or at least onto the incumbent, which appears to be Signal. Signal, Protonmail, even something as familiar as Whatsapp: they&rsquo;re all end-to-end encrypted. This means the servers that run these platforms only store encrypted copies of your data, and the only people who can read your funnier-in-your-head texts are you and the intended recipient. This encryption usually uses big random numbers instead of passwords, since passwords are lower-entropy. These platforms also (usually) won&rsquo;t store your keys for you, since that&rsquo;d undermine the whole point of end-to-end encryption. You&rsquo;re therefore the only person in the world who can correctly decrypt any messages that are sent — nice and safe. But what happens when you lose your phone? Sure, the app prompted you to save a recovery file, store a twelve-word recovery phrase, or back up your keys somewhere before proceeding, but like most folks, you&rsquo;d probably just skipped past that step.</p>
<p>Let&rsquo;s use passwords, you say — folks know how to use them, they generally do a better job at recording them somewhere, and there are password manager tools readily available so it&rsquo;s rare you&rsquo;ll forget them. However, even besides the myriad misconceptions about strong passwords and the dangers of people reusing passwords, there&rsquo;s the fundamental problem that the platform&rsquo;s server will have to store your password. Nowadays, it&rsquo;s usually stored hashed, so that&rsquo;s mostly fine — should someone break into the service&rsquo;s database, their only choice is to brute-force or <a href="https://en.wikipedia.org/wiki/Rainbow_table">rainbow-table</a> their way to steal your original password.</p>
<p>Now consider what happens in between you hitting &lsquo;Log in&rsquo; on the auth page and the server processing your password — or more precisely, how the bytes of your password are transmitted. Yes, hopefully in this day and age it goes through TLS, so no one should be able to read it. But the core problem is that <em>it&rsquo;s still in plaintext</em>. Once the server receives your password via HTTP, it still needs to read and process it into a hash, to compare against the stored hash. And how is this processing done? In plaintext. That puts a lot of trust on the server to behave honestly. Even if the server behaves as expected, the hardware it runs on might be vulnerable to attacks: because the password is transmitted in plaintext, it&rsquo;s also in the RAM and cache in plain text. When I was at Cloudflare, I learned about some of the ways the team architected the Workers platform explicitly for better isolation on all levels, guarding against <a href="https://developers.cloudflare.com/workers/reference/security-model/">speculative execution bugs</a>, for example. If the server&rsquo;s using plain passwords, it&rsquo;s vulnerable to SPECTRE and other lower-level attacks like it.</p>
<p>It seems, then, that there&rsquo;s no safe option. Either you have better security at the risk of fallible users losing access to their data, or you get a more familiar user experience at the expense of many layers of security concerns. However, there&rsquo;s a way to augment passwords with some neat cryptography so that you can effectively get the best of both worlds. Enter OPAQUE: a password-based key exchange protocol that lets the end user input a password and save their keys on the server, while not allowing the server any access to the password. It retains the server-has-zero-knowledge properties that we&rsquo;d expect in an E2E setting and requires both user and server to participate in any login attempts, reducing the feasibility of brute-force attacks. OPAQUE was selected for standardization by the IETF over several other similar password-based protocols, including the other top contender <a href="https://en.wikipedia.org/wiki/Secure_Remote_Password_protocol">SRP-6a</a>, because of this defense against brute-force attacks. It&rsquo;s also been implemented by several companies, including <a href="https://blog.cloudflare.com/opaque-oblivious-passwords/">Cloudflare</a> and <a href="https://github.com/facebook/opaque-ke">Facebook</a>.</p>
<p>In my current research internship, I&rsquo;ve been working with variants of OPAQUE as applied to key recovery for E2E services. We&rsquo;re figuring out how to adapt OPAQUE to store and retrieve a user&rsquo;s private key via a password: for example, in cases where they&rsquo;ve lost access to their old devices. We needed OPAQUE&rsquo;s properties: we don&rsquo;t want the server to be able to reconstruct the password and read the user&rsquo;s keys, nor do we want the user to keep the only copy of keys locally and end up susceptible to brute-force if any encrypted info leaks. We&rsquo;re adding some other goodies on top, too, but I had to implement a vanilla version first. When I was doing so, I had to trawl through tens of pages of dense crypto papers and cross reference the <a href="https://www.ietf.org/archive/id/draft-irtf-cfrg-opaque-17.html">OPAQUE spec</a> with the myriad <a href="https://github.com/search?q=opaque+protocol&amp;type=repositories">example repos</a>, but I think the core ideas boil down much more intuitively.</p>
<p>Protocols like OPAQUE shouldn&rsquo;t be secure-by-obscurity, and certainly not secure-by-lack-of-good-high-level-public-explanations. This post aims to give you the walkthrough of OPAQUE I wish I had when I embarked on this project. I&rsquo;ll assume some general CS knowledge, but otherwise I&rsquo;ll provide the context you need if you&rsquo;re starting from scratch. I&rsquo;ve chosen to gloss over some of the related crypto concepts to focus on just what&rsquo;s needed to understand the OPAQUE exchange, but I&rsquo;ve included links and mentioned other keywords if you&rsquo;d like to dive deeper.</p>
<p>I hope this post will serve to make OPAQUE clear and you less wary about crypto — reams of LaTeX can be scary, but I promise this won&rsquo;t be.</p>
<h2 id="background-elliptic-curves">Background: Elliptic Curves</h2>
<p>One of the concepts I&rsquo;ll assume some background in is <a href="https://en.wikipedia.org/wiki/Public-key_cryptography">public-key cryptography</a>. The basic idea is that you store two keys: one public and one private. You share your public key with others, and keep your private key to yourself, as the names may suggest. To encrypt something, you take your private key and the public key of the intended recipient together and do some operations on it, which ensures that only the recipient can decrypt your message, since only they have the private key associated with their public key. The idea is that public keys are derived from private keys using some hard-to-reverse operation, and that you can easily derive the public key from the private key, but not vice versa. For example, multiplying numbers is easy, but factoring numbers is hard, so part of what underlies the <a href="https://en.wikipedia.org/wiki/RSA_(cryptosystem)">RSA cryptosystem</a> is that you can&rsquo;t easily factor numbers to derive the secrets that are used to create the private key<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<p>Elliptic-curve cryptography works on the same principle. The hard-to-reverse operation here is multiplying a point on the curve with a number. Let&rsquo;s build up to multiplying with points on the curve by first adding a point to itself, which we can then repeat <em>n</em> times to get multiplication by <em>n</em>.</p>
<p>We first have to start with the given point on the curve. The formula of this curve will change depending on the particular curve you use (and there are plenty), so I won&rsquo;t go into detail about it now. Pick a point <code>P</code>. To add the point to itself, you&rsquo;ll need to draw the tangent line to the curve at <code>P</code> — this is the line that follows the shape of the curve at point <code>P</code>. Extend the tangent line far enough, and you&rsquo;ll find that it intersects exactly one point on the curve. Call this point the tangent line intersection.</p>
<figure><img src="/img/241020/adding-points-1.png"
         alt="Figure 1. Drawing the tangent line to P."/><figcaption>
            <p><em>Figure 1. Drawing the tangent line to P.</em></p>
        </figcaption>
</figure>

<p>This tangent line intersection point is then reflected in the x-axis to get the doubled point, <code>2P</code>.</p>
<figure><img src="/img/241020/adding-points-2.png"
         alt="Figure 2. Reflecting across the x-axis to get 2P."/><figcaption>
            <p><em>Figure 2. Reflecting across the x-axis to get 2P.</em></p>
        </figcaption>
</figure>

<p>We can then repeat this again, with the tangent line for the point <code>2P</code>, to get <code>3P</code>, and so on. There are several optimizations to do this multiplication faster, including the <a href="https://en.wikipedia.org/wiki/Elliptic_curve_point_multiplication#Double-and-add">double-and-add</a> algorithm.</p>
<figure><img src="/img/241020/adding-points-3.png"
         alt="Figure 3. Repeating to find 3P."/><figcaption>
            <p><em>Figure 3. Repeating to find 3P.</em></p>
        </figcaption>
</figure>

<p>The private key in elliptic curve cryptography is the choice of a number <code>n</code>, and the public key is the curve&rsquo;s basepoint multiplied by <em>n</em>. The curve&rsquo;s basepoint is a point that&rsquo;s defined along with the curve (technically, the generator of the group we&rsquo;ll be operating in) — just consider it as a constant that&rsquo;s handed to you together with the definition of the curve. It&rsquo;s easy to compute the public key given the private key, since the multiplications aren&rsquo;t hard. However, if you want to recover the private key given just the resulting public key point, you&rsquo;ll have to try every single possible value of <code>n</code>, which is assumed to be infeasible. This multiplication is called the <a href="https://en.wikipedia.org/wiki/Discrete_logarithm_records#Elliptic_curves">elliptic curve discrete log problem</a>, as an analogy to the discrete log problem of finding the number <code>x</code> such that <code>g^x = y</code> for some public number <code>y</code>.</p>
<p>One thing to note is that if you know the number <em>n</em> that you multiplied by, it&rsquo;s easy to &lsquo;undo&rsquo; a multiplication as well. You can multiply by its inverse — think of it like doing <code>n * P * 1/n</code> — to recover just your original point. You might wonder why we can&rsquo;t do something similar to recover the number <code>n</code> from our public point, since the curve&rsquo;s basepoint is a public parameter. However, you can&rsquo;t analogously &lsquo;invert&rsquo; a point, so you&rsquo;ll still have to end up trying all the possible values of <code>n</code>. The core takeaway of all this is that division by a number is easy, and division by a point to get the number back is hard.</p>
<h2 id="metam-oprf-asis">Metam-OPRF-asis</h2>
<p>Let&rsquo;s put that elliptic curve theory to work, starting with the main building block of OPAQUE: the oblivious pseudo-random function (OPRF). We can build up what an OPRF is step by step:</p>
<ul>
<li>A function is a mapping from a domain of inputs to a range of outputs.</li>
<li>A <a href="https://en.wikipedia.org/wiki/Pseudorandom_function_family#Motivations_from_random_functions">random function</a> is an entirely random mapping from inputs to outputs. This means the random outputs should be <a href="https://en.wikipedia.org/wiki/Randomness_test">uniformly distributed</a>, which means there&rsquo;s no obvious bias in the outputs.</li>
<li>A pseudo-random function <em>looks</em> like it&rsquo;s an entirely random mapping, but is actually deterministically mapping inputs to outputs. It&rsquo;s very important that it <em>looks</em> like a random function, so it also needs to have a uniform distribution of outputs.</li>
<li>An oblivious pseudo-random function is a random function that requires two people to evaluate, where neither person learns what the other put in — they&rsquo;re <em>oblivious</em> to the other party&rsquo;s input.</li>
</ul>
<p>More concretely, let&rsquo;s describe the two people as a client and a server and define an OPRF as a function <code>F(server_input, client_input)</code> such that the client never learns the server input, and the server never learns the client input nor even the final result of the function. I think <a href="https://en.wikipedia.org/wiki/Oblivious_pseudorandom_function">Wikipedia</a> and other resources do a terrible job of explaining how this is possible, because intuitively, it isn&rsquo;t. How can the server, who&rsquo;s evaluating the function, not know its own output? How can the server also not know the client&rsquo;s input, if it was required as part of the function&rsquo;s inputs in the first place?</p>
<p>The answer lies in the elliptic-curve operations I touched on before. Here&rsquo;s how the OPRF actually works:</p>
<figure><img src="/img/241020/oprf-exchange.png"
         alt="Figure 4. The full OPRF exchange."/><figcaption>
            <p><em>Figure 4. The full OPRF exchange.</em></p>
        </figcaption>
</figure>

<ul>
<li>The client has <code>x</code>, which is a number it wants to keep secret. It multiplies this point by the curve&rsquo;s basepoint, so now we have a point. Let&rsquo;s call this point <em>x_point</em>.</li>
<li>To keep it secret, the client generates a random number <code>r</code>, which is called the <a href="https://en.wikipedia.org/wiki/Oblivious_pseudorandom_function#EC_and_conventional_Diffie%E2%80%93Hellman"><em>blinding factor</em></a>. The client calculates <code>r * x_point</code> and sends that to the server as <code>client_input</code>. The server doesn&rsquo;t know <code>r</code>, so it can&rsquo;t get the original <code>x_point</code> back. This protects the server from learning what the client inputted, but it&rsquo;s easy for the client to undo this later.</li>
<li>The server receives <code>client_input</code>, and multiplies it by its own secret number, <code>server_input</code>. (In the diagram, I called this secret number <code>key</code> to save space.) This makes the output <code>= client_input * server_input = r * x_point * server_input</code>. The server can&rsquo;t learn the actual execution output without the blinding factor <code>x_point * server_input</code> nor the <code>x_point</code> itself, because that pesky <code>r</code> is there.</li>
<li>The client receives <code>output</code>, and multiplies it by <code>1/r</code>. This lets it recover <code>server_input * x_point</code> — remember that division by a number is easy. However, the client still can&rsquo;t learn <code>server_input</code> either — remember that division by a point (<code>x_point</code>) is hard.</li>
</ul>
<p>To summarize:</p>
<ul>
<li>We want to end up with the client&rsquo;s input multiplied by the server&rsquo;s input, without either party knowing the other&rsquo;s value.</li>
<li>The property that dividing by points is hard prevents the server from learning the client&rsquo;s blinding factor or secret point and similarly prevents the client from learning the server&rsquo;s secret.</li>
<li>The property that dividing by numbers is easy enables the client to unblind the result to get the required <code>client_input * server_input</code>.</li>
</ul>
<h2 id="registration">Registration</h2>
<p>That&rsquo;s actually all the hard crypto out of the way! Let&rsquo;s now cover the three main phases of OPAQUE: the registration, login, and key exchange. The first phase is registration, where the user will use their password (a string) in an OPRF exchange to get an encryption key that only they know that they then can use to encrypt their keypair data.</p>
<p>The registration revolves around an OPRF exchange.</p>
<figure><img src="/img/241020/opaque-registration.png"
         alt="Figure 5. OPAQUE registration."/><figcaption>
            <p><em>Figure 5. OPAQUE registration.</em></p>
        </figcaption>
</figure>

<ul>
<li>The client transforms their password into a point on the curve. This is done via <a href="https://datatracker.ietf.org/doc/rfc9380/">&lsquo;hash-to-curve&rsquo; functions</a> that allow you to take arbitrary inputs to points on the curve. This is the <code>x_point</code> in the OPRF explanation above. The client then blinds their password point with some random blinding factor, <code>r</code>.</li>
<li>The server receives this <code>client_input</code>. The server generates a user-specific keypair that&rsquo;ll just be used for this user. This user-specific private key will be the <code>server_input</code> in the OPRF explanation above. It multiplies the <code>client_input</code> with its <code>server_input</code> and returns this value to the client, along with the server&rsquo;s public key.</li>
<li>The client receives this <code>output</code> and unblinds it. The client now has <code>x_point * server_input</code>. Let&rsquo;s call this unblinded output <code>rwd</code> — it&rsquo;ll be used as the key to (symmetrically) encrypt what we call the <em>envelope</em>.</li>
<li>The client generates an <em>envelope</em>, which includes a new keypair that the client will use in communications with this server. It also puts the server&rsquo;s public key into this envelope. The client then encrypts all of this with the <code>rwd</code> and sends the encrypted envelope to the server.</li>
<li>The server saves the encrypted envelope so the user can access it again later. It can&rsquo;t decrypt this envelope, since it has no way of unblinding its output to recover the <code>rwd</code>.</li>
</ul>
<p>Note that in every step of this process, the server will never learn the password nor the <code>rwd</code> used to encrypt the envelope, so the client can safely trust the server to store its information. This is critical for end-to-end encryption systems.</p>


<div class="grid-element" style="margin-bottom: 0.5em">
<details>
<summary>
A fun challenge: given the protocol as specified above, can you find the DOS attack vector?
</summary>
<hr>
<p>
It's possible for a malicious user masquerading as the client to intercept the server's output and unblind it with some random number, making the `rwd` that the malicious user calculates meaningless. This doesn't matter, though, because it can then encrypt jibberish with the `rwd` or just directly send junk to the server, which will happily store it under the original user's identifier. When the original user tries to log in, they won't be able to decrypt the envelope they retrieve from the server, which effectively DOSes their account for any future use. This means there needs to be some way of checking that the original user is the same one who provides the blinded input and the encrypted envelope.
</p>
<p>
My supervisor pointed this out in my initial implementation of OPAQUE, and I was pretty confused when I saw that none of the other implementations on GitHub handled this in any way. I ended up asking <a href="https://github.com/expede">Brooke Zelenka</a> about it, and she pointed me to <a href="https://www.ietf.org/archive/id/draft-irtf-cfrg-opaque-17.html#name-registration">this section of the OPAQUE spec</a>, which states that registration requires some other method of the server authenticating the client to ensure that it's talking to the right one. I think you can get past this if you assume enough things about the communication channel on which messages are exchanged, but I just slapped some signatures onto the messages to ensure authenticity and called it a day.
</p>
</details>
</div>


<h2 id="logging-in">Logging in</h2>
<p>Logging in also relies on a similar OPRF exchange. This time, the user needs to recover <code>rwd</code> so it can decrypt the encrypted envelope that the server returns.</p>
<figure><img src="/img/241020/opaque-login.png"
         alt="Figure 6. OPAQUE login."/><figcaption>
            <p><em>Figure 6. OPAQUE login.</em></p>
        </figcaption>
</figure>

<ul>
<li>The client transforms their password into the same point on the curve, and chooses a new random blinding factor <code>r</code>. The client blinds their password point and sends it over.</li>
<li>The server receives this <code>client_input</code> and multiplies it with the same <code>server_input</code> secret that it used during registration. The server sends this <code>output</code> along with the stored encrypted envelope to the client.</li>
<li>The client receives this <code>output</code> and their envelope. It unblinds the <code>output</code> to recover the <code>rwd</code> and uses the <code>rwd</code> to decrypt the envelope. The user has now recovered their service-specific keypair and can move onto a key exchange or further communications with the server, since the decrypted envelope will include the server&rsquo;s public key.</li>
</ul>
<p>The two main benefits of OPAQUE were that it prevents the client and server from learning anything about what the other party&rsquo;d stored or used to compute the function and that it avoids offline brute-force attacks. We&rsquo;ve previously discussed how the OPRF provides this first property via the blinding factors and elliptic-curve cryptography, but let&rsquo;s also briefly touch on the brute-force attack part. Without the OPRF, you might just encrypt the envelope with your password directly and send that to the server to store. This is both less secure than using a key, which is likely longer and has more entropy, but also allows any malicious parties, including a dishonest server, to intercept your encrypted envelope and mount an offline brute-force attack. In theory, the cryptography should ensure this takes a very long time, but the OPRF gives you the additional guarantee that any attacker must interact with the server to get their password guess multiplied by the <code>server_input</code>. This means the server is able to detect and rate-limit password attempts, making brute-force much slower than it would be otherwise.</p>
<h2 id="the-ke-rry-on-top">The KE-rry On Top</h2>
<p>The final piece of OPAQUE is the key exchange that needs to follow in order to derive a shared secret with which to encrypt all following communication. I focused less on this part of the protocol, since for my project we were only interested in the recovery of the client&rsquo;s keypair from the envelope. As well, once you&rsquo;re done with the OPRF exchanges, you&rsquo;re in some sense &lsquo;back in safe territory&rsquo; — there are plenty of key exchange protocols proposed, and you could probably plausibly choose any one of them. <a href="https://eprint.iacr.org/2005/176.pdf">HMQV</a>, a Diffie-Hellman variant, was chosen in the original <a href="https://eprint.iacr.org/2018/163.pdf">OPAQUE paper</a> for its performance. However, <a href="https://blog.cloudflare.com/opaque-oblivious-passwords/">Cloudflare&rsquo;s OPAQUE explainer</a> leverages TLS as an AKE, and the <a href="https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-opaque-01">specification</a> mentions another variant using a <a href="https://www.iacr.org/cryptodb/archive/2003/CRYPTO/1495/1495.pdf">SIGMA-I</a> Diffie-Hellman variant.</p>
<p>For the sake of completeness, I&rsquo;ll briefly cover the HMQV calculation used in the original OPAQUE paper. We want to derive a shared secret that both the client and server can calculate, and use this as a key going forward.</p>
<ul>
<li>As part of the login (or in a separate round-trip message), the client chooses a random number <code>c</code>. It multiplies the curve&rsquo;s basepoint with this to get a public point <code>C</code>. This is sent to the server.</li>
<li>Likewise, the server chooses a random number <code>s</code> and multiplies it with the curve&rsquo;s basepoint to get <code>S</code>.</li>
<li>Both client and server then compute a couple of hashes (remember that these are effectively numbers, not points.) First, the client computes <code>blinded_session_identity = H(client_identity, server_identity, r)</code> with its blinding factor. Then, let <code>e_u = H(C, server_identity, blinded_session_identity)</code> and <code>e_s = H(s, server_identity, blinded_session_identity)</code>. The exact details of this are less important to the overall protocol — this just ensures you&rsquo;re talking to the right person.</li>
<li>Then, the client computes its shared secret as <code>H((S + e_s * server_public_key) * (c + e_c * client_private_key))</code> and the server computes <code>H((C + e_c * client_public_key) * (s + e_s * server_private_key))</code>. The first parenthesis of each expression contains the public parameters that are known from the other party, and evaluates to a point; the second parenthesis contains the private parameters from the party computing the hash and evaluates to a number.</li>
</ul>
<p>If we expand both sides, we&rsquo;ll see that what&rsquo;s being hashed is the same. Feel free to skip the proof if you&rsquo;re happy to just trust that the above are equal, but it&rsquo;s neat looking at variants of Diffie-Hellman to prove how both client and server derive the same secret. I&rsquo;d encourage you to give it a go — it&rsquo;s very satisfying to see everything fall into place and the base math isn&rsquo;t challenging, though keeping all the variables in line and recognizing when to factor terms in and out can be a nice puzzle. Let <code>G</code> be the curve basepoint:</p>
<pre tabindex="0"><code>  (S + e_s * server_public_key) * (c + e_c * client_private_key) // what the client hashes
= (s * G + e_s * server_public_key) * (c + e_c * client_private_key)
= (s * G) * (c + e_c * client_private_key) + (e_s * server_public_key) * (c + e_c * client_private_key) // distribute the multiplication
= (s * G * c) + (s * G * e_c * client_private_key) + (e_s * server_public_key) * (c + e_c * client_private_key)
= (s * G * c) + (s * G * e_c * client_private_key) + (e_s * server_private_key * G) * (c + e_c * client_private_key)
= (s * G * c) + (s * G * e_c * client_private_key) + (e_s * server_private_key * G * c) + (e_s * server_private_key * G * e_c * client_private_key)
= (s * G * c) + (e_s * server_private_key * G * c) + (s * G * e_c * client_private_key) + (e_s * server_private_key * G * e_c * client_private_key) // rearranged terms
= (s * G + e_s * server_private_key * G) * c + (s * G + e_s * server_private_key * G) * e_c * client_private_key // factor out c and e_c * client_private_key
= (s + e_s * server_private_key) * c * G + (s + e_s * server_private_key) * e_c * client_private_key * G // factor out G
= (s + e_s * server_private_key) * (c * G + e_c * client_private_key * G) // factor out first term
= (s + e_s * server_private_key) * (C + e_c * client_public_key)
= (C + e_c * client_public_key) * (s + e_s * server_private_key) // what the server hashes
</code></pre><p>This completes the authenticated key exchange, and in turn, the OPAQUE protocol!</p>
<h2 id="conclusion">Conclusion</h2>
<p>When I was first looking into implementing OPAQUE, my supervisor sent me the original paper as some helpful reading, but after seeing the PDF was 61 pages long, I bailed out to go read through the <a href="https://blog.cloudflare.com/opaque-oblivious-passwords/">Cloudflare explainer</a> instead. The original paper only has the full protocol laid out on page 47! The rest of the paper, and even the protocol description itself, is very dense — I suppose it&rsquo;s nicely concise for those who have been in the field for long enough that they can parse the math at first glance, but trudging through all of that isn&rsquo;t fun for a first-timer. I found the crypto explainers in general to also be at weird levels of abstraction that didn&rsquo;t immediately make it clear how primitives built together, particularly for topics like elliptic-curve cryptography or OPRFs.</p>
<p>In general, I&rsquo;ve noticed that theoretical crypto papers always start with a sea of security games where they prove the correctness and security of their protocol, but not actually explain the protocol until later, as if the protocol is an afterthought that derives naturally from the security games. Again, this probably makes sense for the cryptographers who are focusing on the security, but you&rsquo;d think you&rsquo;d put your major contribution up front in the paper. Another thing I&rsquo;ve noted about crypto papers is how little discussion they usually have. Unless it&rsquo;s an applied crypto paper where the system has been fully implemented and benchmarked, there&rsquo;s usually at most a page or so of discussion, which mostly consists of re-explaining that they protocol is better than the others that currently exist. The conclusion is also usually on the order of a paragraph or two, which is a far cry from the systems/HCI papers that I&rsquo;d read. As well, one thing that&rsquo;s nice is that the modern crypto papers in major journals are mostly all available freely via the <a href="https://iacr.org/publications/">IACR</a>. Having almost all my references centralized on the IACR archives and the IACR having a sequential numbering scheme have had the side effect of me memorizing the numbers of the key papers I&rsquo;ve been referencing over and over again, to the point that I can type in the first digit out of the three or four digit ID and have Chrome autofill the rest as the first search result. Such is crypto research.</p>
<p>I came into this project not really having much crypto background besides understanding the basics of public key cryptography and what an elliptic curve was. The main resources I used to hack my way through were Wikipedia entries, which were usually less notation-heavy than papers, Cloudflare explainers published via their blog, and, in an unusual-for-me turn, ChatGPT. It&rsquo;s surprisingly decent at explaining crypto and math topics — I was very wary of it getting things wrong, but it turns out even if it&rsquo;s messing up some of the details, it&rsquo;s good enough at imparting the intuition that lets you get a skeleton of an implementation done, enough to check it against the expected outputs from Wikipedia or the actual paper. I did write the code myself, since it wasn&rsquo;t quite understanding the libraries I needed to use, but it did a fair job of pointing me in the right direction or mentioning keywords (even if they were explained in the wrong contexts) that I could check against other resources. I&rsquo;m decidedly less anxious about using ChatGPT as an assistant now, and I expect it to keep giving me enough nudges to make my way to the end of my project.</p>
<p>Anyways, I hope this explainer has been clearer than the usual seas of math notation that never tell you how things fit together or how the intuition works. I&rsquo;m thinking of doing the same sort of explainer for some other cryptography topics that I&rsquo;ve had to wrestle with for my project recently, like Shamir secret sharing. As my supervisor said, warning people not to roll their own crypto is a bit patronizing when you think of it: it discourages folks from really understanding the protocols they&rsquo;re relying on and creates this out-group of folks who think they&rsquo;re not good enough to do crypto. It emphasizes this mindset where people aren&rsquo;t trusted to understand any crypto well enough to not mess it up. While this was probably trying to prevent people from writing their own very easily breakable ciphers and things of that more trivial nature, I think it&rsquo;s a bit of a gatekeep-y refrain. Go forth and implement your own OPAQUE — hopefully this explainer has made you a little less oblivious to OPRFs and OPAQUE and how to build them!</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Admittedly, not generally known for being truly secure, particularly against malicious Telegram employees, but it marketed itself as something different to the conventional chat platforms I&rsquo;d used before then.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>When I say &lsquo;hard&rsquo; in this post, I generally mean &lsquo;currently no one thinks it&rsquo;s possible&rsquo;, but that doesn&rsquo;t roll off the tongue quite so well. If it makes you happier, replace &lsquo;hard&rsquo; with &lsquo;requires exponential-time brute-force&rsquo;, and &rsquo;easy&rsquo; with &lsquo;polynomial-time or less&rsquo;.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>Truth or DARE</title>
      <link>https://kewbi.sh/blog/posts/241006/</link>
      <pubDate>06 Oct 2024</pubDate>
      
      <description>On Darmstadt and distributed systems.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>I think distributed systems are neat, but I also don&rsquo;t think I understand them well enough to warrant my level of interest in them. I&rsquo;ve never taken a distributed systems course, have only stumbled my way through the <a href="https://fly.io/dist-sys/">Fly.io distributed systems challenges</a>, and have never wrestled with more complicated design constraints while architecting something from the ground up. Whenever folks ask me about what particular subfield I&rsquo;m interested in, I don&rsquo;t have a concrete answer. Sure, I&rsquo;ve read <a href="https://books.google.co.uk/books/about/Designing_Data_Intensive_Applications.html?id=zFheDgAAQBAJ"><em>Designing Data-intensive Applications</em></a> and have covered the concepts behind different consistency models at least three times, but I still tended to butcher their distinctions when explaining them. If distributed systems was a band, I&rsquo;d definitely be labelled as a fake fan. I&rsquo;ve wanted to do something about this for a while — I&rsquo;ve wanted to finally <em>understand</em> distributed systems and be a <em>real</em> distsys engineer. So when a great opportunity cropped up, I shipped myself off to Germany to spend a week among brutalist buildings, looming castles, and most importantly, a cohort of grad students and eminent researchers, to see if I could make some progress.</p>
<p>The <a href="https://dare-summer.github.io/">Second ACM Europe Summer School on Distributed and Replicated Environments</a> is not only a bit of a mouthful — it&rsquo;s also a summer program that invites grad students interested in distributed systems together to participate in lectures and labs taught by leading professors in the area. Last year, it took place in Brussels, Belgium, and this year, we were hosted in Darmstadt, Germany, at the <a href="https://www.tu-darmstadt.de/index.en.jsp">Technische Universität Darmstadt</a>. Each day of the program consisted of several chunks of either lectures, which were more formal presentations about some new research, or labs, which were structured like a programming assignment and set us free to tackle some extension problem related to the lecture. A list of the speakers is available <a href="https://web.archive.org/web/20240929083858/https://dare-summer.github.io/speakers/">here</a>, and the program for this year is <a href="https://web.archive.org/web/20240929083842/https://dare-summer.github.io/program/">here</a>.</p>
<p>I&rsquo;m currently an undergrad, so I would normally be ineligible. However, Professor Kleppmann (author of aforementioned <em>Designing Data-intensive Applications</em>) was supervising me for my current research internship and also speaking at DARE, so he encouraged me to apply. I&rsquo;m very grateful that I was invited to apply, and immensely surprised that my prior research experience and quickly-put-together motivation letter sufficed to make up for my lack of a degree.</p>
<p>My previous experience in distributed systems work was primarily through a <a href="https://www.cs.ubc.ca/students/undergrad/courses/specialty">CPSC448 directed studies</a> that touched on formal verification, though I didn&rsquo;t do any actual proving. As well, my work at Cloudflare and at Stripe were fairly relevant: while I wasn&rsquo;t directly working on quote-unquote distributed systems, I was doing infrastructure work that was appropriately flavoured as such. My motivation letter listed these, as well as name-dropping an <a href="https://kewbi.sh/blog/posts/240526/">unrelated paper</a> I&rsquo;d worked on in the software engineering space. I don&rsquo;t think the applications were particularly selective, since the <a href="https://tuda-dare24.hotcrp.com/">HotCRP</a> listed 32/36 submissions accepted, and I&rsquo;d assume at least a couple of those were test submissions. I was still very nicely surprised when I&rsquo;d heard back about my acceptance, though, and I&rsquo;m again very thankful I was granted the opportunity.</p>
<p>I&rsquo;d taken notes during each lecture and wanted to revisit them, so this is a post summarizing my main takeaways from the talks and from the program overall. Each of the subsections here could be a blog post in itself — this will be the longest post on my blog to date, but it&rsquo;s also because it&rsquo;s covering a packed week of dense lecture material. I&rsquo;ve written up my notes in the same order as the lectures occurred, so you&rsquo;ll get to relive the learning at DARE as it happened. Professors and other students: I might&rsquo;ve made some mistakes or simplifications in the below — please forgive me!</p>
<h2 id="martin-kleppmann--bft-decentralized-access-control-lists">Martin Kleppmann — BFT Decentralized Access Control Lists</h2>
<p>Professor Kleppmann&rsquo;s lecture turned out to be very closely aligned to what I&rsquo;ll be working on this term with him, and it was nice getting the high-level summary lecture at DARE before diving into extension work. His talk revolved around a decentralized access control list protocol, where members can be added and removed from a group chat. This seems simple, but there are tricky edge cases when concurrency gets involved: if two people concurrently remove each other, what happens? What happens if a user who is removed by another user concurrently adds their own guest user? The protocol also needs to be Byzantine-fault tolerant, meaning that the system must continue to provide a consistent access control model despite having nodes that don&rsquo;t correctly follow the protocol. All in all, not as easy as it seems.</p>
<p>We started by covering some of background — how can you implement a BFT system. One approach is a version vector, where every replica keeps track of its idea of the state of the other replicas. However, they aren&rsquo;t safe in a Byzantine context, since a malicious replica can send different updates to different nodes with a version vector crafted to confuse the two nodes into thinking they&rsquo;re in sync when they&rsquo;ve actually diverged.</p>
<p>You can solve this, though, by using a hash graph, like in Git. Each &lsquo;commit&rsquo; or operation contains the hash of the previous one, which implicitly will transitively include the hash of the predecessor&rsquo;s predecessor, and so on. If the hashes of the most recently received operation are the same, you can assume that two nodes are in sync, relying on the property that cryptographic hash collisions are hard to find. Otherwise, you can keep moving backwards in the graph until the two nodes converge on a common point, then exchange all updates after that point. This is good for a Byzantine context, but is inefficient in the number of roundtrip exchanges, since both nodes will have to continuously communicate as they &lsquo;move backwards&rsquo; in the hash graph. It&rsquo;d be nice if we could figure out the set of updates to share more efficiently.</p>
<p>Generally, you can speed up set membership checks by using <a href="https://en.wikipedia.org/wiki/Bloom_filter">Bloom filters</a> to approximate things. <a href="https://martin.kleppmann.com/2020/12/02/bloom-filter-hash-graph-sync.html">A paper</a> he worked on applied that exact idea, which seems like such an obvious idea once you think about it. Both nodes can send a Bloom filter of the operations they currently know about and walks backward to check if operations were in the other side&rsquo;s Bloom filter, and if not, sends them over. This repeats until the nodes are in sync again — no more redundant updates. Someone once told me that the best research is the intuitively obvious stuff, and I think this paper is no different. Applying a relatively basic CS concept to a new problem space in exactly the way the concept should be used makes the work both easier to understand and to defend. I hope that I&rsquo;ll be able to hone my &lsquo;finding obvious directions&rsquo; skills more to take advantage of this.</p>
<p>Returning to the hash graph, Professor Kleppmann also went a bit into detail about blockchains. They&rsquo;re overall similar to hash graphs, but instead have a total ordering of blocks, which requires Byzantine-fault tolerant consensus. This is needed in blockchain settings to prevent double-spending of currency, but in this decentralized ACL group application we don&rsquo;t need there to be only one ordering of operations — we just need some sort of convergence. This taught me about the difference between consensus and collaboration: in consensus, nodes decide on one alternative; in colaboration, we can keep all alternatives and need to figure out how to put them together.</p>
<p>With all this background, we then worked through some design decisions around how to handle the ACL group edge cases mentioned above (spoiler alert: handling them is still active research). Some alternative systems were brought up, like Matrix&rsquo;s <a href="https://matrix.org/docs/older/moderation/#:~:text=better%20to%20redact.-,Power%20levels,-The%20next%20line">power levels</a> or some sort of seniority-based system. We ruled out user voting, since that&rsquo;s susceptible to Sybil attacks, where malicious nodes control a coordinated group of users, and social engineering. For our lab, we were given some starter code implementing one such ACL CRDT and were given freedom to explore implementing some of these ideas.</p>
<h2 id="elisa-gonzalez-boix--crdt-fundamentals-and-ambienttalk">Elisa Gonzalez Boix — CRDT Fundamentals and AmbientTalk</h2>
<p>Professor Gonzalez Boix presented their work on AmbientTalk, an actor-based programming language that supports communication across peers without any explicit infrastructure. In particular, it supports volatile connections that might drop any time.</p>
<p>The language is service-based, with events for when services are discovered and when messages are received or sent. You can define custom services in these languages and use the asynchronous messaging and lease primatives to communicate between them. The <a href="https://soft.vub.ac.be/amop/at/introduction">intro page</a> has more examples, but here&rsquo;s an overview of what it looks like:</p>
<pre tabindex="0"><code>whenever: Service
discovered: {
	|ref| // far reference
	when: message_type@FutureMessage()
	becomes: {
		|ref|
		...rest
	}
}

def service := object: {
    def message() {
	...
	return value
	}
}

export: service as: Service;
</code></pre><p>The language has a concept of <a href="https://soft.vub.ac.be/amop/at/tutorial/actors#actors_and_far_references">far references</a>, which as far as I can tell originated in <a href="https://en.wikipedia.org/wiki/E_(programming_language)">E</a>. These far references are like pointers across machine boundaries that work asynchronously. Messages that are sent to far references when connectivity drops are buffered in a queue and will be resent eventually later. Objects are passed between actors as far references, but primitive data is shared via isolates, working in a pass-by-copy fashion.</p>
<p>Most of the talk was about some CRDT fundamentals, which I won&rsquo;t go into here, as well as AmbientTalk&rsquo;s unique features, but I also wanted to mention the fun lab we had working in it. We were playing around with an interactive shopping list example app based on a CRDT that was handling its network communication via AmbientTalk&rsquo;s runtime. It was nice not having to code any of that and seeing things &lsquo;just work&rsquo;. The idea was that the shopping list app was collaborative, so if you specified the same <code>Service</code> name, you&rsquo;d be able to dynamically discover peers on the same network and listen to their messages. This ran into a slight challenge, because there were thirty of us in a small room all competing for the same messages, but we worked around this by all declaring services with slightly different names. Some of us managed to get collaboration to work across devices, but something to do with my firewall or the eduroam network we were on wasn&rsquo;t letting me try that particular feature out. This was still one of my favourite labs of the program, though.</p>
<h2 id="antonio-fernandez-anta--amecos">Antonio Fernandez Anta — AMECOS</h2>
<p>One of Professor Anta&rsquo;s students actually presented a poster about his lecture&rsquo;s research project at our poster session on the first day, so I had seen a bit of the background before the talk, although I&rsquo;ll admit I still didn&rsquo;t fully follow. The work presented was called AMECOS: A Modular Event-based framework for Concurrent Object Specification, In a similar vein to Professor Kaki&rsquo;s talk below, they noted that currently, concurrent objects are generally specified sequentially and assume some way to keep track of the object&rsquo;s state.</p>
<p>They define events as &lsquo;opex&rsquo;es, or &lsquo;operation executions&rsquo;, then define the various consistency models around the properties that these opexes would hold. For example:</p>
<ul>
<li>Linearizability implies a &lsquo;realtime&rsquo; ability to globally read a value written anywhere else immediately after the write finishes, so there can&rsquo;t be simultaneous opexes and any reads will return the latest write opex&rsquo;s value.</li>
<li>For sequential consistency, the process order must be respected, but the action doesn&rsquo;t necessarily have to take effect instantaneously between its invocation and its response.</li>
<li>For causal consistency, we also require process order to be respected, but each process can have its own order of the other read/write opexes as long as it respects causal order between order events. This allows opexes to be executed locally without requiring communication or agreement with other processes.</li>
</ul>
<p>It&rsquo;d be difficult to specify these consistency models sequentially, so they specify the system modularly as an execution. An execution is correct if there&rsquo;s an opex ordering that satisfies the consistency property required. The advantages of this approach are that they don&rsquo;t assume some omniscient power knowing the total arbitration order, that the object&rsquo;s state doesn&rsquo;t need to be tracked, that the object is described only via its interface, and that the object&rsquo;s specification can then be separated from the consistency definitions it needs.</p>
<h2 id="gowtham-kaki--novel-consensus-proof-techniques">Gowtham Kaki — Novel Consensus Proof Techniques</h2>
<p>Professor Kaki&rsquo;s talk centred around novel ideas for proving distributed systems, particularly for modelling consensus via convergence and monotonicity. The current standard approach to modelling distributed systems is via asynchronous message passing, but it&rsquo;s hard to, say, specify a safety invariant for a leader election with message passing. A first attempt might look like checking &lsquo;if A has elected B as its leader and C has elected D as its leader, then B = D&rsquo;, but induction doesn&rsquo;t work for this. This invariant also allows some invalid state transitions.</p>
<p>The key takeaway I had from his talk was his point that strengthening invariants is a valid proof strategy that can unlock the final proof. Counterintuitively, you now need to prove more properties, but you can also assume more starting points in the inductive step. The final inductive invariant in the leader election might include all of the following properties:</p>
<ul>
<li>if a leader has been assigned to a node, a quorum of votes should exist for that leader</li>
<li>if a leader message exists, a quorum of votes should exist for that node</li>
<li>a node can only vote for one node</li>
<li>if a node has voted for some node and a vote message exists, the message and the leader should be the same</li>
<li>if two voting messages came from the same node, they should be the same</li>
<li>if a voting message exists, the node should have voted</li>
</ul>
<p>We can also avoid using an induction approach for the proof and instead rely on the convergence of leadership — the fact that it doesn&rsquo;t matter in what order the voting messages are processed so long as they vote for the same leader — and monotonicity of leadership — you can either vote for no one or the same leader, which you can model via a lattice. This monotonicity and convergence gives you consensus.</p>
<p>You can then model consensus with some way of replicating state, and place these two invariants on it. They found that while this replicated state approach required more messages than the async message passing approach, it instead performed better in terms of throughput. They&rsquo;re still working on this, as they&rsquo;ve mentioned they haven&rsquo;t implemented garbage collection or crash recovery in the evaluation system, but I think the gist is that this is a viable alternative to the typical message passing implementations for consensus.</p>
<p>I had a bit of trouble following these two specification/verification talks, mostly because of the formality of the details introduced, but I&rsquo;ll come back to revisit the material should they come in handy.</p>
<h2 id="carlos-baquero--state-based-crdt-performance">Carlos Baquero — State-based CRDT Performance</h2>
<p>One of the other lectures (I don&rsquo;t recall who) cited the <a href="https://inria.hal.science/inria-00609399v1/document">2011 CRDT paper</a> that Marc Shapiro wrote. I&rsquo;ve read the paper for my past research internship around formal verification, and I know the paper&rsquo;s a foundational one. I didn&rsquo;t realize, though, that Professor Baquero was also a co-author on the paper — I only put two and two together when the speaker was listing out the authors and gestured towards him. A very small world.</p>
<p>Professor Baquero&rsquo;s talk was on CRDTs and an optimization that can be applied to reduce the size of state you need to send back and forth. The general idea is that state-based CRDTs can be inefficient. For example, take an <a href="https://crdt.tech/glossary#:~:text=Add%2Dwins%20set%20(AWSet)%3A">add-wins set</a>: it keeps track of a set of tombstones for removed elements to guard against cases where an element&rsquo;s &lsquo;remove&rsquo; operation is processed before its &lsquo;add&rsquo;. As well, removed elements are typically also kept in the main set, duplicating the storage needed.</p>
<p>This talk had some background about <a href="https://en.wikipedia.org/wiki/Semilattice">join semi-lattices</a> and partially-ordered logs. A join semi-lattice is a type of structure that defines a join operation that you can apply to two states to get the joint state: the join is a bit like a set union with extra spice. Intuitively, the &rsquo;lattice&rsquo; part of the name connects to the fact that when you draw up all the possible joins and states, you end up with a lattice-like shape, with an empty state at the bottom and a final state at the top. This final state is the result of joining all the possible inputs together — there&rsquo;s some maths calling it the &rsquo;least upper bound&rsquo;, but you can think of it like &rsquo;the state that contains all of these other states joined together&rsquo;. Very frequently, this looks like a union if you squint. These states can be defined by applying this join operation based on the operations in a partially-ordered log, or a polog, for short. If you model state for a <a href="https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type#G-Counter_(Grow-only_Counter)">grow-only counter</a> in the polog like <code>{A(1), B(2)}</code>, for example, then you could define your join operation to take two states and do a member-wise max. If we had <code>{A(1), B(2)}</code> and <code>{A(3), B(1)}</code> to join, we would then get <code>{A(3), B(2)}</code>, then sum across all members to get the final grow-only counter value of of 5. Each replica keeps a local view of this polog to derive its state.</p>
<p>This state ends up getting pretty big, however, when you consider <a href="https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type#2P-Set_(Two-Phase_Set)">tombstone sets</a> and more complicated CRDTs. Instead of sending over the whole state, the approach presented therefore relies on deltas and mutations. For example, if you get an update from replica A that it&rsquo;s been incremented twice, just send that as a delta mutation instead of sending over all the extra state for replicas B, C, and D. You can fragment your state into so-called <em>irredundant join decompositions of state</em>, turning something like <code>{A(1), B(2)}</code> into <code>{A(1)}, {B(2)}</code> and send the deltas based on these smaller bits instead.</p>
<p>One of his students&rsquo; PhD thesis builds this out more, explaining how you can decompose a state, hash the members to assign their values to buckets, then calculate what buckets to send over to each replica to limit sending duplicated state. You can also apply bloom filters to efficiently test what needs to be sent over. However, there are some issues with false positives in Bloom filters, so both bucket and bloom filters need to be combined for a robust system. There&rsquo;s a whole four-round system of sending bloom filters, their differences, and buckets across that you can read more about <a href="https://vitorenes.org/publication/enes-efficient-synchronization/enes-efficient-synchronization.pdf">here</a>.</p>
<p>Aside from the content of the talk, there were two sort of offhand points about communication times that&rsquo;ve instead really stuck. One was that the whole world can be connected at a latency supporting FPS games and other real-time applications just because the Earth&rsquo;s diameter is small. It&rsquo;s neat to think about: if the Earth was larger, there could&rsquo;ve been whole classes of apps that would&rsquo;ve never been invented. The other is that communication round-trips to space colonies (e.g. Mars) will dramatically increase from what we&rsquo;re used to on Earth, and could be up to a 20 minute RTT. It was very thought-provoking to consider space tech as a field where CRDTs will likely become necessary — when you have such long response times and occassional periods where communication isn&rsquo;t possible, you can&rsquo;t really lean too heavily on the classic client-server model. I want to learn more about the state of space tech in the future, especially since a fellow Rise Global Winner has just gotten into YC with their <a href="https://bifrostorbital.com/">satellite startup</a>. I think there&rsquo;s a lot of interesting potential with critical systems that&rsquo;ll need certain consistency and consensus guarantees.</p>
<h2 id="mira-mezini--algebraic-rdts">Mira Mezini — Algebraic RDTs</h2>
<p>Professor Mezini talked about replicated data types, particularly the concept of an ARDT, or an Algebraic Replicated Data Type. Her talk presented how ARDTs can be used for decentralized state management, can be used reactively for propagating changes, and can be used for coordination with various consistency and availability guarantees.</p>
<p>An interesting point she brought up was that Brewer, the inventor of the <a href="https://en.wikipedia.org/wiki/CAP_theorem">CAP theorem</a>, now states that the CAP theorem&rsquo;s application should be relaxed in modern systems to fit with the specific system&rsquo;s requirements. Sometimes, we can relax consistency a little bit to allow for more availability, and vice versa. It doesn&rsquo;t have to be an all-or-nothing choice given today&rsquo;s technologies. I liked this take, particularly because I could see it in the previous internship work I&rsquo;ve done — we used <a href="https://en.wikipedia.org/wiki/Isolation_(database_systems)#Serializable">serializable</a> transactions just for this part for high consistency, but for better performance left everything else in <a href="https://en.wikipedia.org/wiki/Isolation_(database_systems)#Read_uncommitted">read-uncommited transactions</a>.</p>
<p>ARDTs were presented as a standard library of composable, primitive data-type RDTs that you could really customize but still have working together. They differ from CRDTs because off-the-shelf CRDTs have fixed design decisions baked into them, like assumptions about various network models, and causes a bit of an impedance mismatch when trying to actually program with data. Using ARDTs allows you to decouple the state of data from its dissemination, so that the communication and network becomes irrelevant to the actual application logic. They felt a bit like Jacky&rsquo;s <a href="https://jzhao.xyz/posts/bft-json-crdt">BFT JSON CRDT library</a>, and to be honest I can&rsquo;t quite articulate the difference between ARDTs and &lsquo;customizable CRDTs&rsquo; better than this.</p>
<p>She presented a subset of ARDTs called reactive ARDTs that would avoid callback hell as well as having to track data dependencies across systems. Right now, it&rsquo;s difficult to model and manage updates in data being propagated correctly to other places given a system with diverse nodes — I think the final chapter of DDIA covers this a little as well. They designed a language, <a href="https://dl.acm.org/doi/10.1145/3191697.3214337">REScala</a>, that operates with these reactive ARDTs and their dependency effect chains as first-class citizens in the runtime. This way, it&rsquo;s easier to understand, since events are modelled as happening instantaneously, but also lets the runtime handle making the effect chains strictly serializable and more consistent.</p>
<p>There was also some work on coordination ARDTs mentioned along this vein, to enforce application level invariants that are difficult to otherwise manage via consensus. They achieved this via adding a verifying compiler, <a href="https://dl.acm.org/doi/10.1145/3633769">LoRe</a>, to REScala. Their coordination works by making use of locking without requiring offline nodes to agree. The system models interactions between systems as having certain pre/post-conditions and actions, so the compiler can check that the overall invariants hold at each step.</p>
<p>Fun note about her slides is that I noticed one of the images she used on a slide about decentralized collaborative applications looked really familiar. Halfway through, it hit me that it was actually the title image from the Ink and Switch <a href="https://www.inkandswitch.com/crosscut/">Crosscut</a> article, which ironically is explicitly a &lsquo;personal thinking space&rsquo; and not a collaborative tool. Say you will about me being able to recognize the Ink and Switch blog post images on sight.</p>
<h2 id="annette-bieniusa--formalizing-broadcast-tla-and-erla">Annette Bieniusa — Formalizing Broadcast, TLA+, and Erla+</h2>
<p>Professor Bieniusa&rsquo;s talk was listed on the program as something to do with reactive datatypes and Elixir, which I was looking forward to finally learning a bit more about, but she actually spoke on specifying different broadcast models in TLA+! TLA+ is a specification language used in formal verification, and it&rsquo;s been used in the industry to verify and model many critical systems, notably including AWS&rsquo;s S3. I&rsquo;ve worked a little with TLA+ in my previous research internship on an extension to the <a href="https://distcompiler.github.io/">PGo</a> project, a compiler that translated Modular PlusCal (which itself compiles to TLA+) directly into production-ready Go systems.</p>
<p>So imagine my surprise when the project Professor Bieniusa talks about is about a project, Erla+, that&rsquo;s almost exactly that, just with Go replaced with Erlang! However, their work compiles from a subset of PlusCal directly into TLA+ and Erlang, whereas PGo requires the use of our custom Modular PlusCal extension language first, so they&rsquo;ve cut out the need to learn new syntax. Also, their compiler produces actor-based systems, which I don&rsquo;t know much about but seem to map quite naturally to having multiple distributed nodes that one needs to coordinate. It was neat to see how much the two projects naturally mirrored each other.</p>
<p>The bulk of her talk was primarily about broadcast, though. We talked through several variations, including best-effort broadcast, reliable broadcast, and uniform reliable broadcast. Best-effort broadcast is just a broadcast where you try your best to deliver messages with no retries or other guarantees. For a reliable broadcast, you force each node to forward its messages, and for a uniform reliable broadcast, you need to ensure all messages are also <em>delivered</em>, or received. Unfortunately, uniform reliable broadcast can&rsquo;t exist if the majority of nodes fail, for obvious reasons. We also defined a few properties that we used in these definitions:</p>
<ul>
<li>FIFO property → if you broadcast <code>m</code> from <code>p</code> then <code>m'</code>, then <code>m</code> is delivered before <code>m'</code></li>
<li>causal property → if you broadcast <code>m</code> from <code>p</code> then <code>m'</code> from <code>q</code>, then <code>m</code> is delivered before <code>m'</code></li>
<li>total order property → if <code>m</code> is broadcast from <code>p</code> before <code>m'</code>, then <code>m</code> is broadcast from <code>q</code> before <code>m'</code></li>
</ul>
<p>We then spent some time learning TLA+ syntax and primitives to formally model these properties. I learned that TLA+ uses something called linear-time logic, which gives you a set of executions that are considered correct. You can then define linear-time properties, like safety and liveness properties. A safety property requires that if any execution is incorrect, then there was a prefix of that execution where the remainder of the execution did not fulfill the property — intuitively, it requires that if something went wrong, there was a particular &rsquo;turning point&rsquo; where things went south. Safety can only be satisfied given infinite time, but can be violated in finite time.</p>
<p>On the other hand, a liveness property requires that for any prefix of an execution, there is a set of following executions for which the property is also satisfied — intuitively, that the execution &lsquo;keeps running&rsquo;. Liveness can conversely only be violated in infinite time and is satisfied in finite time.</p>
<p>To model these in TLA+, you need a couple operators: <code>[]F</code> denotes that <code>F</code> is always true, and <code>&lt;&gt;F</code> that <code>F</code> is eventually true. You can also combine these, so <code>[]&lt;&gt;F</code> states that at all times, <code>F</code> is either true or will be true, so intuitively this expresses that some progress will be made towards getting to <code>F</code> eventually. The reverse, <code>&lt;&gt;[]F</code>, expresses that eventually, <code>F</code> will always hold. Intuitively, this denotes stability.</p>
<p>The final concept I&rsquo;ll cover is how we apply these to express &lsquo;fairness&rsquo;. It&rsquo;s a property that states that if something happens &lsquo;often enough&rsquo;, it should eventually happen. There&rsquo;s variations: weak fairness can be expressed as <code>&lt;&gt;[]F → []&lt;&gt;F</code> and says that a step towards <code>F</code> must eventually occur. The implication reads that if <code>F</code> is eventually continually true, then it must eventually occur. Strong fairness, on the other hand, can be expressed as <code>[]&lt;&gt;F → []&lt;&gt;F</code>, which says that a step must eventually occur even if something is not eventually continually true. For more intuition about the difference between weak and strong fairness, think of a traffic light. If the traffic light is strongly fair, the car will eventually have to go, because it&rsquo;ll eventually be green before switching back to red. However, if the traffic light is weakly fair, then the car might never go, because the traffic light will eventually switch back to red and never has a point where it will continue to always be green. This was really mind-bending to wrap my head around, and I think the concepts of eventual-ness and the timing logic here is fun to dig into.</p>
<p>Another coincidence: I also learned that Professor Bieniusa will be collaborating with my supervisor for my research internship next term, so we might get to connect again soon!</p>
<h2 id="german-efficiency">German Efficiency</h2>
<p>In addition to the cold, hard, technical details, I also learned about the finer details of European education systems (the Belgian and German ones, in particular) and about Darmstadt and Germany as a whole. I speak no German, so I had to rely on the locals speaking English. I was a little self-conscious about being the classic clueless North American tourist who romps about Europe and is generally a nuisance. Granted, I don&rsquo;t think I bothered anyone, but I really felt like I was very uncultured and not well-informed before learning about any of this. I&rsquo;m starting to get why people recommend exchange programs and travelling so much — you learn so much by osmosis and vibes, even from a quick stay where you&rsquo;re not interacting much with the locals.</p>
<p>Some quick-fire notes:</p>
<ul>
<li>The Frankfurt airport seemed very empty, even though I was arriving on a weekend afternoon, when I&rsquo;d have expected it to be bustling. Maybe I was in a quieter terminal.</li>
<li>On the other hand, the smoking lounges seemed very full. I was mildly shocked to see a smoking lounge right out the gate, especially indoors. It was also odd to see people smoking right outside doors, young people smoking, and other indoor smoking areas. In Vancouver, it tends to be fairly rare and is almost always an older person huddled in an alleyway, not a well-dressed twenty-something strutting by with friends.</li>
<li>Darmstadt is literally translated as &ldquo;colon/intestines-city&rdquo;. Something about how if Germany was anthropomorphized into a human, Darmstadt would be smack where the bowels were. Apologies if you were enjoying a nice meal at this point in reading.</li>
<li>Trains seem to be consistently late. We took a train a bit closer to Frankfurt to hike, and our train there was almost ten minutes late, and our train back was closer to fifteen late. I was told that German efficiency only applies to cars.</li>
<li>Germans are pretty intense about their hiking. We went up a &lsquo;small hike&rsquo; to <a href="https://www.schloesser-hessen.de/en/schloss-auerbach">Auerbach Castle</a>, which was the better part of an hour up a fairly steep hill. I&rsquo;d assumed since they didn&rsquo;t ask about accessibility restrictions that the &lsquo;hike&rsquo; meant a flat walk, but no, this was really a hike. At some point we saw people <em>biking</em> down the very steep, narrow path, and at the top I was told that most Germans would not consider our trek anything near a hike.</li>
<li>Everything initially seems more expensive than Canada — for example, ramen might run you 14 euros. I was told that this was relatively cheaper than other parts of Europe, but the converted equivalent of ~$21 CAD seemed a little steep. Something closer to $12-15 CAD is what I&rsquo;m used to. However, when you factor in the lack of tip and the already-included tax, it&rsquo;s not far off from Vancouver prices. It was nice that most prices were round numbers too, which helped with sorting out change.</li>
<li>It rains a lot, and people are used to it. One of the highlights of the program was a walking tour around the city on our first day, during which it started thunderstorming and pouring. The lightning and thunder were just a few seconds apart, and we were huddling, trying to recall the conversion for time between lightning and thunder to how close the storm was to figure out screwed we might be. Our guide, completely unfazed, led us around into the main city centre castle and continued peppering us with facts.</li>
<li>Darmstadt has their own 9/11 story, albeit in 1944. The city was <a href="https://en.wikipedia.org/wiki/Bombing_of_Darmstadt_in_World_War_II">heavily bombed by the RAF</a> during WWII, destroying half the town&rsquo;s homes overnight. We were actually in town for the 80th anniversary memorial event, and all through Wednesday we heard the bells tolling across town.</li>
<li>The cafeteria food at TU Darmstadt is quite good. It felt a bit like Ikea standing in the cafeteria line and grabbing lunch, but there was solid variety. I will warn folks that when they translate something as dumplings, though, they mean American/Western-style dumplings — I was not expecting a dense dough ball.
<ul>
<li>The group I was with was very interested in having Asian food for our free meals, seeing as we had a German-cuisine dinner already scheduled. The <a href="https://www.moschmosch.com/">two</a> <a href="https://g.co/kgs/qaPbk23">places</a> we tried were great. It was very fun teaching them to use chopsticks and see them tank Szechuan peppercorns for the first time.</li>
<li>I feel obligated to also especially shout out <a href="https://g.co/kgs/3Fgcghn">this hole-in-the-wall Tibetan dumpling</a> takeaway place, which I tried on my last night. The staff offered me a free sample of mango lassi and were very sweet in explaining everything in English.</li>
</ul>
</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p>Besides all the distributed systems and multicultural learning, I also met many lovely people (who are probably reading this post — I can still see my blog analytics getting a suspicious number of views from Belgium). It was great making some new friends, since I think I was one of the only people who wasn&rsquo;t with a contingent from their home university and perhaps the only North American. I did my share of cultural exchange too: other than being the de-facto Asian-culture expert, I also taught folks some Canadian and American slang<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> and explained how university works here.</p>
<p>I&rsquo;m very happy that I was able to attend and feel very fulfilled in the new fields I&rsquo;ve gotten a tour of during DARE. I&rsquo;m proud to say that this was the longest continuous period in my life where I felt like I fully grasped the difference between strict, sequential, and causal consistency, and I can still mostly reason about the details now. Being exposed to different areas of research that I had no real background in was a challenge — once the LaTeX started flowing in a talk, I admit things generally started going over my head — but was also great to be able to build an idea of the different subareas within distributed systems. DARE was a shortcut in getting to the frontier of research in a short week, and I&rsquo;m excited to be able to continue thinking about some of these problems in my current and upcoming research internships.</p>
<p>The other students who took this program for ECTS credits were required to do another two-week research project following DARE, building on one of these lectures. They&rsquo;ll be doing a presentation soon in a few weeks, and I&rsquo;m really looking forward to seeing what they&rsquo;ve come up with. I didn&rsquo;t have to do one since the transfer credits aren&rsquo;t going to meaningfully affect my courseload next year, but in a way my current internship work is one big extension of Professor Kleppmann&rsquo;s talk and work, so I&rsquo;ll say it counts.</p>
<p>I&rsquo;d very much recommend the program to anyone even tangentially interested in distributed systems. I believe the talks will be different year-to-year, and I&rsquo;ve heard the next iteration is planned in Porto, Portugal. There&rsquo;s funding available for European students via the Erasmus program and no fee for the program itself otherwise. Stay on the lookout for DARE 2025 — I&rsquo;d strongly encourage other students to go for it!</p>


<style>
ul {
margin-bottom: 0.5em;
}
</style>


<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>I will never forget when we were standing in a circle with someone else from Portugal explaining Skibidi Toilet to the others and chatting about brainrot, when Professor Baquero joined the group and said something along the lines of, &ldquo;ah yes, &lsquo;brainrot&rsquo;, that must be what my daughter has&rdquo; and taking a look at a Skibidi Toilet episode. Oh, how I love the Internet.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>Magic! at the Mall</title>
      <link>https://kewbi.sh/blog/posts/240825/</link>
      <pubDate>25 Aug 2024</pubDate>
      
      <description>On new phones and new paradigms.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>I got my first phone sometime around 2018<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. I got my second last month.</p>
<p>My first phone was a silver iPhone 7. I remember my friends with iPhones had iPhone SEs or 6s back then, and having a 7 was a subtle step up. I was the one who’d take photos because I had the “best” camera. I rarely used it when I got it — our school had a phone-free policy, so I’d have to dump my phone into this wicker basket of phones at the start of the day. I’d rescue it at the end of the day, then not really do anything with it after I got home. I didn’t even get a phone plan until I started high school, and til then I didn’t use my phone for much besides Google Hangouts and the occasional FaceTime.</p>
<p>There was one classmate who had an iPhone 8, and I recall thinking the X was so excessive. What were the new features, besides a slightly bigger screen and no home button? Big deal. Fast forward a few years, and I’d think people who got the then brand-new iPhone 13 were on the bleeding edge, perhaps a little extravagantly so.</p>
<p>But it’s 2024, and the dynamic island and USB-C charger and three cameras of the iPhone 15 are all the rage. I’d still stubbornly stuck to my iPhone 7. All of a sudden, I was the only friend with a home button. I’d stopped getting updates a few years ago, but I didn’t really miss any of the new features. My 7 served me well — besides, well, Uber Eats not working with anything under iOS 16 and FaceTime and Discord calls starting to stutter out. I still didn’t really feel a need to upgrade, and having an old phone felt almost like a point of pride for me at this point: I’d taken such good care of my phone that here it was, six years later, with no scratches, a two day battery life, and working like a charm.</p>
<p>Unfortunately, the day came last month: I’m headed to Cambridge (UK) for the fall, and I’ll need both my Canadian SIM, to receive SMS verification codes and such, and a local SIM for calling. My iPhone 7 doesn’t support an eSIM, so I won’t be able to dual-SIM. With a heavy heart, I made my way to my local Apple Store.</p>
<p>There, I got an iPhone 14 (doubling my model number!). While I was there, a lot of casual magic happened. I’d never bought a phone in person before, or really spent significant time in an Apple Store, so I was pleasantly surprised by some of the little touches I noticed. This was also my first time switching phones, so I got to experience the sheer wizardry that is Quick Start. And while I was booking my pickup slot, I also noticed Apple Vision Pro demos available, and I impulsively booked a demo. The AVP isn’t something that I’ve really thought about, besides seeing a few tweets and video thumbnails, or considered for actual use, but I found the demo fairly enchanting.</p>
<p>I think there are a few aspects that <em>make magic</em>.</p>
<ul>
<li>Magic inverts expectations while building on them. It makes the hard things unexpectedly easy and the impossible things possible, but it has to do so in an incredibly intuitive way. There shouldn’t be a &lsquo;why does this work?&rsquo; — there can only be an ‘of course’.</li>
<li>Magic is embedded and composable, not standalone and sandboxed. It’s adaptable to whatever you need in the moment, and comprehensive in covering everything you might think about.</li>
<li>Magic is predictive but forgiving. It figures out the word on the tip of your tongue and the recurring patterns that make up your day. When you get something wrong, it gently nudges you to ask if that’s what you really want.</li>
<li>Magic can disappear. With more magic comes more responsibility. The more magical an experience, the more tiny flaws can quickly break the suspension of disbelief.</li>
</ul>
<p>The Quick Start and AVP demo experiences both captured these traits — I think that afternoon in the Apple store was the highest density of casual magic I’ve experienced this summer. This is a post about those moments of magic at the mall and what makes a computing experience compelling. I’ve always been jokingly disdainful about Apple fans, but in those few hours in the store, I started to see what they’re so enthusiastic about.</p>
<h2 id="dont-quick-start-now">Don&rsquo;t (Quick) Start Now</h2>
<p>Picking up my phone was a very straightforward 5-minute errand. Unfortunately, when I got home and was admiring my edge-to-edge screen, I noticed the phone had a scratch. I wasn&rsquo;t about to spend that much money and take a defective phone, so I had to go back and exchange it<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. I didn&rsquo;t want to risk taking another trip again if there was another visual defect or some software problem, so I decided to do my setup in-store (and also because the only AVP demos were at the end of the day, so I had some time.)</p>
<p>I don&rsquo;t think I had to worry much about the migration though, since Quick Start was seamless. <a href="https://support.apple.com/en-ca/102659">Quick Start</a> is a way to wirelessly transfer all your apps, app data, messages, and preferences over to a new device. You can do Quick Start via an iCloud backup, or you can run one directly from the device. One really nice touch was that I didn&rsquo;t even have to log into or search for the Apple store WiFi, and even over the public WiFi, which I didn&rsquo;t expect to be very fast, the setup was done in less than five minutes. I don&rsquo;t have a lot of photos or data backed up on my phone, but nonetheless I was impressed. This speed underscores something about magic: it works fast. Magic doesn&rsquo;t need endless loading bars and doesn&rsquo;t get stuck downloading something.</p>
<p>There are plenty of small touches that transferred over: my texts and contacts were just as I&rsquo;d left them, I was already logged into most apps, my wallpaper and lock screen was identical, the years of settings I&rsquo;d carefully curated were in place, my pirated textbook PDFs were set up perfectly with Apple Books. My muscle memory for everything still worked, without any of the tedious setup and comparing things between either phone. It was like having an exact, scaled-up replica of my old phone.</p>
<p>I don&rsquo;t think I&rsquo;ve ever been so pleasantly surprised with a migration process. When I bought my phone, I was dreading the hours I&rsquo;d expected of downloading everything and setting up logins and preferences again. Taking all that away with such attention to detail was a very good investment on Apple’s part. I used to be distrustful of cloud syncs and signing into browser/device services<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>, but they’re so helpful for applications where they’re (somewhat) strictly necessary. I think background processes like syncs and preprocessing do a lot of the heavy lifting for magic. I have no insight into how Apple’s photos systems works, for example, but I’d bet they’re creating their recap albums with an off-device queueing service and they’re running their people detection online when a new photo’s uploaded to iCloud Photos. This ties into my previous point about magic working to match your speed — because the processing happens before you’re looking to access something, it seems more magical when you go to look something up and it’s already there.</p>
<p>This background processing is also a lot easier when your endpoints are all centralized, because this provides consistency in how data is stored/read. This is how you can make your service feel omniscient and predictive, since you have all the data to figure out all the patterns. You can certainly engineer a way to connect more disparate data sources: I keep coming back to reference <a href="https://www.inkandswitch.com/cambria/">Cambria lenses</a>, and I was introduced to the <a href="https://en.wikipedia.org/wiki/Resource_Description_Framework">Resource Description Framework</a> model in one of the first few chapters of <em>Designing Data-Intensive Applications</em>. I just finished the book, and one of the primary focuses of the last chapter is on data integration via derived state, batch processing, and federated/unbundled databases, which I expect would be the key components for bringing something like this to generic devices. To this end, Samsung’s Smart Switch seems to be able to bring iOS data to Android devices as well as from other Android devices to a Samsung, and I’d be interested in learning more about how it all works.</p>
<p>I think Quick Start’s adaptability and coverage were key to making it feel like magic. I wouldn’t have expected Apple Books, for example, to get ported over so seamlessly, though now that I think about it, it probably does back up to iCloud. Even apps outside of the Apple ecosystem, like Discord and FitBit had me already authenticated and all my app-specific settings were translated over. Magic works over everything – it’s not meant to let on that it’s forgotten something and can’t be even slightly inconsistent with the ecosystem’s ‘magic system’, and Quick Start does so expertly. Maybe this is all because I haven’t done iOS development before and this is all thanks to some slick data access API requirements, but I’m so awed by how flexible the setup feels.</p>
<p>I think two things that could be improved, perhaps, are the Apple Wallet transfers for credit cards (although I guess there&rsquo;s good regulatory and privacy reasons for this) as well as SIM-card / phone-number based apps like Whatsapp or Signal. But even off the top of my head, I can think of some technical limitations with each, so there&rsquo;s probably a reason they&rsquo;ve not been implemented.</p>
<h2 id="interlude-i-miss-my-home-button">Interlude: I Miss My Home Button</h2>
<p>With my new phone itself, there are a few key things I’ve noticed. The first is fairly obvious: the camera quality is certainly a step up (and in tandem, the quality of the screen to view the photos I take has definitely improved). I was recently at a work dinner with my fellow interns, and I took out my phone to take a quick photo for my parents. The second I opened the camera app, I blurted out that the camera quality was so much better than my old phone. Doubling my model number also seemed to double the warmth, depth of colour, and sharpness of even casual pics. When I was on a trip to the UK earlier this month, I took a few nighttime photos of the very classic architecture – lots of fine details and masonry. Despite the dim lighting, the photos were still able to capture things quite well with minimal grain. I’m very impressed with this camera, and I can’t imagine the further upgrades that the newest models are supposed to deliver.</p>
<p>Another thing I’ve appreciated is finally having good NFC support! I wrote another blog post about <a href="https://kewbi.sh/blog/posts/240811/">building a webring that interacts with a physical NFC ring</a>, something that wasn’t possible to test with my old phone. I’ve been a little obsessed with NFC tags and having little physical checkpoints that interact with my digital world (for example, <a href="https://x.com/spencerc99/status/1818721711858368890">this do-not-disturb phone pillow</a>, or its more consumer counterpart <a href="https://getbrick.app/">Brick</a>). I&rsquo;ve played around a little with NFC and the Shortcuts app, and I’m also happy with how much Shortcuts has levelled up since iOS 15.</p>
<p>Finally, I’ve realized how nice it is that apps and features are able to pick up on patterns of usage. One example is the wallet app – I was travelling abroad and was using a credit card that I don’t normally use. Within a few days, Apple Wallet knew to bring that card up as my default when I double-pressed the power button. This is a nice tidbit of magic — Apple Wallet was smart enough to pick up on my intents without explicit configuration, but it’s easy to override and pick a different card if I needed to.</p>
<p>There’s much more, like being able to customise my home screen more with different icons and widgets and fonts, but I’d like to move on to another major magic experience.</p>
<h2 id="14-pounds">1.4 Pounds</h2>
<p>I impulsively booked an AVP demo since I’d be there at the store anyways, having not really seriously thought about the device or read up about its features beyond the ‘first look’ demo that was all over my feed. When I got there, the first thing the Specialist said was that the AVP was not a VR headset: it was a spatial computer. I still don’t really buy the rebranding — it feels a tad pedantic — but I will say it’s unlike anything I’d ever tried before.</p>
<p>My headset experience is limited to a ten-minute stint playing Fruit Ninja on an Oculus Quest during a summer camp, so maybe that&rsquo;s why I was so intrigued. First off, I was not expecting to have my face scanned and a custom-fit headset delivered to the demo station. I think I have a fairly normal set of face shape/head size/vision requirements, so maybe it was all a bit of theatre to make the demo feel more personalised.</p>
<p>Tailoring the demo is the major thing I felt like was lacking. The experience starts with learning how to browse photos, view live photos immersively, and navigate around apps, culminating in a very well-shot immersive video. This really highlighted the gestures and new interactions that were possible with the AVP and certainly provided the most wow-factor. I wish I&rsquo;d have gotten more walkthroughs through more productivity and everyday work demos, though. The AVP was constantly touted as a portable way to make the world your workspace for anything, but we never got to actually see what doing work was like. Going through a spreadsheets program, editing a video, or doing some debugging might have made the demo more compelling for people looking for a more serious, professional use-case for the AVP. It would have been annoying to set up and pair a MacBook for every demo, but surely there&rsquo;s a way to streamline this (make more magic, y&rsquo;all!) It would have been extra amazing if the demo could pull from iCloud data – there must be something they can copy-paste from Quick Start. I would have loved to learn how to read my EPUBs from Apple Books or how to use Shortcuts with the AVP, especially any cross-device capabilities. If anything, the current demo sells the AVP as a (heavier), more immersive version of the VR headsets already available, focusing on entertainment and casual usage: exactly what Apple was trying to avoid.</p>
<p>Controlling the AVP was a bit like how I imagine Harry Potter et al. felt at Hogwarts — harnessing magic is tricky. The calibration helped serve as a tutorial to pick up the mouse mechanics, and I liked the slight gamification. Once you got used to having to look exactly where you wanted the cursor and do the pinch-clicking, the tutorial went by fast. Pointing with my eyes didn&rsquo;t feel very accurate at first, though, unless I really focused on a point, or if I shifted my focus, then looked back. Doing gestures without a button also felt a little unusual due to the lack of tactile feedback, but by the end of the demo I was well-adjusted. I was still relying on instructions for when to use the crown and for what, but I&rsquo;m sure with more time it&rsquo;d have gotten ingrained into muscle memory.</p>
<p>One of my favourite parts of the demo was that cinematic video I mentioned before — it tied together the best of the audio/video capabilities. There are a bunch of safari and immersive walk-with-the-animals-type clips, and I&rsquo;d swear that the elephants were right by me. There was a scene with a tightrope walker, and I felt my heart drop when they also fell off. There was also an NBA scene where the player throws the ball right at your face — I visibly flinched, and the Specialist said she uses it as a marker of how far people are into the video when they recoil.</p>
<p>I loved the depth of field of the AVP, and I think it really helped with the suspension of disbelief and the resulting magic. Had the videos felt flatter or the layers more compressed, it wouldn&rsquo;t have gotten the feeling just right. Because most of the demo was so flawlessly executed, I think it made it obvious when and highlighted when things didn&rsquo;t go quite so well (e.g. the visual pointing). That&rsquo;s another aspect of magic: if you&rsquo;re going to build an immersive and comprehensive experience, it&rsquo;s crucial you take care of the smallest details too.</p>
<h2 id="conclusion">Conclusion</h2>
<p>A few days ago, before I started writing this post, I was trying to check my phone when it froze. I’d opened the Clock app to set an alarm, but all I saw were a row of icons at the bottom with a grey screen. I locked my phone and swiped up to open it — it faded out the clock font but wouldn’t open the app. It was much too late to go out to the Apple Store, and I was dreading making the trek the next day. The illusion of magic? Gone<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.</p>
<p>But I’ll forgive Apple on this one, since the whole experience of getting the phone itself was enough magic for a good bit. Between Quick Start, old new features, and the Apple Vision Pro, I’ve both been able to be enchanted and think a little more about what made each so charming. All of the encounters captured some of the core aspects of magic: Quick Start flipped the script on tedious phone migrations and did so in an incredibly intuitive, embedded, and comprehensive way. My new phone made me appreciate the power of good predictions, and how those little touches can seem so obvious in hindsight. On the other hand, the Apple Vision Pro underscored the responsibility that comes with this power — its immersion felt magical, which especially underscored the handful of papercuts along the way.</p>
<p>Overall, I think what really makes or breaks magic is how well it fits into an existing worldview and how intuitive it is. I was comparing magic in software to classic fantasy book series like Harry Potter, which have very composable, expansive, and consistent magic systems. Tech is how we’ll make hard things easy and make the impossible possible, but we’ll have to do so carefully to really capture the magic. Radically different offerings, like the AVP, bring a lot of opportunities for moments for casual magic, but I can appreciate the challenges that must have come up in order to make it feel so spellbinding.</p>
<p>I thought I’d have hated getting a new phone, but I both appreciate the phone itself and the fascinating few hours I had at the Apple store because of it. I plan on holding on to this phone for a long time again. I went from an iPhone 7 to an iPhone 14, so the logical next step is to wait for the next multiple of seven. When I drop by to pick up my iPhone 21, I’ll see what demos, features, and crazy, magical moments are possible then.</p>
<hr>
<p>P.S. If you&rsquo;re looking for posters or art, you should check out the <a href="https://www.etsy.com/ca/listing/1762613124/6-minimalist-computer-patent-prints">Toronto Island Patent Press</a> on Etsy! A friend and I made a set of posters based on retro schematics and patents for classic computing companies, like IBM, Nortel, and DeskMaster. They&rsquo;re available as digital downloads in a wide variety of very aesthetic colourways — perfect for the sort of people who geek out over the first monitors and blueprint drawings.</p>
<figure><img src="/img/240825/promo-pic.png"
         alt="Figure 1. Example posters and colourways."/><figcaption>
            <p><em>Figure 1. Example posters and colourways.</em></p>
        </figcaption>
</figure>

<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>This is a lie – my parents had gotten me my own phone for my birthday the year before IIRC, but I told them to return it since I didn’t really need it for anything. I think you can draw many conclusions about my personality from this.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>This was also an ordeal in and of itself — if you ever get a friend to buy you a phone with a friends-and-family discount, try to avoid having to exchange it since you&rsquo;ll have to get them to refund, then re-buy the phone with the discount. A bit of a hassle if your friend isn&rsquo;t local and doesn’t come with you.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>When I got my first computer, I not only refused to use Chrome logged in, but I used Chrome only in an incognito window. No history, no way to keep my tabs between sessions. I’d only put my laptop into Sleep instead of ever shutting it down, and I’d try to put off Chrome updates as long as possible. Whenever I was forced to re-open Chrome, I made a trigger list of sites to have to log back into again, and I’d need to go through and login to each and every one of them. Like footnote 1, I think you can see how this fits into my personal lore.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>If you run into the same issue, press and release volume up, then volume down, then hold the power button til after the Apple logo comes up.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
